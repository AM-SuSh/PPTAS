# PPTAS AI Agent 系统技术文档


此部分为 AI Agent LLM 流水线、全局分析、单页分析以及数据持久化系统的技术实现细节。

## 1. 概述

### 1.1 实现目标

PPTAS 系统旨在通过多 Agent 协作的方式，对 PPT 文档进行智能分析和扩展，为读者提供结构化的学习笔记、知识缺口识别和补充说明。

### 1.2 核心功能

1. **全局分析**: 分析整个文档的主题、章节结构和知识点框架
2. **单页分析**: 对单个页面进行深度分析，包括知识聚类、理解笔记生成、知识缺口识别等
3. **知识扩展**: 为识别的知识缺口生成补充说明和参考资料
4. **数据持久化**: 将分析结果持久化存储到 SQLite 数据库

### 1.3 技术架构

系统采用分层架构设计：

- **API 层**: FastAPI 框架，提供 RESTful API 接口
- **服务层**: 业务逻辑封装，包括页面分析服务、持久化服务等
- **Agent 层**: 基于 LangChain 和 LangGraph 的多 Agent 协作系统
- **数据层**: SQLite 数据库，存储文档和分析结果

## 2. AI Agent LLM 流水线设计

### 2.1 流水线架构

AI Agent LLM 流水线基于 LangGraph 框架实现，采用有向无环图（DAG）结构组织多个 Agent 的协作流程。

#### 2.1.1 流水线流程图

```
初始状态
  ↓
[GlobalStructureAgent] → 全局结构解析
  ↓
[KnowledgeClusteringAgent] → 全局知识点聚类
  ↓
[PageKnowledgeClusterer] → 单页知识聚类
  ↓
[StructureUnderstandingAgent] → 结构理解与笔记生成
  ↓
[GapIdentificationAgent] → 知识缺口识别
  ↓
[KnowledgeExpansionAgent] → 知识扩展
  ↓
[RetrievalAgent] → 外部检索（并行）
  ↓
[ConsistencyCheckAgent] → 一致性校验
  ↓
[StructuredOrganizationAgent] → 内容整理
  ↓
最终状态
```

#### 2.1.2 状态管理

流水线使用 `GraphState` 类型管理状态，定义在 `src/agents/models.py`：

```python
class GraphState(TypedDict):
    ppt_texts: List[str]                    # PPT 文本列表
    global_outline: Dict[str, Any]          # 全局概览
    knowledge_units: List[KnowledgeUnit]     # 知识点单元
    current_unit_id: str                    # 当前单元ID
    current_page_id: int                     # 当前页面ID
    raw_text: str                           # 原始文本
    page_structure: Dict[str, Any]          # 页面结构
    knowledge_clusters: List[Dict[str, Any]] # 知识聚类
    understanding_notes: str                 # 理解笔记
    knowledge_gaps: List[KnowledgeGap]      # 知识缺口
    expanded_content: List[ExpandedContent] # 扩展内容
    retrieved_docs: List[Document]          # 检索文档
    check_result: CheckResult                # 校验结果
    final_notes: str                         # 最终笔记
    revision_count: int                      # 修订次数
    max_revisions: int                       # 最大修订次数
    streaming_chunks: List[str]             # 流式输出块
```

### 2.2 Agent 实现细节

#### 2.2.1 GlobalStructureAgent

**功能**: 分析整个文档的主题、章节结构和知识逻辑流程

**实现位置**: `src/agents/base.py`

**核心逻辑**:

1. 接收所有页面的文本内容
2. 如果页数过多（>20页），进行文本摘要以减少 token 消耗
3. 使用 LLM 提取主题、章节和知识流程
4. 解析 JSON 格式的输出

**Prompt 设计**:

```
你是一个教育专家，需要分析这份PPT/PDF文档的整体结构和知识框架。

文档内容（共{total_pages}页）:
{ppt_texts}

请仔细分析整个文档，提取以下信息：
1. **主题**：整个文档的核心主题是什么？
2. **章节结构**：文档分为哪些主要章节？每个章节包含哪些页面？
3. **知识逻辑流程**：这些章节之间的知识逻辑关系是什么？

请以JSON格式输出...
```

**技术要点**:

- 温度参数设置为 0，确保输出稳定性
- 对长文档进行智能摘要，平衡信息完整性和 token 消耗
- 输入限制: 超过 20 页时，每页 200 字摘要
- 使用结构化输出解析器确保 JSON 格式正确

#### 2.2.2 KnowledgeClusteringAgent

**功能**: 将文档内容聚类为知识点单元

**实现位置**: `src/agents/base.py`

**核心逻辑**:

1. 基于全局结构信息，识别跨页面的知识点单元
2. 每个知识点单元包含：单元ID、标题、涉及页面、核心概念
3. 确保知识点单元的完整性和教学闭环

**输出格式**:

```python
[
    KnowledgeUnit(
        unit_id="unit_1",
        title="知识点单元标题",
        pages=[1, 2, 3],
        core_concepts=["概念1", "概念2"]
    )
]
```

#### 2.2.3 PageKnowledgeClusterer

**功能**: 识别单页中难以理解的概念

**实现位置**: `src/services/page_analysis_service.py`

**核心逻辑**:

1. 支持基于全局上下文的增强分析
2. 识别概念、评估难度级别、说明困难原因
3. 限制最多 10 个概念，避免信息过载

**配置参数**:

- 温度: 0.3（有创意但相对稳定）
- 最多 10 个概念
- 难度分级: 1（简单）到 5（很难）

**全局上下文支持**:

当提供全局分析结果时，Agent 会：
- 参考全局知识点单元
- 考虑概念在整个文档知识体系中的位置
- 提供更准确的难度评估

**输出格式**:

```python
[
    {
        "concept": "概念名称",
        "difficulty_level": 3,              # 1-5
        "why_difficult": "为什么难理解",
        "related_concepts": ["相关概念"],
        "global_context": "在全局知识框架中的位置"
    }
]
```

#### 2.2.4 StructureUnderstandingAgent

**功能**: 生成结构化学习笔记

**实现位置**: `src/agents/base.py`

**核心逻辑**:

1. 两个 LLM 调用：
   - 第一个：生成 Markdown 格式的学习笔记
   - 第二个：提取页面结构信息
2. 笔记格式适合快速复习
3. 结构化提取便于后续处理

**配置参数**:

- 温度: 0.5（平衡创意和准确性）
- 输入限制: 1000 字 + 800 字
- 笔记长度限制: 300 字

**输出格式**:

- `understanding_notes`: Markdown 格式的学习笔记
- `page_structure`: 包含页面ID、标题、核心概念、关键要点等

#### 2.2.5 GapIdentificationAgent

**功能**: 识别读者理解障碍点

**实现位置**: `src/agents/base.py`

**核心逻辑**:

1. 基于页面内容和已识别的难点概念
2. 识别四种类型的知识缺口：
   - 直观解释：缺少通俗易懂的解释
   - 应用示例：缺少实际应用场景
   - 背景知识：缺少前置知识
   - 公式推导：缺少公式推导过程
3. 为每个缺口分配优先级（1-5）

**配置参数**:

- 温度: 0.2（确定性，聚焦实际问题）
- 最多 5 个缺口，避免过度分析
- 优先级: 1（可选）到 5（必须）

**全局上下文支持**:

当提供全局分析结果时，Agent 会：
- 考虑概念在整个文档中的位置和关系
- 识别跨页面的知识依赖关系

**输出格式**:

```python
[
    KnowledgeGap(
        concept="需要补充的概念",
        gap_types=["直观解释", "应用示例"],
        priority=4
    )
]
```

#### 2.2.6 KnowledgeExpansionAgent

**功能**: 为识别的知识缺口生成补充说明

**实现位置**: `src/agents/base.py`

**核心逻辑**:

1. 针对每个知识缺口，生成相应的补充说明
2. 根据缺口类型选择不同的生成策略：
   - 直观解释：提供通俗易懂的解释
   - 应用示例：提供实际应用场景
   - 背景知识：补充前置知识
   - 公式推导：提供详细的推导过程
3. 引用已检索的外部文档作为支撑
4. 按优先级排序，只处理前 3 个缺口
5. 严格控制长度（150 字内），确保简洁

**配置参数**:

- 温度: 0.6（更多创意用于举例）
- 最多处理 3 个缺口
- 输出限制: 150 字 + 300 字最终长度
- 输入: 500 字内容

**输出格式**:

```python
[
    ExpandedContent(
        concept="概念",
        gap_type="直观解释",
        content="补充说明内容",
        sources=["来源1", "来源2"]
    )
]
```

#### 2.2.7 RetrievalAgent

**功能**: 从外部知识源检索参考资料

**实现位置**: `src/agents/base.py`

**核心逻辑**:

1. 基于知识缺口中的概念，构建检索查询
2. 从多个知识源检索：
   - Wikipedia（维基百科）
   - Arxiv（学术论文）
   - Google Scholar（学术搜索）
   - Baidu Baike（百度百科）
3. 合并和去重检索结果
4. 限制结果数量，避免信息过载

**检索策略**:

1. **智能源检测**: 初始化时测试外部源连通性
2. **优先本地 RAG**: 先从向量数据库检索
3. **外部检索**: 仅当本地结果不足且源可用时查询
4. **早期退出**: 所有源都不可用时立即返回
5. **结果过滤**: 去除占位符文档，仅保留有有效 URL 的结果

**知识源优先级**:

根据配置的 `retrieval.preferred_sources` 决定优先使用的知识源。

**配置参数**:

- 温度: 0（确定性检索）
- 最多 5 条检索结果
- 只为高优先级缺口（priority ≥ 4）检索
- 合并查询以减少检索次数

**输出格式**:

```python
[
    Document(
        page_content="文档内容",
        metadata={
            "title": "标题",
            "url": "URL",
            "source": "wikipedia"
        }
    )
]
```

#### 2.2.8 ConsistencyCheckAgent

**功能**: 校验扩展内容与原始内容的一致性

**实现位置**: `src/agents/base.py`

**核心逻辑**:

1. 对比扩展内容与原始 PPT 内容
2. 检查是否存在：
   - 信息矛盾
   - 过度延伸
   - 偏离主题
3. 生成校验结果和建议

**校验规则**:

1. **禁止编造**: 不能提及 PPT 未涉及的新概念
2. **有据可查**: 所有陈述必须来自 PPT 或参考资料
3. **标记推测**: 不确定的内容标记为"推测"
4. **发现矛盾**: 与 PPT 或参考资料矛盾时标记为修正

**配置参数**:

- 温度: 0（严格检查，无创意）
- 输入限制: PPT 600 字 + 参考 3 条

**输出格式**:

```python
CheckResult(
    status="pass" | "revise",
    issues=["问题1", "问题2"],
    suggestions=["建议1", "建议2"]
)
```

**修订机制**:

如果校验失败（status="revise"），且修订次数未达到上限，系统会：
1. 增加修订计数
2. 重新执行知识扩展
3. 再次进行一致性校验

#### 2.2.9 StructuredOrganizationAgent

**功能**: 将分析结果整理为最终格式

**实现位置**: `src/agents/base.py`

**核心逻辑**:

1. 整合所有分析结果
2. 生成结构化的最终学习笔记
3. 确保内容不偏离源文本
4. 优化 Markdown 格式
5. 去重和去冗余
6. 生成适合读者快速查阅的版本

**笔记格式**:

- 标题明确
- 核心概念优先
- 避免重复原文
- 适合快速复习
- 严格控制长度（300 字内）

**配置参数**:

- 温度: 0.5（平衡）
- 输出限制: 300 字内
- 输入: 500 字内容 + 补充说明

**输出格式**:

- `final_notes`: 最终整理的学习笔记（Markdown 格式）

### 2.3 流水线执行方式

#### 2.3.1 完整流水线（PPTExpansionService）

使用 `PPTExpansionService` 执行完整的 LangGraph 工作流：

**实现位置**: `src/services/ppt_expansion_service.py`

**工作流构建**:

```python
def _build_graph(self) -> StateGraph:
    workflow = StateGraph(GraphState)
    
    # 添加节点
    workflow.add_node("global_structure", self.global_structure_agent.run)
    workflow.add_node("knowledge_clustering", self.clustering_agent.run)
    workflow.add_node("structure_understanding", self.structure_agent.run)
    workflow.add_node("gap_identification", self.gap_agent.run)
    workflow.add_node("knowledge_expansion", self.expansion_agent.run)
    workflow.add_node("retrieval", self.retrieval_agent.run)
    workflow.add_node("consistency_check", self.check_agent.run)
    workflow.add_node("structured_organization", self.organization_agent.run)
    
    # 定义边
    workflow.set_entry_point("global_structure")
    workflow.add_edge("global_structure", "knowledge_clustering")
    workflow.add_edge("knowledge_clustering", "structure_understanding")
    workflow.add_edge("structure_understanding", "gap_identification")
    workflow.add_edge("gap_identification", "knowledge_expansion")
    workflow.add_edge("gap_identification", "retrieval")  # 并行
    workflow.add_edge("knowledge_expansion", "consistency_check")
    workflow.add_edge("retrieval", "consistency_check")
    
    # 条件边：校验通过 -> 整理，校验失败 -> 重新扩展
    workflow.add_conditional_edges(
        "consistency_check",
        self._should_revise,
        {
            "revise": "knowledge_expansion",
            "pass": "structured_organization"
        }
    )
    
    workflow.add_edge("structured_organization", END)
    
    return workflow.compile()
```

**执行流程**:

1. 初始化状态
2. 按顺序执行各个 Agent
3. 根据条件边决定是否修订
4. 返回最终状态

#### 2.3.2 单页分析（PageDeepAnalysisService）

使用 `PageDeepAnalysisService` 对单个页面进行分析：

**实现位置**: `src/services/page_analysis_service.py`

**执行流程**:

```python
def analyze_page(self, page_id, title, content, raw_points=None):
    # 初始化状态
    state = {
        "ppt_texts": [content],
        "global_outline": {},
        "knowledge_units": [],
        "current_page_id": page_id,
        "raw_text": content,
        # ... 其他字段
    }
    
    # 步骤1: 知识聚类
    knowledge_clusters = self.clustering_agent.run(content)
    state["knowledge_clusters"] = knowledge_clusters
    
    # 步骤2: 理解笔记
    state = self.understanding_agent.run(state)
    
    # 步骤3: 知识缺口识别
    state = self.gap_agent.run(state)
    
    # 步骤4: 知识扩展
    state = self.expansion_agent.run(state)
    
    # 步骤5: 外部检索
    state = self.retrieval_agent.run(state)
    
    # 步骤6: 一致性校验
    state = self.consistency_agent.run(state)
    
    # 步骤7: 内容整理
    state = self.organization_agent.run(state)
    
    # 构建返回结果
    return DeepAnalysisResult(...)
```

**与完整流水线的区别**:

- 不执行全局结构解析和全局知识点聚类
- 可以基于已有的全局分析结果进行增强分析
- 更适合逐页分析的场景

## 3. 全局分析功能实现

### 3.1 功能设计

全局分析功能旨在分析整个文档的主题、章节结构和知识点框架，为单页分析提供上下文信息。

### 3.2 实现架构

#### 3.2.1 API 端点

**位置**: `src/app.py` - `POST /api/v1/analyze-document-global`

**请求处理流程**:

1. 接收请求，包含 `doc_id` 和可选的 `force` 参数
2. 从数据库加载文档
3. 检查是否已有全局分析结果（除非 `force=true`）
4. 提取所有页面的文本内容
5. 执行全局分析
6. 保存分析结果到数据库
7. 返回分析结果

#### 3.2.2 分析流程

**步骤 1: 全局结构解析**

```python
# 初始化状态
state = {
    "ppt_texts": ppt_texts,
    "global_outline": {},
    # ... 其他字段
}

# 调用 GlobalStructureAgent
state = service.structure_agent.run(state)
global_outline = state.get("global_outline", {})
```

**步骤 2: 全局知识点聚类**

```python
# 调用 KnowledgeClusteringAgent
from src.agents.base import KnowledgeClusteringAgent
clustering_agent = KnowledgeClusteringAgent(service.llm_config)
state = clustering_agent.run(state)
knowledge_units = state.get("knowledge_units", [])
```

**步骤 3: 构建结果**

```python
global_analysis = {
    "main_topic": global_outline.get("main_topic", ""),
    "chapters": global_outline.get("chapters", []),
    "knowledge_flow": global_outline.get("knowledge_flow", ""),
    "knowledge_units": [
        {
            "unit_id": unit.unit_id,
            "title": unit.title,
            "pages": unit.pages,
            "core_concepts": unit.core_concepts
        } for unit in knowledge_units
    ],
    "total_pages": len(ppt_texts)
}
```

**步骤 4: 保存结果**

```python
persistence.update_global_analysis(request.doc_id, global_analysis)
```

### 3.3 缓存机制

#### 3.3.1 缓存策略

- 全局分析结果存储在数据库的 `documents.global_analysis_json` 字段
- 如果文档已有全局分析结果，直接返回缓存（除非 `force=true`）
- 使用 `force=true` 可以强制重新分析

#### 3.3.2 缓存检查

```python
# 检查是否已有全局分析（除非强制重新分析）
if not request.force and doc.get("global_analysis"):
    return {
        "success": True,
        "global_analysis": doc["global_analysis"],
        "cached": True
    }
```

### 3.4 数据流

#### 3.4.1 全局分析数据流

```
文档上传
  ↓
解析slides
  ↓
存储到数据库（documents表）
  ↓
提取所有页面文本
  ↓
GlobalStructureAgent → global_outline
  ↓
KnowledgeClusteringAgent → knowledge_units
  ↓
构建全局分析结果
  ↓
保存到 documents.global_analysis_json
  ↓
返回结果
```

#### 3.4.2 页面分析数据流

```
用户点击分析
  ↓
加载全局分析结果（documents.global_analysis_json）
  ↓
构建state（包含global_outline和knowledge_units）
  ↓
PageKnowledgeClusterer（使用全局上下文）
  ↓
StructureUnderstandingAgent（使用全局上下文）
  ↓
GapIdentificationAgent（使用全局上下文）
  ↓
其他agents...
  ↓
保存到 page_analysis表
```

### 3.5 实现效果


- 先进行全局分析，建立知识框架
- 页面分析时参考全局知识框架
- 能理解页面在整个文档中的位置
- 知识点识别更准确，能识别更多知识点
- 知识缺口识别更精准，考虑前置知识

## 4. 单页分析功能实现

### 4.1 功能设计

单页分析功能对单个页面进行深度分析，包括知识聚类、理解笔记生成、知识缺口识别、内容扩展等。

### 4.2 实现架构

#### 4.2.1 API 端点

**非流式分析**: `POST /api/v1/analyze-page`

**流式分析**: `POST /api/v1/analyze-page-stream`

#### 4.2.2 分析流程

**步骤 1: 知识聚类**

```python
# 如果有全局分析，将全局知识点单元传递给聚类agent
knowledge_clusters = service.clustering_agent.run(
    request.content,
    global_context=global_analysis
)
```

**步骤 2: 学习笔记生成**

```python
state = {
    "raw_text": request.content,
    "knowledge_clusters": knowledge_clusters,
    "global_outline": global_analysis or {},
    # ... 其他字段
}
state = service.understanding_agent.run(state)
```

**步骤 3: 知识缺口识别**

```python
state = service.gap_agent.run(state)
```

**步骤 4: 知识扩展**

```python
state = service.expansion_agent.run(state)
```

**步骤 5: 外部检索**

```python
state = service.retrieval_agent.run(state)
```

**步骤 6: 一致性校验**

```python
state = service.consistency_agent.run(state)
```

**步骤 7: 内容整理**

```python
state = service.organization_agent.run(state)
```

### 4.3 流式分析实现

#### 4.3.1 Server-Sent Events (SSE)

流式分析使用 SSE 协议实时返回各阶段的分析结果。

**实现方式**:

```python
async def event_generator():
    # 步骤1: 知识聚类
    yield f"data: {json.dumps({'stage': 'clustering', 'data': knowledge_clusters, 'message': '...'})}\n\n"
    
    # 步骤2: 学习笔记
    yield f"data: {json.dumps({'stage': 'understanding', 'data': understanding_notes, 'message': '...'})}\n\n"
    
    # 步骤3: 知识缺口
    yield f"data: {json.dumps({'stage': 'gaps', 'data': knowledge_gaps, 'message': '...'})}\n\n"
    
    # ... 其他步骤
    
    # 完成
    yield f"data: {json.dumps({'stage': 'complete', 'data': final_result, 'message': '分析完成'})}\n\n"

return StreamingResponse(event_generator(), media_type="text/event-stream")
```

#### 4.3.2 事件阶段

- `clustering`: 知识聚类结果
- `understanding`: 理解笔记
- `gaps`: 知识缺口
- `expansion`: 扩展内容
- `retrieval`: 参考资料
- `complete`: 分析完成

### 4.4 全局上下文支持

#### 4.4.1 上下文加载

```python
# 获取全局分析结果（如果有）
global_analysis = None
if request.doc_id:
    doc = persistence.get_document_by_id(request.doc_id)
    if doc and doc.get("global_analysis"):
        global_analysis = doc["global_analysis"]
```

#### 4.4.2 上下文传递

各个 Agent 会根据是否有全局上下文，使用不同的 prompt：

- **有全局上下文**: 参考全局知识点框架，提供更准确的分析
- **无全局上下文**: 仅基于当前页面内容进行分析

### 4.5 缓存机制

#### 4.5.1 缓存策略

- 单页分析结果存储在 `page_analysis` 表中
- 如果页面已有分析结果，直接返回缓存（除非 `force=true`）
- 流式分析也会检查缓存，如果有缓存则直接回放

#### 4.5.2 缓存检查

```python
# 如果 force=False 且有缓存，则直接返回
if request.doc_id and not request.force:
    cached = persistence.get_page_analysis(request.doc_id, request.page_id)
    if cached:
        return {"success": True, "cached": True, "data": cached}
```

### 4.6 注意事项

#### 4.6.1 全局分析是异步的

- 前端在后台调用，不阻塞UI
- 如果全局分析未完成，页面分析仍可进行（但效果较差）

#### 4.6.2 向后兼容

- 如果没有全局分析结果，页面分析仍可进行
- 各个 Agent 会检查是否有全局上下文，没有时使用原始逻辑

#### 4.6.3 性能考虑

- 全局分析可能耗时较长（取决于文档页数）
- 建议在后台异步执行，不阻塞用户操作

## 5. 数据持久化系统实现

### 5.1 数据库设计

#### 5.1.1 数据库选择

选择 SQLite 作为持久化存储，原因：
- 轻量级，无需单独部署数据库服务
- 文件型数据库，易于备份和迁移
- 支持 JSON 字段存储复杂数据结构
- 性能满足中小规模应用需求

#### 5.1.2 表结构设计

**documents 表**:

```sql
CREATE TABLE documents (
    doc_id TEXT PRIMARY KEY,              -- 文档唯一标识
    file_name TEXT,                       -- 文件名
    file_type TEXT,                       -- 文件类型
    file_hash TEXT UNIQUE,                -- 文件哈希值（SHA256）
    slides_json TEXT,                     -- slides 数据（JSON）
    global_analysis_json TEXT,            -- 全局分析结果（JSON）
    created_at TEXT,                      -- 创建时间
    updated_at TEXT                       -- 更新时间
)
```

**设计要点**:

- `doc_id`: 使用 UUID 作为主键，确保唯一性
- `file_hash`: 使用 SHA256 哈希值，用于文件去重
- `slides_json`: 存储解析后的 slides 数据，使用 JSON 格式
- `global_analysis_json`: 存储全局分析结果，使用 JSON 格式
- 索引: 在 `file_hash` 上创建索引，加快查询速度

**page_analysis 表**:

```sql
CREATE TABLE page_analysis (
    doc_id TEXT NOT NULL,                 -- 文档ID（外键）
    page_id INTEGER NOT NULL,             -- 页面编号
    analysis_json TEXT NOT NULL,          -- 分析结果（JSON）
    created_at TEXT,                      -- 创建时间
    updated_at TEXT,                      -- 更新时间
    PRIMARY KEY (doc_id, page_id),
    FOREIGN KEY (doc_id) REFERENCES documents(doc_id) ON DELETE CASCADE
)
```

**设计要点**:

- 复合主键: `(doc_id, page_id)` 确保每个文档的每个页面只有一条分析记录
- 外键约束: 关联到 `documents` 表，删除文档时级联删除页面分析
- `analysis_json`: 存储单页分析结果，使用 JSON 格式
- 索引: 在 `doc_id` 上创建索引，加快查询速度

### 5.2 数据操作实现

#### 5.2.1 服务类设计

**类名**: `PersistenceService`

**位置**: `src/services/persistence_service.py`

**核心方法**:

- `_init_db()`: 初始化数据库和表结构
- `upsert_document()`: 插入或更新文档
- `update_global_analysis()`: 更新全局分析结果
- `get_document_by_id()`: 通过 doc_id 查询文档
- `get_document_by_hash()`: 通过 file_hash 查询文档
- `upsert_page_analysis()`: 插入或更新单页分析
- `get_page_analysis()`: 查询单页分析
- `list_page_analyses()`: 查询文档所有页面分析

#### 5.2.2 线程安全

使用 `threading.Lock` 确保数据库操作的线程安全：

```python
def __init__(self, db_path: str):
    self._lock = threading.Lock()
    # ...

def upsert_document(self, ...):
    with self._lock:
        with self._connect() as conn:
            # 数据库操作
            conn.commit()
```

#### 5.2.3 JSON 序列化

使用 `json.dumps()` 和 `json.loads()` 进行 JSON 序列化和反序列化：

```python
# 存储
slides_json = json.dumps(slides, ensure_ascii=False)
conn.execute("INSERT INTO documents (slides_json) VALUES (?)", (slides_json,))

# 读取
doc = dict(row)
doc["slides"] = json.loads(doc["slides_json"]) if doc.get("slides_json") else []
```

### 5.3 数据迁移

#### 5.3.1 自动迁移

系统会自动检测数据库表结构，如果缺少 `global_analysis_json` 字段，会自动添加：

```python
def _init_db(self):
    # 创建表
    conn.execute("CREATE TABLE IF NOT EXISTS documents (...)")
    
    # 检查是否需要迁移
    cursor = conn.execute("PRAGMA table_info(documents)")
    columns = [row[1] for row in cursor.fetchall()]
    if 'global_analysis_json' not in columns:
        conn.execute("ALTER TABLE documents ADD COLUMN global_analysis_json TEXT")
        conn.commit()
```

### 5.4 性能优化

#### 5.4.1 索引优化

- `idx_documents_file_hash`: 基于 `file_hash` 的索引，用于快速查找文档
- `idx_page_analysis_doc_id`: 基于 `doc_id` 的索引，用于快速查找文档的所有页面分析

#### 5.4.2 连接管理

使用上下文管理器确保数据库连接正确关闭：

```python
def _connect(self) -> sqlite3.Connection:
    conn = sqlite3.connect(self.db_path, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn

# 使用
with self._connect() as conn:
    # 数据库操作
    conn.commit()
```

## 6. 配置系统实现

### 6.1 配置管理设计

#### 6.1.1 配置类结构

**位置**: `src/config.py`

**配置类**:

- `LLMConfig`: LLM 配置
- `RetrievalConfig`: 检索配置
- `ExpansionConfig`: 扩展配置
- `StreamingConfig`: 流式配置
- `KnowledgeBaseConfig`: 知识库配置

#### 6.1.2 配置管理器

**类名**: `ConfigManager`

**设计模式**: 单例模式

**功能**:

- 加载配置文件（`config.json`）
- 提供类型安全的配置访问
- 支持环境变量覆盖

### 6.2 配置加载流程

#### 6.2.1 配置文件查找

按优先级查找配置文件：

1. `backend/config.json`
2. `src/config.json`
3. 当前工作目录的 `config.json`

#### 6.2.2 环境变量支持

配置管理器会自动读取环境变量：

- `OPENAI_API_KEY` → `llm.api_key`
- `OPENAI_BASE_URL` → `llm.base_url`
- `OPENAI_MODEL` → `llm.model`

### 6.3 配置使用

#### 6.3.1 获取配置

```python
from src.config import get_llm_config, get_retrieval_config

llm_config = get_llm_config()
retrieval_config = get_retrieval_config()
```

#### 6.3.2 配置传递

配置通过依赖注入传递给服务：

```python
def get_page_analysis_service():
    config = load_config()
    llm_config = LLMConfig(
        api_key=config["llm"]["api_key"],
        base_url=config["llm"]["base_url"],
        model=config["llm"]["model"]
    )
    return PageDeepAnalysisService(llm_config)
```

## 7. 系统集成

### 7.1 API 层集成

#### 7.1.1 FastAPI 应用

**位置**: `src/app.py`

**核心功能**:

- 定义 API 端点
- 处理请求和响应
- 依赖注入服务实例
- CORS 中间件配置

#### 7.1.2 端点组织

- `/api/v1/expand-ppt`: 文档上传和解析
- `/api/v1/analyze-document-global`: 全局分析
- `/api/v1/analyze-page`: 单页分析（非流式）
- `/api/v1/analyze-page-stream`: 单页分析（流式）
- `/api/v1/page-analysis`: 查询单页分析
- `/api/v1/page-analysis/all`: 查询所有页面分析

### 7.2 服务层集成

#### 7.2.1 服务依赖关系

```
PageDeepAnalysisService
  ├─→ LLMConfig
  ├─→ GlobalStructureAgent
  ├─→ PageKnowledgeClusterer
  ├─→ StructureUnderstandingAgent
  ├─→ GapIdentificationAgent
  ├─→ KnowledgeExpansionAgent
  ├─→ RetrievalAgent
  ├─→ ConsistencyCheckAgent
  └─→ StructuredOrganizationAgent

PersistenceService
  └─→ SQLite Database

PPTExpansionService
  └─→ LangGraph Workflow
      └─→ All Agents
```

#### 7.2.2 服务初始化

使用单例模式管理服务实例：

```python
_page_analysis_service = None

def get_page_analysis_service():
    global _page_analysis_service
    if _page_analysis_service is None:
        config = load_config()
        llm_config = LLMConfig(...)
        _page_analysis_service = PageDeepAnalysisService(llm_config)
    return _page_analysis_service
```

## 8. 性能优化

### 8.1 缓存策略

#### 8.1.1 分析结果缓存

- 全局分析结果缓存在数据库
- 单页分析结果缓存在数据库
- 避免重复分析，提高响应速度

#### 8.1.2 缓存失效

- 使用 `force=true` 参数强制重新分析
- 删除文档时自动删除相关分析结果（级联删除）

### 8.2 流式输出

#### 8.2.1 实时反馈

使用 SSE 协议实时返回各阶段的分析结果，提升用户体验。

#### 8.2.2 减少等待时间

用户无需等待所有分析完成，可以提前查看部分结果。

### 8.3 文本处理优化

#### 8.3.1 文本摘要

对于长文档（>20页），自动进行文本摘要，减少 token 消耗。

#### 8.3.2 输入限制

各个 Agent 对输入文本长度进行限制，避免超出模型上下文窗口。

### 8.4 Token 消耗优化

#### 8.4.1 输入截断

- 全局分析: 每页 200 字摘要
- 单页分析: 1000-1500 字限制
- 检索查询: 800 字限制

#### 8.4.2 模型温度设置

- 结构化任务: 0（确定性）
- 生成任务: 0.5-0.6（创意平衡）
- 创意任务: 0.3-0.5（相对保守）

#### 8.4.3 并行处理

- 知识聚类和笔记生成可并行
- 多页面分析支持批处理

### 8.5 网络请求优化

#### 8.5.1 源可用性缓存

- 初始化时测试一次
- 如果所有源不可用，立即跳过检索
- 不浪费时间在失败的查询上

#### 8.5.2 早期退出策略

- 仅为高优先级缺口检索（priority ≥ 4）
- 本地 RAG 足够时跳过外部检索
- 获得足够结果后停止查询

#### 8.5.3 结果过滤

- 去除占位符文档（"未找到..."）
- 仅保留有有效 URL 的结果
- 去重处理

## 9. 错误处理

### 9.1 异常处理策略

#### 9.1.1 Agent 级别

每个 Agent 的 `run()` 方法都包含 try-except 块，捕获异常并返回默认值：

```python
try:
    state = self.agent.run(state)
except Exception as e:
    print(f"Agent 执行失败: {e}")
    # 返回默认值，不中断流程
    state["result"] = []
```

#### 9.1.2 API 级别

API 端点使用 FastAPI 的异常处理机制：

```python
try:
    result = service.analyze_page(...)
    return {"success": True, "data": result}
except HTTPException:
    raise
except Exception as e:
    raise HTTPException(status_code=500, detail=f"分析失败: {str(e)}")
```

### 9.2 日志记录

#### 9.2.1 日志级别

- INFO: 正常流程信息
- WARNING: 警告信息（如缓存未命中）
- ERROR: 错误信息（如 Agent 执行失败）

#### 9.2.2 日志内容

- Agent 执行开始和结束
- LLM 调用信息
- 缓存命中情况
- 错误堆栈信息

## 10. 总结

### 10.1 技术亮点

1. **多 Agent 协作**: 基于 LangGraph 实现复杂的多 Agent 协作流程
2. **全局上下文支持**: 单页分析可以基于全局分析结果，提供更准确的上下文
3. **流式输出**: 使用 SSE 协议实时返回分析结果，提升用户体验
4. **数据持久化**: 使用 SQLite 数据库持久化存储分析结果，支持缓存和查询
5. **配置管理**: 灵活的配置系统，支持配置文件和环境变量

### 10.2 系统优势

1. **模块化设计**: 各个组件职责清晰，易于维护和扩展
2. **可扩展性**: 易于添加新的 Agent 或知识源
3. **性能优化**: 缓存机制和流式输出提升系统性能
4. **错误处理**: 完善的异常处理和日志记录

### 10.3 未来改进方向

1. **模型优化**:
   - 微调模型用于教育领域
   - 针对特定学科的专化版本

2. **缓存机制**:
   - 缓存常见概念的补充说明
   - 缓存已检索的参考资料

3. **用户反馈**:
   - 收集读者对笔记有用性的反馈
   - 根据反馈调整 Agent 参数

4.  **分布式部署**: 支持多实例部署，提高并发处理能力

5. **更多知识源**: 集成更多外部知识源，提供更丰富的参考资料

---
