# AI Agent LLM 流水线系统使用文档

## 概述

以下为 AI Agent LLM 流水线、全局分析、单页分析以及数据持久化（pptas_cache.sqlite3）的使用方法和配置说明。

## 目录

1. [系统架构](#系统架构)
2. [AI Agent LLM 流水线](#ai-agent-llm-流水线)
3. [全局分析功能](#全局分析功能)
4. [单页分析功能](#单页分析功能)
5. [数据持久化（pptas_cache.sqlite3）](#数据持久化pptas_cachesqlite3)
6. [配置说明](#配置说明)
7. [API 接口说明](#api-接口说明)
8. [使用步骤教程](#使用步骤教程)
9. [常见问题](#常见问题)

---

## 系统架构

### 整体流程

```
文档上传
  ↓
文档解析（提取文本内容）
  ↓
全局分析（可选，建议先执行）
  ├─→ GlobalStructureAgent（全局结构解析）
  └─→ KnowledgeClusteringAgent（全局知识点聚类）
  ↓
单页分析（逐页执行）
  ├─→ PageKnowledgeClusterer（知识聚类）
  ├─→ StructureUnderstandingAgent（理解笔记生成）
  ├─→ GapIdentificationAgent（知识缺口识别）
  ├─→ KnowledgeExpansionAgent（知识扩展）
  ├─→ RetrievalAgent（外部检索）
  ├─→ ConsistencyCheckAgent（一致性校验）
  └─→ StructuredOrganizationAgent（内容整理）
  ↓
结果持久化到 pptas_cache.sqlite3
```

### 核心组件

- **Agent 层** (`src/agents/base.py`): 包含 8 个核心 Agent 实现
- **服务层** (`src/services/`): 包含页面分析服务、持久化服务等
- **数据层** (`pptas_cache.sqlite3`): SQLite 数据库，存储文档和分析结果

---
## 配置说明

### 配置文件位置

- **主配置文件**: `backend/config.json`
- **配置管理类**: `src/config.py` - `ConfigManager`

### 配置结构

```json
{
  "llm": {
    "api_key": "your-api-key",
    "base_url": "https://api.openai.com/v1",
    "model": "gpt-4"
  },
  "retrieval": {
    "preferred_sources": ["arxiv", "wikipedia"],
    "max_results": 3,
    "local_rag_priority": true
  },
  "expansion": {
    "max_revisions": 2,
    "min_gap_priority": 3,
    "temperature": 0.7
  },
  "streaming": {
    "enabled": true,
    "chunk_size": 50
  },
  "knowledge_base": {
    "path": "./knowledge_base",
    "chunk_size": 1000,
    "chunk_overlap": 200
  },
  "vector_store": {
    "path": "./ppt_vector_db",
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "embedding_model": "BAAI/bge-m3"
  }
}
```

### 配置项说明

#### LLM 配置

- `llm.api_key`: LLM API 密钥（必需）
- `llm.base_url`: LLM API 基础 URL（必需）
- `llm.model`: 使用的模型名称（必需）

**示例**:

```json
{
  "llm": {
    "api_key": "sk-xxx",
    "base_url": "https://api.siliconflow.cn/v1",
    "model": "deepseek-ai/DeepSeek-V3.2-Exp"
  }
}
```

#### 检索配置

- `retrieval.preferred_sources`: 优先使用的知识源列表
- `retrieval.max_results`: 每个知识源的最大结果数
- `retrieval.local_rag_priority`: 是否优先使用本地 RAG

**示例**:

```json
{
  "retrieval": {
    "preferred_sources": ["arxiv", "wikipedia", "baidu_baike"],
    "max_results": 5,
    "local_rag_priority": true
  }
}
```

#### 扩展配置

- `expansion.max_revisions`: 最大修订次数（一致性校验失败时）
- `expansion.min_gap_priority`: 最小缺口优先级（低于此优先级的缺口会被忽略）
- `expansion.temperature`: LLM 温度参数

**示例**:

```json
{
  "expansion": {
    "max_revisions": 3,
    "min_gap_priority": 2,
    "temperature": 0.5
  }
}
```

#### 流式配置

- `streaming.enabled`: 是否启用流式输出
- `streaming.chunk_size`: 流式输出的块大小

**示例**:

```json
{
  "streaming": {
    "enabled": true,
    "chunk_size": 100
  }
}
```

#### 知识库配置

- `knowledge_base.path`: 本地知识库路径
- `knowledge_base.chunk_size`: 文档分块大小
- `knowledge_base.chunk_overlap`: 分块重叠大小

**示例**:

```json
{
  "knowledge_base": {
    "path": "./knowledge_base",
    "chunk_size": 1500,
    "chunk_overlap": 300
  }
}
```

#### 向量存储配置

- `vector_store.path`: 向量数据库路径
- `vector_store.chunk_size`: 向量化分块大小
- `vector_store.chunk_overlap`: 分块重叠大小
- `vector_store.embedding_model`: 嵌入模型名称

**示例**:

```json
{
  "vector_store": {
    "path": "./ppt_vector_db",
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "embedding_model": "BAAI/bge-m3"
  }
}
```

### 环境变量配置

除了配置文件，也可以通过环境变量设置：

```bash
export OPENAI_API_KEY="your-api-key"
export OPENAI_BASE_URL="https://api.openai.com/v1"
export OPENAI_MODEL="gpt-4"
```

### 配置加载优先级

1. 环境变量（最高优先级）
2. `config.json` 文件
3. 默认值（最低优先级）

---


## AI Agent LLM 流水线

### 流水线概述

AI Agent LLM 流水线是一个基于 LangGraph 的多 Agent 协作系统，用于对 PPT 文档进行智能分析和扩展。

### 核心 Agent 列表

#### 1. GlobalStructureAgent（全局结构解析）

**功能**: 分析整个文档的主题、章节结构和知识逻辑流程

**位置**: `src/agents/base.py`

**输入**:
- `state["ppt_texts"]`: 所有页面的文本内容列表

**输出**:
- `state["global_outline"]`: 包含主题、章节、知识流程的字典

**配置参数**:
- 温度: 0（确定性输出）
- 输入限制: 超过 20 页时自动摘要，每页 200 字摘要

#### 2. KnowledgeClusteringAgent（全局知识点聚类）

**功能**: 将文档内容聚类为知识点单元

**位置**: `src/agents/base.py`

**输入**:
- `state["global_outline"]`: 全局结构信息
- `state["ppt_texts"]`: 所有页面文本

**输出**:
- `state["knowledge_units"]`: 知识点单元列表

#### 3. PageKnowledgeClusterer（单页知识聚类）

**功能**: 识别单页中难以理解的概念

**位置**: `src/services/page_analysis_service.py`

**输入**:
- `raw_text`: 页面原始文本
- `global_context`: 可选的全局分析结果

**输出**:
- 概念列表，包含难度级别、原因说明等

**配置参数**:
- 温度: 0.3（有创意但相对稳定）
- 最多 10 个概念，避免信息过载
- 难度分级: 1（简单）到 5（很难）

#### 4. StructureUnderstandingAgent（结构理解）

**功能**: 生成结构化学习笔记

**位置**: `src/agents/base.py`

**输入**:
- `state["raw_text"]`: 页面原始文本
- `state["global_outline"]`: 全局上下文（可选）

**输出**:
- `state["page_structure"]`: 页面结构信息
- `state["understanding_notes"]`: Markdown 格式学习笔记

**配置参数**:
- 温度: 0.5（平衡创意和准确性）
- 输入限制: 1000 字 + 800 字
- 笔记长度限制: 300 字

#### 5. GapIdentificationAgent（知识缺口识别）

**功能**: 识别学生理解障碍点

**位置**: `src/agents/base.py`

**输入**:
- `state["raw_text"]`: 页面原始文本
- `state["global_outline"]`: 全局上下文（可选）

**输出**:
- `state["knowledge_gaps"]`: 知识缺口列表

**缺口类型**:
- `直观解释`: 需要通俗易懂的解释
- `应用示例`: 需要实际应用场景
- `背景知识`: 需要前置知识补充
- `公式推导`: 需要数学推导过程

**配置参数**:
- 温度: 0.2（确定性，聚焦实际问题）
- 最多 5 个缺口，避免过度分析
- 优先级: 1（可选）到 5（必须）

#### 6. KnowledgeExpansionAgent（知识扩展）

**功能**: 为识别的知识缺口生成补充说明

**位置**: `src/agents/base.py`

**输入**:
- `state["knowledge_gaps"]`: 知识缺口列表

**输出**:
- `state["expanded_content"]`: 扩展内容列表

**配置参数**:
- 温度: 0.6（更多创意用于举例）
- 最多处理 3 个缺口（按优先级排序）
- 输出限制: 150 字 + 300 字最终长度
- 输入: 500 字内容

#### 7. RetrievalAgent（外部检索）

**功能**: 从外部知识源检索参考资料

**位置**: `src/agents/base.py`

**输入**:
- `state["knowledge_gaps"]`: 知识缺口列表

**输出**:
- `state["retrieved_docs"]`: 检索到的文档列表

**检索策略**:
1. **智能源检测**: 初始化时测试外部源连通性
2. **优先本地 RAG**: 先从向量数据库检索
3. **外部检索**: 仅当本地结果不足且源可用时查询
4. **早期退出**: 所有源都不可用时立即返回

**可用检索源**:
- `arxiv`: 学术论文（最优先）
- `wikipedia`: 百科知识（中英文）
- `baidu_baike`: 百度百科（中文优先）
- 本地向量数据库

**配置参数**:
- 温度: 0（确定性检索）
- 最多 5 条检索结果
- 只为高优先级缺口（priority ≥ 4）检索
- 合并查询以减少检索次数

#### 8. ConsistencyCheckAgent（一致性校验）

**功能**: 校验扩展内容与原始内容的一致性

**位置**: `src/agents/base.py`

**输入**:
- `state["expanded_content"]`: 扩展内容
- `state["raw_text"]`: 原始文本

**输出**:
- `state["check_result"]`: 校验结果

**校验规则**:
1. **禁止编造**: 不能提及 PPT 未涉及的新概念
2. **有据可查**: 所有陈述必须来自 PPT 或参考资料
3. **标记推测**: 不确定的内容标记为"推测"
4. **发现矛盾**: 与 PPT 或参考资料矛盾时标记为修正

**配置参数**:
- 温度: 0（严格检查，无创意）
- 输入限制: PPT 600 字 + 参考 3 条

#### 9. StructuredOrganizationAgent（内容整理）

**功能**: 将分析结果整理为最终格式

**位置**: `src/agents/base.py`

**输入**:
- `state`: 包含所有分析结果的状态

**输出**:
- `state["final_notes"]`: 最终整理的学习笔记

**笔记格式**:
- 标题明确
- 核心概念优先
- 避免重复原文
- 适合快速复习
- 严格控制长度（300 字内）

**配置参数**:
- 温度: 0.5（平衡）
- 输出限制: 300 字内
- 输入: 500 字内容 + 补充说明

### 流水线执行方式

#### 方式一：完整流水线（PPTExpansionService）

使用 `PPTExpansionService` 执行完整的 LangGraph 工作流：

```python
from src.services.ppt_expansion_service import PPTExpansionService
from src.agents.base import LLMConfig

# 初始化配置
llm_config = LLMConfig(
    api_key="your-api-key",
    base_url="https://api.openai.com/v1",
    model="gpt-4"
)

# 创建服务
service = PPTExpansionService(llm_config)

# 执行流水线
ppt_texts = ["第1页内容", "第2页内容", ...]
result = service.run(ppt_texts, max_revisions=2)
```

#### 方式二：单页分析（PageDeepAnalysisService）

使用 `PageDeepAnalysisService` 对单个页面进行分析：

```python
from src.services.page_analysis_service import PageDeepAnalysisService
from src.agents.base import LLMConfig

# 初始化配置
llm_config = LLMConfig(
    api_key="your-api-key",
    base_url="https://api.openai.com/v1",
    model="gpt-4"
)

# 创建服务
service = PageDeepAnalysisService(llm_config)

# 分析单页
result = service.analyze_page(
    page_id=1,
    title="页面标题",
    content="页面内容",
    raw_points=None
)
```

---

## 全局分析功能

### 功能概述

全局分析功能用于分析整个文档的主题、章节结构和知识点框架，为单页分析提供上下文信息。

### 实现位置

- **API 端点**: `src/app.py` - `POST /api/v1/analyze-document-global`
- **Agent**: `src/agents/base.py` - `GlobalStructureAgent`, `KnowledgeClusteringAgent`
- **服务**: `src/services/page_analysis_service.py` - `PageDeepAnalysisService`

### 分析流程

1. **文档加载**: 从数据库加载文档的所有 slides
2. **文本提取**: 提取所有页面的文本内容
3. **全局结构解析**: 调用 `GlobalStructureAgent` 提取主题和章节结构
4. **知识点聚类**: 调用 `KnowledgeClusteringAgent` 提取知识点单元
5. **结果保存**: 将分析结果保存到数据库的 `global_analysis_json` 字段

### 输出数据结构

```json
{
  "main_topic": "文档核心主题",
  "chapters": [
    {
      "title": "章节标题",
      "pages": [1, 2, 3],
      "key_concepts": ["概念1", "概念2"]
    }
  ],
  "knowledge_flow": "知识逻辑流程描述",
  "knowledge_units": [
    {
      "unit_id": "unit_1",
      "title": "知识点单元标题",
      "pages": [1, 2],
      "core_concepts": ["核心概念1", "核心概念2"]
    }
  ],
  "total_pages": 10
}
```

### 缓存机制

- 全局分析结果会缓存在数据库中
- 如果文档已有全局分析结果，直接返回缓存（除非 `force=true`）
- 使用 `force=true` 可以强制重新分析

### 使用示例

#### 通过 API 调用

```bash
curl -X POST "http://localhost:8000/api/v1/analyze-document-global" \
  -H "Content-Type: application/json" \
  -d '{
    "doc_id": "your-doc-id",
    "force": false
  }'
```

#### 通过 Python 代码

```python
from src.services.persistence_service import PersistenceService
from src.services.page_analysis_service import PageDeepAnalysisService
from src.agents.base import LLMConfig

# 初始化服务
persistence = PersistenceService("backend/pptas_cache.sqlite3")
llm_config = LLMConfig(
    api_key="your-api-key",
    base_url="https://api.openai.com/v1",
    model="gpt-4"
)
service = PageDeepAnalysisService(llm_config)

# 获取文档
doc = persistence.get_document_by_id("your-doc-id")

# 提取所有页面文本
ppt_texts = []
for slide in doc["slides"]:
    content_parts = []
    if slide.get("title"):
        content_parts.append(f"标题: {slide['title']}")
    if slide.get("raw_content"):
        content_parts.append(slide["raw_content"])
    ppt_texts.append("\n".join(content_parts))

# 执行全局分析
state = {
    "ppt_texts": ppt_texts,
    "global_outline": {},
    "knowledge_units": [],
    # ... 其他字段
}

# 步骤1: 全局结构解析
state = service.structure_agent.run(state)

# 步骤2: 全局知识点聚类
from src.agents.base import KnowledgeClusteringAgent
clustering_agent = KnowledgeClusteringAgent(llm_config)
state = clustering_agent.run(state)

# 构建结果
global_analysis = {
    "main_topic": state["global_outline"].get("main_topic", ""),
    "chapters": state["global_outline"].get("chapters", []),
    "knowledge_flow": state["global_outline"].get("knowledge_flow", ""),
    "knowledge_units": [
        {
            "unit_id": unit.unit_id,
            "title": unit.title,
            "pages": unit.pages,
            "core_concepts": unit.core_concepts
        } for unit in state["knowledge_units"]
    ],
    "total_pages": len(ppt_texts)
}

# 保存结果
persistence.update_global_analysis("your-doc-id", global_analysis)
```

---

## 单页分析功能

### 功能概述

单页分析功能对单个页面进行深度分析，包括知识聚类、理解笔记生成、知识缺口识别、内容扩展等。

### 实现位置

- **API 端点**: `src/app.py` - `POST /api/v1/analyze-page`, `POST /api/v1/analyze-page-stream`
- **服务**: `src/services/page_analysis_service.py` - `PageDeepAnalysisService`

### 分析流程

1. **知识聚类**: 识别页面中难以理解的概念
2. **理解笔记生成**: 生成结构化学习笔记
3. **知识缺口识别**: 识别学生理解障碍点
4. **知识扩展**: 为缺口生成补充说明
5. **外部检索**: 从网络检索参考资料
6. **一致性校验**: 校验扩展内容的准确性
7. **内容整理**: 整理为最终格式

### 输出数据结构

```json
{
  "page_id": 1,
  "title": "页面标题",
  "raw_content": "原始内容",
  "page_structure": {
    "page_id": 1,
    "title": "页面标题",
    "main_concepts": ["概念1", "概念2"],
    "key_points": ["要点1", "要点2"]
  },
  "knowledge_clusters": [
    {
      "concept": "概念名称",
      "difficulty_level": 3,
      "why_difficult": "为什么难理解",
      "related_concepts": ["相关概念"]
    }
  ],
  "understanding_notes": "Markdown 格式的学习笔记",
  "knowledge_gaps": [
    {
      "concept": "需要补充的概念",
      "gap_types": ["直观解释"],
      "priority": 4
    }
  ],
  "expanded_content": [
    {
      "concept": "概念",
      "gap_type": "直观解释",
      "content": "补充说明内容",
      "sources": ["来源1", "来源2"]
    }
  ],
  "references": [
    {
      "title": "参考资料标题",
      "url": "https://example.com",
      "source": "wikipedia",
      "snippet": "内容摘要"
    }
  ]
}
```

### 流式分析

系统支持流式分析，实时返回各阶段的分析结果：

- `stage: 'clustering'`: 知识聚类结果
- `stage: 'understanding'`: 理解笔记
- `stage: 'gaps'`: 知识缺口
- `stage: 'expansion'`: 扩展内容
- `stage: 'retrieval'`: 参考资料
- `stage: 'complete'`: 分析完成

### 缓存机制

- 单页分析结果会缓存在 `page_analysis` 表中
- 如果页面已有分析结果，直接返回缓存（除非 `force=true`）
- 使用 `force=true` 可以强制重新分析

### 全局上下文支持

单页分析可以基于全局分析结果进行，提供更准确的上下文：

- 如果文档有全局分析结果，会自动加载并传递给各个 Agent
- Agent 会参考全局知识点框架，提供更准确的分析

### 使用示例

#### 通过 API 调用（非流式）

```bash
curl -X POST "http://localhost:8000/api/v1/analyze-page" \
  -H "Content-Type: application/json" \
  -d '{
    "doc_id": "your-doc-id",
    "page_id": 1,
    "title": "页面标题",
    "content": "页面内容",
    "raw_points": [],
    "force": false
  }'
```

#### 通过 API 调用（流式）

```bash
curl -X POST "http://localhost:8000/api/v1/analyze-page-stream" \
  -H "Content-Type: application/json" \
  -d '{
    "doc_id": "your-doc-id",
    "page_id": 1,
    "title": "页面标题",
    "content": "页面内容",
    "raw_points": [],
    "force": false
  }'
```

#### 通过 Python 代码

```python
from src.services.page_analysis_service import PageDeepAnalysisService
from src.agents.base import LLMConfig

# 初始化服务
llm_config = LLMConfig(
    api_key="your-api-key",
    base_url="https://api.openai.com/v1",
    model="gpt-4"
)
service = PageDeepAnalysisService(llm_config)

# 分析单页
result = service.analyze_page(
    page_id=1,
    title="页面标题",
    content="页面内容",
    raw_points=None
)

# 访问结果
print(f"页面ID: {result.page_id}")
print(f"标题: {result.title}")
print(f"理解笔记: {result.understanding_notes}")
print(f"知识缺口数量: {len(result.knowledge_gaps)}")
print(f"扩展内容数量: {len(result.expanded_content)}")
```

---

## 数据持久化（pptas_cache.sqlite3）

### 数据库概述

`pptas_cache.sqlite3` 是一个 SQLite 数据库，用于持久化存储文档和分析结果。

### 数据库位置

- **路径**: `backend/pptas_cache.sqlite3`
- **服务类**: `src/services/persistence_service.py` - `PersistenceService`

### 数据库结构

#### 1. documents 表

存储文档基本信息和分析结果。

**表结构**:

```sql
CREATE TABLE documents (
    doc_id TEXT PRIMARY KEY,              -- 文档唯一标识
    file_name TEXT,                       -- 文件名
    file_type TEXT,                       -- 文件类型（pptx/pdf）
    file_hash TEXT UNIQUE,                -- 文件哈希值（SHA256）
    slides_json TEXT,                     -- slides 数据（JSON 字符串）
    global_analysis_json TEXT,            -- 全局分析结果（JSON 字符串）
    created_at TEXT,                      -- 创建时间
    updated_at TEXT                       -- 更新时间
)
```

**字段说明**:

- `doc_id`: 文档唯一标识，由系统生成（UUID）
- `file_name`: 原始文件名
- `file_type`: 文件类型，如 "pptx" 或 "pdf"
- `file_hash`: 文件内容的 SHA256 哈希值，用于去重
- `slides_json`: 解析后的 slides 数据，JSON 格式
- `global_analysis_json`: 全局分析结果，JSON 格式
- `created_at`: 文档创建时间（ISO 格式）
- `updated_at`: 文档最后更新时间（ISO 格式）

**索引**:

- `idx_documents_file_hash`: 基于 `file_hash` 的索引，用于快速查找

#### 2. page_analysis 表

存储单页分析结果。

**表结构**:

```sql
CREATE TABLE page_analysis (
    doc_id TEXT NOT NULL,                 -- 文档ID（外键）
    page_id INTEGER NOT NULL,             -- 页面编号（从1开始）
    analysis_json TEXT NOT NULL,          -- 分析结果（JSON 字符串）
    created_at TEXT,                      -- 创建时间
    updated_at TEXT,                      -- 更新时间
    PRIMARY KEY (doc_id, page_id),
    FOREIGN KEY (doc_id) REFERENCES documents(doc_id) ON DELETE CASCADE
)
```

**字段说明**:

- `doc_id`: 文档ID，关联到 `documents` 表
- `page_id`: 页面编号，从 1 开始
- `analysis_json`: 单页分析结果，JSON 格式
- `created_at`: 分析结果创建时间
- `updated_at`: 分析结果最后更新时间

**索引**:

- `idx_page_analysis_doc_id`: 基于 `doc_id` 的索引，用于快速查找文档的所有页面分析

### 数据操作

#### 初始化数据库

数据库会在首次使用 `PersistenceService` 时自动创建：

```python
from src.services.persistence_service import PersistenceService

persistence = PersistenceService("backend/pptas_cache.sqlite3")
# 数据库和表会自动创建
```

#### 保存文档

```python
persistence.upsert_document(
    doc_id="doc-123",
    file_name="presentation.pptx",
    file_type="pptx",
    file_hash="abc123...",
    slides=[
        {
            "title": "第1页",
            "raw_content": "内容...",
            "raw_points": []
        }
    ],
    global_analysis=None  # 可选
)
```

#### 更新全局分析

```python
global_analysis = {
    "main_topic": "主题",
    "chapters": [...],
    "knowledge_units": [...]
}

persistence.update_global_analysis("doc-123", global_analysis)
```

#### 保存单页分析

```python
analysis_result = {
    "page_id": 1,
    "title": "页面标题",
    "understanding_notes": "...",
    "knowledge_gaps": [...],
    "expanded_content": [...]
}

persistence.upsert_page_analysis("doc-123", 1, analysis_result)
```

#### 查询文档

```python
# 通过 doc_id 查询
doc = persistence.get_document_by_id("doc-123")

# 通过 file_hash 查询
doc = persistence.get_document_by_hash("abc123...")

# 获取文档的全局分析
if doc and doc.get("global_analysis"):
    global_analysis = doc["global_analysis"]
```

#### 查询单页分析

```python
# 获取单个页面的分析
analysis = persistence.get_page_analysis("doc-123", 1)

# 获取文档所有页面的分析
all_analyses = persistence.list_page_analyses("doc-123")
# 返回: {1: {...}, 2: {...}, ...}
```

### 数据迁移

如果数据库表已存在但没有 `global_analysis_json` 字段，系统会自动添加：

```python
# 在 _init_db 方法中自动执行
ALTER TABLE documents ADD COLUMN global_analysis_json TEXT
```

### 数据备份

建议定期备份数据库文件：

```bash
# 备份数据库
cp backend/pptas_cache.sqlite3 backend/pptas_cache.sqlite3.backup

# 恢复数据库
cp backend/pptas_cache.sqlite3.backup backend/pptas_cache.sqlite3
```

### 数据查询示例

使用 SQLite 命令行工具查询数据：

```bash
# 打开数据库
sqlite3 backend/pptas_cache.sqlite3

# 查看所有文档
SELECT doc_id, file_name, file_type, created_at FROM documents;

# 查看文档的全局分析
SELECT doc_id, json_extract(global_analysis_json, '$.main_topic') as main_topic 
FROM documents 
WHERE global_analysis_json IS NOT NULL;

# 查看特定文档的全局分析（格式化JSON）
SELECT json_extract(global_analysis_json, '$.main_topic') as main_topic,
       json_array_length(global_analysis_json, '$.knowledge_units') as knowledge_count
FROM documents
WHERE doc_id = 'your-doc-id';

# 查看所有文档的全局分析状态
SELECT doc_id, file_name, 
       CASE 
         WHEN global_analysis_json IS NULL THEN 'NULL'
         ELSE '有数据'
       END as has_analysis
FROM documents;

# 查看某个文档的所有页面分析
SELECT page_id, json_extract(analysis_json, '$.title') as title 
FROM page_analysis 
WHERE doc_id = 'your-doc-id';

# 查看分析结果的数量
SELECT COUNT(*) FROM page_analysis WHERE doc_id = 'your-doc-id';

# 查看分析结果的时间信息
SELECT page_id, created_at, updated_at 
FROM page_analysis 
WHERE doc_id = 'your-doc-id';
```

### 调试技巧

#### Python 调试脚本

```python
# debug_global_analysis.py
import sqlite3
import json
from src.services.persistence_service import PersistenceService

persistence = PersistenceService("backend/pptas_cache.sqlite3")

# 获取所有文档
conn = persistence._connect()
cursor = conn.execute("SELECT doc_id, file_name, global_analysis_json FROM documents")
rows = cursor.fetchall()

for row in rows:
    doc_id = row['doc_id']
    file_name = row['file_name']
    global_analysis_json = row['global_analysis_json']
    
    if global_analysis_json:
        global_analysis = json.loads(global_analysis_json)
        print(f"\n文档: {file_name} ({doc_id})")
        print(f"  主题: {global_analysis.get('main_topic', '未知')}")
        print(f"  知识点数量: {len(global_analysis.get('knowledge_units', []))}")
    else:
        print(f"\n文档: {file_name} ({doc_id}) - 未进行全局分析")

conn.close()
```

---

## API 接口说明

### 全局分析接口

#### POST /api/v1/analyze-document-global

对整个文档进行全局分析。

**请求体**:

```json
{
  "doc_id": "your-doc-id",
  "force": false
}
```

**参数说明**:

- `doc_id` (string, 必需): 文档ID
- `force` (boolean, 可选): 是否强制重新分析，忽略缓存（默认: false）

**响应**:

```json
{
  "success": true,
  "doc_id": "your-doc-id",
  "global_analysis": {
    "main_topic": "文档主题",
    "chapters": [...],
    "knowledge_flow": "知识流程",
    "knowledge_units": [...],
    "total_pages": 10
  },
  "cached": false
}
```

### 单页分析接口

#### POST /api/v1/analyze-page

对单个页面进行深度分析（非流式）。

**请求体**:

```json
{
  "doc_id": "your-doc-id",
  "page_id": 1,
  "title": "页面标题",
  "content": "页面内容",
  "raw_points": [],
  "force": false
}
```

**参数说明**:

- `doc_id` (string, 可选): 文档ID，用于缓存
- `page_id` (integer, 必需): 页面编号（从1开始）
- `title` (string, 必需): 页面标题
- `content` (string, 必需): 页面内容
- `raw_points` (array, 可选): 原始要点列表
- `force` (boolean, 可选): 是否强制重新分析（默认: false）

**响应**:

```json
{
  "success": true,
  "cached": false,
  "data": {
    "page_id": 1,
    "title": "页面标题",
    "raw_content": "原始内容",
    "page_structure": {...},
    "knowledge_clusters": [...],
    "understanding_notes": "...",
    "knowledge_gaps": [...],
    "expanded_content": [...],
    "references": [...]
  }
}
```

#### POST /api/v1/analyze-page-stream

对单个页面进行流式深度分析。

**请求体**: 同 `/api/v1/analyze-page`

**响应**: Server-Sent Events (SSE) 流

**事件格式**:

```
data: {"stage": "clustering", "data": [...], "message": "正在分析难点概念..."}

data: {"stage": "understanding", "data": "...", "message": "正在生成学习笔记..."}

data: {"stage": "gaps", "data": [...], "message": "正在识别知识缺口..."}

data: {"stage": "expansion", "data": [...], "message": "正在生成补充说明..."}

data: {"stage": "retrieval", "data": [...], "message": "正在检索参考资料..."}

data: {"stage": "complete", "data": {...}, "message": "分析完成"}
```

### 查询接口

#### GET /api/v1/page-analysis

获取单页历史分析结果。

**查询参数**:

- `doc_id` (string, 必需): 文档ID
- `page_id` (integer, 必需): 页面编号

**响应**:

```json
{
  "success": true,
  "data": {
    "page_id": 1,
    "title": "页面标题",
    "understanding_notes": "...",
    "knowledge_gaps": [...],
    "_meta": {
      "created_at": "2024-01-01T00:00:00",
      "updated_at": "2024-01-01T00:00:00"
    }
  }
}
```

#### GET /api/v1/page-analysis/all

获取文档所有已保存的页分析。

**查询参数**:

- `doc_id` (string, 必需): 文档ID

**响应**:

```json
{
  "success": true,
  "data": {
    "1": {
      "page_id": 1,
      "title": "第1页",
      ...
    },
    "2": {
      "page_id": 2,
      "title": "第2页",
      ...
    }
  }
}
```

---

## 使用步骤教程

### 步骤 1: 环境准备

#### 1.1 安装依赖

```bash
cd backend
pip install -r requirements.txt
```

#### 1.2 配置 LLM

编辑 `backend/config.json`:

```json
{
  "llm": {
    "api_key": "your-api-key",
    "base_url": "https://api.openai.com/v1",
    "model": "gpt-4"
  }
}
```

### 步骤 2: 启动后端服务

```bash
cd backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### 步骤 3: 上传文档

#### 3.1 通过 API 上传

```bash
curl -X POST "http://localhost:8000/api/v1/expand-ppt" \
  -F "file=@presentation.pptx"
```

**响应**:

```json
{
  "doc_id": "abc123-def456-...",
  "file_hash": "sha256-hash",
  "slides": [
    {
      "title": "第1页",
      "raw_content": "内容...",
      "raw_points": []
    }
  ]
}
```

**保存 `doc_id`，后续步骤需要使用。**

### 步骤 4: 执行全局分析

#### 4.1 调用全局分析接口

```bash
curl -X POST "http://localhost:8000/api/v1/analyze-document-global" \
  -H "Content-Type: application/json" \
  -d '{
    "doc_id": "abc123-def456-...",
    "force": false
  }'
```

#### 4.2 查看结果

响应中包含全局分析结果：

```json
{
  "success": true,
  "global_analysis": {
    "main_topic": "文档主题",
    "chapters": [...],
    "knowledge_units": [...]
  }
}
```

**注意**: 全局分析结果会自动保存到数据库，后续单页分析会自动使用。

### 步骤 5: 执行单页分析

#### 5.1 非流式分析

```bash
curl -X POST "http://localhost:8000/api/v1/analyze-page" \
  -H "Content-Type: application/json" \
  -d '{
    "doc_id": "abc123-def456-...",
    "page_id": 1,
    "title": "第1页标题",
    "content": "第1页内容",
    "raw_points": [],
    "force": false
  }'
```

#### 5.2 流式分析（推荐）

使用支持 SSE 的客户端（如浏览器或专门的工具）：

```javascript
const eventSource = new EventSource(
  'http://localhost:8000/api/v1/analyze-page-stream',
  {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      doc_id: "abc123-def456-...",
      page_id: 1,
      title: "第1页标题",
      content: "第1页内容",
      raw_points: [],
      force: false
    })
  }
);

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log(`阶段: ${data.stage}, 消息: ${data.message}`);
  console.log('数据:', data.data);
};
```

### 步骤 6: 查询分析结果

#### 6.1 查询单页分析

```bash
curl "http://localhost:8000/api/v1/page-analysis?doc_id=abc123-def456-...&page_id=1"
```

#### 6.2 查询所有页面分析

```bash
curl "http://localhost:8000/api/v1/page-analysis/all?doc_id=abc123-def456-..."
```

### 步骤 7: 查看数据库

#### 7.1 使用 SQLite 命令行

```bash
sqlite3 backend/pptas_cache.sqlite3
```

#### 7.2 查询文档

```sql
SELECT doc_id, file_name, json_extract(global_analysis_json, '$.main_topic') 
FROM documents;
```

#### 7.3 查询页面分析

```sql
SELECT page_id, json_extract(analysis_json, '$.title') 
FROM page_analysis 
WHERE doc_id = 'abc123-def456-...';
```

### 完整示例脚本

```python
import requests
import json

# 配置
BASE_URL = "http://localhost:8000"
DOC_FILE = "presentation.pptx"

# 步骤1: 上传文档
print("步骤1: 上传文档...")
with open(DOC_FILE, "rb") as f:
    response = requests.post(
        f"{BASE_URL}/api/v1/expand-ppt",
        files={"file": f}
    )
doc_data = response.json()
doc_id = doc_data["doc_id"]
print(f"文档ID: {doc_id}")

# 步骤2: 全局分析
print("\n步骤2: 执行全局分析...")
response = requests.post(
    f"{BASE_URL}/api/v1/analyze-document-global",
    json={"doc_id": doc_id, "force": False}
)
global_analysis = response.json()["global_analysis"]
print(f"主题: {global_analysis['main_topic']}")
print(f"知识点单元数: {len(global_analysis['knowledge_units'])}")

# 步骤3: 单页分析
print("\n步骤3: 执行单页分析...")
slides = doc_data["slides"]
for i, slide in enumerate(slides[:3], 1):  # 只分析前3页
    print(f"\n分析第 {i} 页...")
    response = requests.post(
        f"{BASE_URL}/api/v1/analyze-page",
        json={
            "doc_id": doc_id,
            "page_id": i,
            "title": slide.get("title", ""),
            "content": slide.get("raw_content", ""),
            "raw_points": slide.get("raw_points", []),
            "force": False
        }
    )
    result = response.json()["data"]
    print(f"  理解笔记长度: {len(result['understanding_notes'])}")
    print(f"  知识缺口数: {len(result['knowledge_gaps'])}")
    print(f"  扩展内容数: {len(result['expanded_content'])}")

print("\n完成！")
```

---

## 常见问题

### Q1: 如何配置不同的 LLM 提供商？

A: 修改 `config.json` 中的 `llm.base_url` 和 `llm.model`:

```json
{
  "llm": {
    "api_key": "your-key",
    "base_url": "https://api.siliconflow.cn/v1",
    "model": "deepseek-ai/DeepSeek-V3.2-Exp"
  }
}
```

### Q2: 全局分析和单页分析的关系是什么？

A: 
- 全局分析分析整个文档的主题和知识点框架
- 单页分析可以基于全局分析结果，提供更准确的上下文
- 建议先执行全局分析，再执行单页分析

### Q3: 如何强制重新分析？

A: 在 API 请求中设置 `force: true`:

```json
{
  "doc_id": "your-doc-id",
  "force": true
}
```

### Q4: 数据库文件损坏怎么办？

A: 
1. 停止服务
2. 备份当前数据库文件
3. 删除损坏的数据库文件
4. 重启服务，系统会自动创建新的数据库

### Q5: 如何查看分析历史？

A: 使用查询接口或直接查询数据库:

```bash
# 通过 API
curl "http://localhost:8000/api/v1/page-analysis/all?doc_id=your-doc-id"

# 通过数据库
sqlite3 backend/pptas_cache.sqlite3
SELECT page_id, created_at FROM page_analysis WHERE doc_id = 'your-doc-id';
```

### Q6: 分析结果保存在哪里？

A: 分析结果保存在 `backend/pptas_cache.sqlite3` 数据库中:
- 全局分析: `documents.global_analysis_json`
- 单页分析: `page_analysis.analysis_json`

### Q7: 如何备份分析结果？

A: 备份数据库文件:

```bash
cp backend/pptas_cache.sqlite3 backend/pptas_cache.sqlite3.backup
```

### Q8: 流式分析和非流式分析的区别？

A: 
- 非流式分析: 等待所有分析完成后一次性返回结果
- 流式分析: 实时返回各阶段的分析结果，用户体验更好

### Q9: 如何提高分析速度？

A: 
1. 使用更快的 LLM 模型
2. 减少 `retrieval.max_results` 的值
3. 启用缓存，避免重复分析
4. 使用流式分析，提前显示部分结果

### Q10: 如何自定义 Agent 行为？

A: 修改 `src/agents/base.py` 中对应 Agent 的 prompt 和逻辑。

### Q11: 全局分析是异步的吗？

A: 是的，全局分析是异步的。前端在后台调用，不阻塞UI。如果全局分析未完成，页面分析仍可进行（但效果较差）。

### Q12: 系统向后兼容吗？

A: 是的，如果没有全局分析结果，页面分析仍可进行。各个 Agent 会检查是否有全局上下文，没有时使用原始逻辑。

### Q13: 性能如何优化？

A: 
1. **Token 消耗优化**:
   - 全局分析: 每页 200 字摘要
   - 单页分析: 1000-1500 字限制
   - 检索查询: 800 字限制
2. **网络请求优化**:
   - 源可用性缓存，初始化时测试一次
   - 早期退出策略，所有源不可用时立即跳过检索
   - 结果过滤，去除占位符文档
3. **模型温度设置**:
   - 结构化任务: 0（确定性）
   - 生成任务: 0.5-0.6（创意平衡）
   - 创意任务: 0.3-0.5（相对保守）

### Q14: 系统适用于哪些场景？

A: 
**最佳表现**:
- 技术性强的课程（算法、数据结构）
- 概念密集型内容（理论、原理）
- 需要案例说明的知识点
- 需要背景知识铺垫的话题

**限制条件**:
- 网络不稳定时，外部检索受限
- 非常规术语可能难以识别难点
- 极其简洁的 PPT（缺少上下文）
- 多语言混合内容

### Q15: 如何调试和监控？

A: 
**后端日志**:
- `开始全局分析，文档 {doc_id}，共 {页数} 页`
- `全局结构解析完成: {主题}`
- `全局知识点聚类完成: {数量} 个知识点单元`
- `加载全局分析结果: 主题={主题}, 知识点单元={数量}`

**前端日志**:
- `开始全局文档分析...`
- `全局分析完成: {主题, 知识点单元数量}`
- `全局分析已存在，直接使用`

---

## 总结

本文档详细说明了 AI Agent LLM 流水线、全局分析、单页分析以及数据持久化的使用方法。通过遵循本文档的步骤，您可以：

1. 配置和启动系统
2. 上传文档并执行分析
3. 查询和查看分析结果
4. 管理数据库和缓存

如有问题，请参考常见问题部分或查看源代码注释。
