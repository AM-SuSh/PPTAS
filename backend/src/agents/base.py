"""ä¼˜åŒ–åçš„ PPT æ‰©å±•ç³»ç»Ÿ Agent å®ç°"""

from typing import List, Dict, Any
import json
import requests
from urllib.parse import urlparse

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser, StructuredOutputParser, ResponseSchema
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

from .models import (
    PageStructure,
    KnowledgeGap,
    ExpandedContent,
    CheckResult,
    KnowledgeUnit,
    GraphState,
)


# ==================== é…ç½®ç®¡ç† ====================
class LLMConfig:
    """LLM é…ç½®"""
    def __init__(self, api_key: str = "", base_url: str = "", model: str = "gpt-4"):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
    
    def create_llm(self, temperature: float = 0.5) -> ChatOpenAI:
        return ChatOpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            model=self.model,
            max_retries=3,
            temperature=temperature
        )


# ==================== å·¥å…·å‡½æ•° ====================
def test_url_connectivity(url: str, timeout: int = 3) -> bool:
    """æµ‹è¯•URLè¿é€šæ€§"""
    try:
        response = requests.head(url, timeout=timeout, allow_redirects=True)
        return response.status_code < 400
    except:
        return False


# ==================== Step 0-A: å…¨å±€ç»“æ„è§£æ Agent (ç®€åŒ–ç‰ˆ) ====================
class GlobalStructureAgent:
    """å…¨å±€ç»“æ„è§£æ Agent - æå–æ•´ä½“çŸ¥è¯†æ¡†æ¶"""
    
    def __init__(self, llm_config: LLMConfig):
        self.llm = llm_config.create_llm(temperature=0)
    
    def run(self, state: GraphState) -> GraphState:
        """æ‰§è¡Œå…¨å±€ç»“æ„è§£æ"""
        # æ”¹è¿›çš„ prompt: æ›´æ˜ç¡®çš„è¦æ±‚
        template = """ä½ æ˜¯ä¸€ä¸ªæ•™è‚²ä¸“å®¶ï¼Œéœ€è¦åˆ†æè¿™ä»½PPT/PDFæ–‡æ¡£çš„æ•´ä½“ç»“æ„å’ŒçŸ¥è¯†æ¡†æ¶ã€‚

æ–‡æ¡£å†…å®¹ï¼ˆå…±{total_pages}é¡µï¼‰:
{ppt_texts}

è¯·ä»”ç»†åˆ†ææ•´ä¸ªæ–‡æ¡£ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **ä¸»é¢˜**ï¼šæ•´ä¸ªæ–‡æ¡£çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
2. **ç« èŠ‚ç»“æ„**ï¼šæ–‡æ¡£åˆ†ä¸ºå“ªäº›ä¸»è¦ç« èŠ‚ï¼Ÿæ¯ä¸ªç« èŠ‚åŒ…å«å“ªäº›é¡µé¢ï¼Ÿ
3. **çŸ¥è¯†é€»è¾‘æµç¨‹**ï¼šè¿™äº›ç« èŠ‚ä¹‹é—´çš„çŸ¥è¯†é€»è¾‘å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "main_topic": "æ–‡æ¡£çš„æ ¸å¿ƒä¸»é¢˜ï¼ˆå¿…é¡»å¡«å†™ï¼Œä¸èƒ½ä¸ºç©ºï¼‰",
  "chapters": [
    {{
      "title": "ç« èŠ‚æ ‡é¢˜",
      "pages": [é¡µç åˆ—è¡¨ï¼Œä¾‹å¦‚[1,2,3]],
      "key_concepts": ["æ ¸å¿ƒæ¦‚å¿µ1", "æ ¸å¿ƒæ¦‚å¿µ2"]
    }}
  ],
  "knowledge_flow": "çŸ¥è¯†é€»è¾‘æµç¨‹çš„ç®€è¦æè¿°ï¼ˆ50å­—å†…ï¼‰"
}}

é‡è¦è¦æ±‚ï¼š
- main_topic å¿…é¡»å¡«å†™ï¼Œä¸èƒ½ä¸ºç©ºæˆ–"æœªçŸ¥"
- è‡³å°‘è¯†åˆ«1-3ä¸ªä¸»è¦ç« èŠ‚
- åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—è¯´æ˜
"""
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.llm
        
        # æ”¹è¿›ï¼šä¼ é€’æ›´å¤šæ–‡æœ¬å†…å®¹ï¼Œä½†é™åˆ¶æ€»é•¿åº¦
        ppt_texts = state["ppt_texts"]
        total_pages = len(ppt_texts)
        
        # å¦‚æœé¡µæ•°å¤ªå¤šï¼Œåªå–å‰å‡ é¡µå’Œåå‡ é¡µï¼Œä»¥åŠä¸­é—´å‡ é¡µçš„æ‘˜è¦
        if total_pages > 20:
            # å–å‰5é¡µã€å5é¡µï¼Œä¸­é—´æ¯5é¡µå–1é¡µ
            selected_indices = list(range(min(5, total_pages)))
            for i in range(5, total_pages - 5, 5):
                selected_indices.append(i)
            selected_indices.extend(range(max(total_pages - 5, 5), total_pages))
            selected_texts = [ppt_texts[i] for i in selected_indices if i < len(ppt_texts)]
            ppt_summary = "\n\n".join([
                f"ç¬¬{i+1}é¡µ:\n{text[:500]}" for i, text in enumerate(selected_texts)
            ])
            ppt_summary += f"\n\n[æ³¨ï¼šæ–‡æ¡£å…±{total_pages}é¡µï¼Œæ­¤å¤„æ˜¾ç¤ºäº†{len(selected_texts)}é¡µçš„å†…å®¹]"
        else:
            # é¡µæ•°ä¸å¤šï¼Œä¼ é€’æ‰€æœ‰å†…å®¹ï¼Œä½†æ¯é¡µé™åˆ¶é•¿åº¦
            ppt_summary = "\n\n".join([
                f"ç¬¬{i+1}é¡µ:\n{text[:800]}" for i, text in enumerate(ppt_texts)
            ])
        
        print(f"ğŸ“ å‘é€ç»™LLMçš„æ–‡æœ¬é•¿åº¦: {len(ppt_summary)} å­—ç¬¦")
        response = chain.invoke({"ppt_texts": ppt_summary, "total_pages": total_pages})
        
        print(f"ğŸ“¥ LLMè¿”å›çš„åŸå§‹å†…å®¹: {response.content[:500]}...")
        
        try:
            # å°è¯•æå–JSONï¼ˆå¯èƒ½åŒ…å«markdownä»£ç å—ï¼‰
            content = response.content.strip()
            # å¦‚æœåŒ…å«```jsonï¼Œæå–å…¶ä¸­çš„å†…å®¹
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end > start:
                    content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                if end > start:
                    content = content[start:end].strip()
            
            result = json.loads(content)
            
            # éªŒè¯ç»“æœ
            if not result.get("main_topic") or result.get("main_topic") == "æœªçŸ¥":
                print("âš ï¸  LLMè¿”å›çš„ä¸»é¢˜ä¸ºç©ºæˆ–'æœªçŸ¥'ï¼Œå°è¯•ä»å†…å®¹æ¨æ–­...")
                # å°è¯•ä»ç¬¬ä¸€é¡µæ ‡é¢˜æ¨æ–­ä¸»é¢˜
                if ppt_texts and len(ppt_texts) > 0:
                    first_page = ppt_texts[0]
                    if "æ ‡é¢˜:" in first_page:
                        inferred_topic = first_page.split("æ ‡é¢˜:")[1].split("\n")[0].strip()
                        if inferred_topic:
                            result["main_topic"] = inferred_topic
                            print(f"âœ… ä»ç¬¬ä¸€é¡µæ ‡é¢˜æ¨æ–­ä¸»é¢˜: {inferred_topic}")
            
            print(f"âœ… è§£ææˆåŠŸ: ä¸»é¢˜={result.get('main_topic', 'æœªçŸ¥')}, ç« èŠ‚æ•°={len(result.get('chapters', []))}")
        except Exception as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"   åŸå§‹å†…å®¹: {response.content[:500]}")
            # å°è¯•ä»å†…å®¹æ¨æ–­åŸºæœ¬ä¿¡æ¯
            result = {"main_topic": "æœªçŸ¥", "chapters": [], "knowledge_flow": ""}
            if ppt_texts and len(ppt_texts) > 0:
                first_page = ppt_texts[0]
                if "æ ‡é¢˜:" in first_page:
                    inferred_topic = first_page.split("æ ‡é¢˜:")[1].split("\n")[0].strip()
                    if inferred_topic:
                        result["main_topic"] = inferred_topic
                        print(f"âœ… ä»ç¬¬ä¸€é¡µæ ‡é¢˜æ¨æ–­ä¸»é¢˜: {inferred_topic}")
        
        state["global_outline"] = result
        return state


# ==================== Step 0-B: çŸ¥è¯†ç‚¹åˆ’åˆ† Agent (å…¨å±€è§†è§’) ====================
class KnowledgeClusteringAgent:
    """çŸ¥è¯†ç‚¹åˆ’åˆ† Agent - ä»å…¨å±€PPTæå–çŸ¥è¯†å•å…ƒ"""
    
    def __init__(self, llm_config: LLMConfig):
        self.llm = llm_config.create_llm(temperature=0.2)
    
    def run(self, state: GraphState) -> GraphState:
        """æ‰§è¡ŒçŸ¥è¯†ç‚¹èšç±» - ä»å…¨å±€è§†è§’"""
        # æ”¹è¿›çš„ prompt: æ›´æ˜ç¡®çš„è¦æ±‚å’Œæ›´å¥½çš„æ ¼å¼
        global_outline = state.get("global_outline", {})
        main_topic = global_outline.get("main_topic", "æœªçŸ¥")
        
        template = """ä½ æ˜¯å­¦ä¹ ä¸“å®¶ï¼Œéœ€è¦ä»æ•´ä¸ªPPT/PDFæ–‡æ¡£ä¸­æå–æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚

æ–‡æ¡£ä¸»é¢˜: {main_topic}

æ–‡æ¡£ç»“æ„:
{global_outline}

æ–‡æ¡£å†…å®¹ï¼ˆå…±{total_pages}é¡µï¼‰:
{ppt_texts}

ä»»åŠ¡: ä»æ•´ä¸ªæ–‡æ¡£ä¸­æå–æ ¸å¿ƒçŸ¥è¯†ç‚¹å•å…ƒ
è¦æ±‚:
1. è¯†åˆ«æ–‡æ¡£ä¸­æœ€é‡è¦çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆè‡³å°‘5ä¸ªï¼‰
2. æ¯ä¸ªçŸ¥è¯†ç‚¹åº”è¯¥ï¼š
   - æœ‰æ˜ç¡®çš„åç§°
   - æ ‡æ³¨æ¶‰åŠçš„é¡µç 
   - è¯´æ˜ä¸ºä»€ä¹ˆå­¦ç”Ÿå¯èƒ½ä¸ç†è§£
   - æŒ‡å‡ºéœ€è¦è¡¥å……ä»€ä¹ˆå†…å®¹

è¾“å‡ºJSONæ•°ç»„ï¼Œæ ¼å¼å¦‚ä¸‹:
[
  {{
    "concept": "æ¦‚å¿µåç§°ï¼ˆå¿…é¡»å¡«å†™ï¼‰",
    "pages": [é¡µç åˆ—è¡¨ï¼Œä¾‹å¦‚[1,2,3]],
    "why_difficult": "ä¸ºä»€ä¹ˆå­¦ç”Ÿå¯èƒ½ä¸ç†è§£ï¼ˆ20å­—å†…ï¼‰",
    "è¡¥å……æ–¹å‘": "éœ€è¦è¡¥å……ä»€ä¹ˆï¼ˆä¾‹å¦‚:åŸç†/ç¤ºä¾‹/èƒŒæ™¯/å…¬å¼æ¨å¯¼ï¼‰"
  }}
]

é‡è¦è¦æ±‚:
- å¿…é¡»è‡³å°‘æå–5ä¸ªæ ¸å¿ƒçŸ¥è¯†ç‚¹
- conceptå­—æ®µä¸èƒ½ä¸ºç©º
- pageså­—æ®µå¿…é¡»æ˜¯æ•°å­—æ•°ç»„
- åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—è¯´æ˜
"""
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.llm
        
        # æ”¹è¿›ï¼šå¦‚æœé¡µæ•°å¤ªå¤šï¼Œä½¿ç”¨æ‘˜è¦
        ppt_texts = state["ppt_texts"]
        total_pages = len(ppt_texts)
        
        if total_pages > 15:
            # ä½¿ç”¨æ‘˜è¦ï¼šæ¯é¡µå–å‰500å­—ç¬¦
            ppt_summary = "\n\n".join([
                f"ç¬¬{i+1}é¡µ:\n{text[:500]}..." for i, text in enumerate(ppt_texts)
            ])
        else:
            # é¡µæ•°ä¸å¤šï¼Œä¼ é€’å®Œæ•´å†…å®¹
            ppt_summary = "\n\n".join([
                f"ç¬¬{i+1}é¡µ:\n{text[:1000]}" for i, text in enumerate(ppt_texts)
            ])
        
        print(f"ğŸ“ å‘é€ç»™LLMçš„æ–‡æœ¬é•¿åº¦: {len(ppt_summary)} å­—ç¬¦")
        response = chain.invoke({
            "main_topic": main_topic,
            "global_outline": json.dumps(global_outline, ensure_ascii=False, indent=2),
            "ppt_texts": ppt_summary,
            "total_pages": total_pages
        })
        
        print(f"ğŸ“¥ LLMè¿”å›çš„åŸå§‹å†…å®¹: {response.content[:500]}...")
        
        try:
            # å°è¯•æå–JSON
            content = response.content.strip()
            # å¦‚æœåŒ…å«```jsonï¼Œæå–å…¶ä¸­çš„å†…å®¹
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                if end > start:
                    content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                if end > start:
                    content = content[start:end].strip()
            
            concepts_data = json.loads(content)
            
            # éªŒè¯å’Œè¿‡æ»¤
            valid_concepts = []
            for concept in concepts_data:
                if concept.get("concept") and concept.get("concept").strip():
                    # ç¡®ä¿pagesæ˜¯åˆ—è¡¨
                    pages = concept.get("pages", [])
                    if not isinstance(pages, list):
                        pages = []
                    valid_concepts.append({
                        "concept": concept.get("concept", "").strip(),
                        "pages": pages,
                        "why_difficult": concept.get("why_difficult", ""),
                        "è¡¥å……æ–¹å‘": concept.get("è¡¥å……æ–¹å‘", "")
                    })
            
            print(f"âœ… è§£ææˆåŠŸ: æå–åˆ° {len(valid_concepts)} ä¸ªæœ‰æ•ˆçŸ¥è¯†ç‚¹")
            
            # è½¬æ¢ä¸º KnowledgeUnit æ ¼å¼
            knowledge_units = []
            for i, concept in enumerate(valid_concepts[:15]):  # æœ€å¤š15ä¸ª
                pages = concept.get("pages", [])
                # ç¡®ä¿é¡µç æœ‰æ•ˆ
                valid_pages = [p for p in pages if isinstance(p, int) and 0 < p <= total_pages]
                if not valid_pages:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆé¡µç ï¼Œå°è¯•ä»æ¦‚å¿µåç§°æ¨æ–­
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´æ™ºèƒ½çš„æ¨æ–­é€»è¾‘
                    valid_pages = []
                
                knowledge_units.append(KnowledgeUnit(
                    unit_id=f"unit_{i+1}",
                    title=concept.get("concept", ""),
                    pages=valid_pages,
                    core_concepts=[concept.get("concept", "")],
                    raw_texts=[state["ppt_texts"][p-1] for p in valid_pages if 0 < p <= len(state["ppt_texts"])]
                ))
        except Exception as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"   åŸå§‹å†…å®¹: {response.content[:500]}")
            knowledge_units = []
        
        print(f"âœ… æœ€ç»ˆç”Ÿæˆ {len(knowledge_units)} ä¸ªçŸ¥è¯†ç‚¹å•å…ƒ")
        state["knowledge_units"] = knowledge_units
        return state


# ==================== Step 1: ç»“æ„è¯­ä¹‰ç†è§£ Agent (ç®€åŒ–) ====================
class StructureUnderstandingAgent:
    """ç»“æ„è¯­ä¹‰ç†è§£ Agent - ç”Ÿæˆå­¦ç”Ÿç†è§£ç¬”è®°"""
    
    def __init__(self, llm_config: LLMConfig):
        self.llm = llm_config.create_llm(temperature=0.5)
    
    def run(self, state: GraphState) -> GraphState:
        """æ‰§è¡Œç»“æ„è¯­ä¹‰ç†è§£å’Œç¬”è®°ç”Ÿæˆï¼ˆåŸºäºå…¨å±€ä¸Šä¸‹æ–‡ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¨å±€ä¸Šä¸‹æ–‡
        has_global_context = state.get("global_outline") and state.get("knowledge_units")
        
        if has_global_context:
            # æœ‰å…¨å±€ä¸Šä¸‹æ–‡æ—¶ï¼Œä½¿ç”¨å¢å¼ºçš„prompt
            template = """åŸºäºæ•´ä¸ªæ–‡æ¡£çš„å…¨å±€åˆ†æç»“æœï¼Œä¸ºå­¦ç”Ÿç”Ÿæˆç»“æ„åŒ–å­¦ä¹ ç¬”è®°(Markdownæ ¼å¼ï¼Œ300å­—å†…):

æ–‡æ¡£å…¨å±€ä¿¡æ¯:
- ä¸»é¢˜: {main_topic}
- çŸ¥è¯†é€»è¾‘æµç¨‹: {knowledge_flow}
- å½“å‰é¡µé¢åœ¨å…¨å±€çŸ¥è¯†ä½“ç³»ä¸­çš„ä½ç½®: {page_context}

å½“å‰é¡µé¢å†…å®¹: {raw_text}

ç¬”è®°æ ¼å¼:
## [é¡µé¢ä¸»é¢˜]

### æ ¸å¿ƒæ¦‚å¿µ
- æ¦‚å¿µ1: ç®€è¦è¯´æ˜ï¼ˆç»“åˆå…¨å±€çŸ¥è¯†æ¡†æ¶ï¼‰
- æ¦‚å¿µ2: ç®€è¦è¯´æ˜

### å…³é”®è¦ç‚¹
- è¦ç‚¹1
- è¦ç‚¹2

### é‡ç‚¹ç†è§£
[ç®€æ´çš„ç†è§£è¦ç‚¹ï¼Œè¯´æ˜åœ¨å½“å‰é¡µé¢åœ¨æ•´ä¸ªæ–‡æ¡£çŸ¥è¯†ä½“ç³»ä¸­çš„ä½ç½®]

è¦æ±‚:
- ç»“åˆå…¨å±€çŸ¥è¯†æ¡†æ¶ï¼Œçªå‡ºæœ€é‡è¦çš„æ¦‚å¿µ
- è¯´æ˜å½“å‰é¡µé¢ä¸æ•´ä½“çŸ¥è¯†ä½“ç³»çš„å…³ç³»
- æ ‡æ³¨å­¦ç”Ÿåº”è¯¥æŒæ¡çš„è¦ç‚¹
- é€‚åˆå¿«é€Ÿå¤ä¹ 
"""
            # æ„å»ºé¡µé¢ä¸Šä¸‹æ–‡ä¿¡æ¯
            page_id = state.get("current_page_id", 0)
            page_context = f"ç¬¬{page_id}é¡µ"
            if state.get("global_outline", {}).get("chapters"):
                for chapter in state["global_outline"]["chapters"]:
                    if page_id in chapter.get("pages", []):
                        page_context = f"ç¬¬{page_id}é¡µï¼Œå±äºç« èŠ‚ï¼š{chapter.get('title', '')}"
                        break
            
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.llm
            
            response = chain.invoke({
                "main_topic": state.get("global_outline", {}).get("main_topic", "æœªçŸ¥"),
                "knowledge_flow": state.get("global_outline", {}).get("knowledge_flow", ""),
                "page_context": page_context,
                "raw_text": state["raw_text"][:1000]
            })
        else:
            # æ²¡æœ‰å…¨å±€ä¸Šä¸‹æ–‡æ—¶ï¼Œä½¿ç”¨åŸå§‹prompt
            template = """æ ¹æ®ä»¥ä¸‹å†…å®¹ï¼Œä¸ºå­¦ç”Ÿç”Ÿæˆç»“æ„åŒ–å­¦ä¹ ç¬”è®°(Markdownæ ¼å¼ï¼Œ300å­—å†…):

å†…å®¹: {raw_text}

ç¬”è®°æ ¼å¼:
## [é¡µé¢ä¸»é¢˜]

### æ ¸å¿ƒæ¦‚å¿µ
- æ¦‚å¿µ1: ç®€è¦è¯´æ˜
- æ¦‚å¿µ2: ç®€è¦è¯´æ˜

### å…³é”®è¦ç‚¹
- è¦ç‚¹1
- è¦ç‚¹2

### é‡ç‚¹ç†è§£
[ç®€æ´çš„ç†è§£è¦ç‚¹]

è¦æ±‚:
- çªå‡ºæœ€é‡è¦çš„æ¦‚å¿µ
- æ ‡æ³¨å­¦ç”Ÿåº”è¯¥æŒæ¡çš„è¦ç‚¹
- é€‚åˆå¿«é€Ÿå¤ä¹ 
"""
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.llm
            
            response = chain.invoke({"raw_text": state["raw_text"][:1000]})  # é™åˆ¶è¾“å…¥é•¿åº¦
        
        # ç”Ÿæˆå­¦ä¹ ç¬”è®°
        understanding_notes = response.content
        
        # åŒæ—¶æå–é¡µé¢ç»“æ„ä¿¡æ¯
        structure_template = """æå–é¡µé¢çš„ç»“æ„åŒ–ä¿¡æ¯(JSONæ ¼å¼):

å†…å®¹: {raw_text}

{{
  "title": "é¡µé¢æ ‡é¢˜",
  "main_concepts": ["æ ¸å¿ƒæ¦‚å¿µ1", "æ ¸å¿ƒæ¦‚å¿µ2"],
  "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"]
}}

ä»…è¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
        structure_prompt = ChatPromptTemplate.from_template(structure_template)
        structure_chain = structure_prompt | self.llm
        
        structure_response = structure_chain.invoke({"raw_text": state["raw_text"][:800]})
        
        try:
            structure_data = json.loads(structure_response.content)
            page_structure = {
                "page_id": state.get("current_page_id", 0),
                "title": structure_data.get("title", ""),
                "main_concepts": structure_data.get("main_concepts", []),
                "key_points": structure_data.get("key_points", []),
                "relationships": {},
                "teaching_goal": ""
            }
        except:
            page_structure = {
                "page_id": 0, 
                "title": "", 
                "main_concepts": [], 
                "key_points": [], 
                "relationships": {}, 
                "teaching_goal": ""
            }
        
        state["page_structure"] = page_structure
        state["understanding_notes"] = understanding_notes
        return state


# ==================== Step 2: çŸ¥è¯†ç¼ºå£è¯†åˆ« Agent (é’ˆå¯¹å­¦ç”Ÿ) ====================
class GapIdentificationAgent:
    """çŸ¥è¯†ç¼ºå£è¯†åˆ« Agent - è¯†åˆ«å­¦ç”Ÿç†è§£éšœç¢"""
    
    def __init__(self, llm_config: LLMConfig):
        self.llm = llm_config.create_llm(temperature=0.2)
    
    def _parse_partial_json(self, text: str) -> List[Dict]:
        """æ‰‹åŠ¨è§£æéƒ¨åˆ†JSONï¼Œæå–æœ‰æ•ˆçš„å¯¹è±¡ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰"""
        import re
        gaps = []
        
        if not text or not text.strip():
            return gaps
        
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        text = re.sub(r'```json\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'```\s*$', '', text)
        text = text.strip()
        
        # æ–¹æ³•1: æŸ¥æ‰¾å®Œæ•´çš„JSONå¯¹è±¡ { "concept": "...", "gap_type": "...", "priority": ... }
        # æ”¯æŒå¤šè¡Œå’Œå¯èƒ½çš„æˆªæ–­ï¼Œæ›´å®½æ¾çš„æ¨¡å¼
        pattern1 = r'\{\s*"concept"\s*:\s*"([^"]+)"\s*,\s*"gap_type"\s*:\s*"([^"]+)"\s*,\s*"priority"\s*:\s*(\d+)'
        
        matches = re.finditer(pattern1, text, re.DOTALL)
        for match in matches:
            try:
                concept = match.group(1).strip()
                gap_type = match.group(2).strip()
                priority = int(match.group(3))
                
                if concept and gap_type:
                    gaps.append({
                        "concept": concept,
                        "gap_type": gap_type,
                        "priority": max(1, min(5, priority))
                    })
            except Exception as e:
                continue
        
        # æ–¹æ³•2: å¦‚æœæ–¹æ³•1æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„æ¨¡å¼ï¼ˆå…è®¸å­—æ®µé¡ºåºä¸åŒï¼‰
        if not gaps:
            # åŒ¹é… concept å’Œ gap_typeï¼Œä¸è¦æ±‚é¡ºåº
            pattern2 = r'"concept"\s*:\s*"([^"]+)"[^}]*"gap_type"\s*:\s*"([^"]+)"[^}]*"priority"\s*:\s*(\d+)'
            matches = re.finditer(pattern2, text, re.DOTALL)
            for match in matches:
                try:
                    concept = match.group(1).strip()
                    gap_type = match.group(2).strip()
                    priority = int(match.group(3))
                    
                    if concept and gap_type:
                        gaps.append({
                            "concept": concept,
                            "gap_type": gap_type,
                            "priority": max(1, min(5, priority))
                        })
                except:
                    continue
        
        return gaps
    
    def run(self, state: GraphState) -> GraphState:
        """è¯†åˆ«çŸ¥è¯†ç¼ºå£ï¼ˆåŸºäºå…¨å±€ä¸Šä¸‹æ–‡ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¨å±€ä¸Šä¸‹æ–‡
        has_global_context = state.get("global_outline") and state.get("knowledge_units")
        
        if has_global_context:
            # æœ‰å…¨å±€ä¸Šä¸‹æ–‡æ—¶ï¼Œä½¿ç”¨å¢å¼ºçš„prompt
            template = """ä½ æ˜¯æ•™å­¦åŠ©æ‰‹,åŸºäºæ•´ä¸ªæ–‡æ¡£çš„å…¨å±€åˆ†æç»“æœ,è¯†åˆ«å­¦ç”Ÿç†è§£å½“å‰é¡µé¢å†…å®¹çš„éšœç¢ç‚¹ã€‚

æ–‡æ¡£å…¨å±€ä¿¡æ¯:
- ä¸»é¢˜: {main_topic}
- çŸ¥è¯†é€»è¾‘æµç¨‹: {knowledge_flow}
- å…¨å±€çŸ¥è¯†ç‚¹å•å…ƒ: {knowledge_units}

å½“å‰é¡µé¢å†…å®¹: {raw_text}

ä»»åŠ¡: ç»“åˆå…¨å±€çŸ¥è¯†æ¡†æ¶,è¯†åˆ«å½“å‰é¡µé¢ä¸­å­¦ç”Ÿå¯èƒ½ç¼ºå°‘çš„çŸ¥è¯†
è¦æ±‚:
1. å‚è€ƒå…¨å±€çŸ¥è¯†ç‚¹å•å…ƒ,è¯†åˆ«å½“å‰é¡µé¢æ¶‰åŠçš„æ¦‚å¿µ
2. è€ƒè™‘æ¦‚å¿µåœ¨æ•´ä¸ªæ–‡æ¡£çŸ¥è¯†ä½“ç³»ä¸­çš„ä½ç½®
3. è¯†åˆ«å­¦ç”Ÿå¯èƒ½ç¼ºå°‘çš„èƒŒæ™¯çŸ¥è¯†æˆ–å‰ç½®çŸ¥è¯†

è¯†åˆ«(JSONæ•°ç»„,æœ€å¤š5ä¸ª):
[
  {{
    "concept": "æ¦‚å¿µ",
    "gap_type": "ç¼ºå°‘ä»€ä¹ˆ(é€‰ä¸€ä¸ª: ç›´è§‚è§£é‡Š/åº”ç”¨ç¤ºä¾‹/èƒŒæ™¯çŸ¥è¯†/å…¬å¼æ¨å¯¼/å‰ç½®çŸ¥è¯†)",
    "priority": ä¼˜å…ˆçº§1-5,
    "global_relation": "åœ¨å…¨å±€çŸ¥è¯†æ¡†æ¶ä¸­çš„ä½ç½®(å¯é€‰)"
  }}
]

åŸåˆ™:
- ç»“åˆå…¨å±€çŸ¥è¯†æ¡†æ¶ï¼Œåªæ ‡æ³¨çœŸæ­£å½±å“ç†è§£çš„ç¼ºå£
- ä¼˜å…ˆçº§é«˜çš„æ˜¯å¿…é¡»è¡¥å……çš„
- è€ƒè™‘æ¦‚å¿µåœ¨æ•´ä¸ªæ–‡æ¡£ä¸­çš„ä½ç½®å’Œå…³ç³»
- ä¸è¦è¿‡åº¦å»¶ä¼¸
"""
            # æ ¼å¼åŒ–å…¨å±€çŸ¥è¯†ç‚¹å•å…ƒ
            knowledge_units_str = ""
            if state.get("knowledge_units"):
                for unit in state["knowledge_units"][:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                    pages_str = ",".join(map(str, unit.pages))
                    concepts_str = ",".join(unit.core_concepts)
                    knowledge_units_str += f"- {unit.title} (é¡µç : {pages_str}, æ ¸å¿ƒæ¦‚å¿µ: {concepts_str})\n"
            
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.llm
            
            print(f"ğŸ“¤ è°ƒç”¨LLMè¿›è¡ŒçŸ¥è¯†ç¼ºå£è¯†åˆ«...")
            try:
                response = chain.invoke({
                    "main_topic": state.get("global_outline", {}).get("main_topic", "æœªçŸ¥"),
                    "knowledge_flow": state.get("global_outline", {}).get("knowledge_flow", ""),
                    "knowledge_units": knowledge_units_str or "æ— ",
                    "raw_text": state["raw_text"][:800]
                })
            except Exception as e:
                print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                state["knowledge_gaps"] = []
                return state
        else:
            # æ²¡æœ‰å…¨å±€ä¸Šä¸‹æ–‡æ—¶ï¼Œä½¿ç”¨åŸå§‹prompt
            template = """ä½ æ˜¯æ•™å­¦åŠ©æ‰‹,è¯†åˆ«å­¦ç”Ÿç†è§£è¿™æ®µå†…å®¹çš„éšœç¢ç‚¹ã€‚

å†…å®¹: {raw_text}

è¯†åˆ«(JSONæ•°ç»„,æœ€å¤š5ä¸ª):
[
  {{
    "concept": "æ¦‚å¿µ",
    "gap_type": "ç¼ºå°‘ä»€ä¹ˆ(é€‰ä¸€ä¸ª: ç›´è§‚è§£é‡Š/åº”ç”¨ç¤ºä¾‹/èƒŒæ™¯çŸ¥è¯†/å…¬å¼æ¨å¯¼)",
    "priority": ä¼˜å…ˆçº§1-5
  }}
]

åŸåˆ™:
- åªæ ‡æ³¨çœŸæ­£å½±å“ç†è§£çš„ç¼ºå£
- ä¼˜å…ˆçº§é«˜çš„æ˜¯å¿…é¡»è¡¥å……çš„
- ä¸è¦è¿‡åº¦å»¶ä¼¸
"""
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.llm
            
            print(f"ğŸ“¤ è°ƒç”¨LLMè¿›è¡ŒçŸ¥è¯†ç¼ºå£è¯†åˆ«...")
            try:
                response = chain.invoke({"raw_text": state["raw_text"][:800]})
            except Exception as e:
                print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                state["knowledge_gaps"] = []
                return state
        
        # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
        if not response:
            print(f"âŒ LLMè¿”å›ç©ºå“åº”å¯¹è±¡")
            state["knowledge_gaps"] = []
            return state
        
        if not hasattr(response, 'content'):
            print(f"âŒ LLMå“åº”å¯¹è±¡æ²¡æœ‰contentå±æ€§")
            print(f"   å“åº”å¯¹è±¡ç±»å‹: {type(response)}")
            print(f"   å“åº”å¯¹è±¡: {response}")
            state["knowledge_gaps"] = []
            return state
        
        try:
            # å°è¯•è§£æJSONï¼Œæ”¯æŒmarkdownä»£ç å—å’Œæˆªæ–­çš„JSON
            response_text = response.content.strip() if response.content else ""
            original_text = response_text
            
            # æ‰“å°åŸå§‹å“åº”ç”¨äºè°ƒè¯•
            print(f"ğŸ” åŸå§‹LLMå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
            if len(response_text) == 0:
                print(f"âŒ LLMå“åº”ä¸ºç©ºï¼")
                print(f"   åŸå§‹response.contentç±»å‹: {type(response.content)}")
                print(f"   åŸå§‹response.contentå€¼: {repr(response.content)}")
                state["knowledge_gaps"] = []
                return state
            
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            if response_text.startswith("```"):
                print(f"ğŸ” æ£€æµ‹åˆ°markdownä»£ç å—ï¼Œå¼€å§‹æå–JSON...")
                # ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•ï¼šç›´æ¥æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
                if "```json" in response_text:
                    start = response_text.find("```json") + 7
                    end = response_text.find("```", start)
                    if end > start:
                        response_text = response_text[start:end].strip()
                        print(f"âœ… ä½¿ç”¨ç®€å•æ–¹æ³•æå–JSONï¼Œé•¿åº¦: {len(response_text)} å­—ç¬¦")
                    else:
                        # å¦‚æœæ²¡æ‰¾åˆ°ç»“æŸæ ‡è®°ï¼Œå°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ª```
                        end = response_text.rfind("```")
                        if end > start:
                            response_text = response_text[start:end].strip()
                            print(f"âœ… ä½¿ç”¨ç®€å•æ–¹æ³•æå–JSONï¼ˆæœªæ‰¾åˆ°ç»“æŸæ ‡è®°ï¼‰ï¼Œé•¿åº¦: {len(response_text)} å­—ç¬¦")
                        else:
                            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
                            print(f"âš ï¸  æ— æ³•æ‰¾åˆ°ä»£ç å—ç»“æŸæ ‡è®°ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•...")
                            lines = response_text.split("\n")
                            json_lines = []
                            in_code_block = False
                            for i, line in enumerate(lines):
                                line_stripped = line.strip()
                                if line_stripped.startswith("```"):
                                    in_code_block = not in_code_block
                                    print(f"   ç¬¬{i+1}è¡Œ: ä»£ç å—æ ‡è®°ï¼Œin_code_block={in_code_block}")
                                    continue
                                if in_code_block:  # ä¿®å¤ï¼šåº”è¯¥åœ¨ä»£ç å—å†…æ—¶æ·»åŠ 
                                    json_lines.append(line)
                                    if len(json_lines) <= 3:
                                        print(f"   ç¬¬{i+1}è¡Œ: æ·»åŠ åˆ°JSON ({len(line)} å­—ç¬¦)")
                            response_text = "\n".join(json_lines).strip()
                elif "```" in response_text:
                    # å¤„ç†æ™®é€šçš„```ä»£ç å—
                    start = response_text.find("```") + 3
                    end = response_text.find("```", start)
                    if end > start:
                        response_text = response_text[start:end].strip()
                        print(f"âœ… æå–æ™®é€šä»£ç å—å†…å®¹ï¼Œé•¿åº¦: {len(response_text)} å­—ç¬¦")
                    else:
                        end = response_text.rfind("```")
                        if end > start:
                            response_text = response_text[start:end].strip()
                            print(f"âœ… æå–æ™®é€šä»£ç å—å†…å®¹ï¼ˆæœªæ‰¾åˆ°ç»“æŸæ ‡è®°ï¼‰ï¼Œé•¿åº¦: {len(response_text)} å­—ç¬¦")
                
                print(f"ğŸ” æå–åJSONé•¿åº¦: {len(response_text)} å­—ç¬¦")
                if len(response_text) == 0:
                    print(f"âŒ æå–JSONåä¸ºç©ºï¼")
                    print(f"   åŸå§‹å“åº”å‰500å­—ç¬¦: {original_text[:500]}")
                    # å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥æå–
                    import re
                    json_match = re.search(r'\[[\s\S]*?\]', original_text)
                    if json_match:
                        response_text = json_match.group(0)
                        print(f"âœ… ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSONæ•°ç»„ï¼Œé•¿åº¦: {len(response_text)} å­—ç¬¦")
            
            print(f"ğŸ” LLMå“åº”å‰300å­—ç¬¦: {response_text[:300]}")
            
            # å°è¯•ç›´æ¥è§£æ
            gaps_data = None
            try:
                gaps_data = json.loads(response_text)
                print(f"âœ… JSONè§£ææˆåŠŸ")
            except json.JSONDecodeError as e:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤å¸¸è§çš„æˆªæ–­é—®é¢˜
                print(f"âš ï¸  JSONè§£æå¤±è´¥ï¼Œé”™è¯¯ä½ç½®: {e.pos}, é”™è¯¯ä¿¡æ¯: {e.msg}")
                
                # å¦‚æœé”™è¯¯ä½ç½®ä¸º0ï¼Œå¯èƒ½æ˜¯å“åº”æ ¼å¼ä¸å¯¹æˆ–ä¸ºç©º
                if e.pos == 0:
                    print(f"âš ï¸  é”™è¯¯ä½ç½®ä¸º0ï¼Œå¯èƒ½æ˜¯å“åº”æ ¼å¼ä¸å¯¹æˆ–ä¸ºç©º")
                    print(f"ğŸ” å®Œæ•´å“åº”å†…å®¹:\n{response_text}")
                    
                    # å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥æå–
                    gaps_data = self._parse_partial_json(response_text)
                    if gaps_data:
                        print(f"âœ… é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–äº† {len(gaps_data)} ä¸ªå¯¹è±¡")
                    else:
                        # å¦‚æœæ­£åˆ™ä¹Ÿå¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾JSONæ•°ç»„
                        import re
                        # å°è¯•æ‰¾åˆ° [ ... ] æ¨¡å¼
                        array_match = re.search(r'\[[\s\S]*?\]', response_text)
                        if array_match:
                            try:
                                gaps_data = json.loads(array_match.group(0))
                                print(f"âœ… ä»å“åº”ä¸­æå–JSONæ•°ç»„æˆåŠŸ")
                            except:
                                gaps_data = []
                elif e.pos > 0:
                    # å¦‚æœJSONè¢«æˆªæ–­ï¼Œå°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
                    print(f"âš ï¸  JSONè¢«æˆªæ–­ï¼Œå°è¯•ä¿®å¤...")
                    truncated_text = response_text[:e.pos]
                    
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
                    last_brace = truncated_text.rfind('}')
                    if last_brace > 0:
                        # æ‰¾åˆ°è¿™ä¸ªå¯¹è±¡æ‰€å±çš„æ•°ç»„
                        before_brace = truncated_text[:last_brace]
                        last_bracket = before_brace.rfind('[')
                        if last_bracket >= 0:
                            # å°è¯•æå–å®Œæ•´çš„æ•°ç»„
                            potential_json = truncated_text[last_bracket:last_brace+1] + ']'
                            try:
                                gaps_data = json.loads(potential_json)
                                print(f"âœ… æˆåŠŸä¿®å¤æˆªæ–­çš„JSONï¼Œæå–äº† {len(gaps_data) if isinstance(gaps_data, list) else 1} ä¸ªå¯¹è±¡")
                            except:
                                # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨è§£æ
                                gaps_data = self._parse_partial_json(truncated_text)
                                if gaps_data:
                                    print(f"âœ… é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä»æˆªæ–­æ–‡æœ¬ä¸­æå–äº† {len(gaps_data)} ä¸ªå¯¹è±¡")
                        else:
                            gaps_data = self._parse_partial_json(truncated_text)
                            if gaps_data:
                                print(f"âœ… é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä»æˆªæ–­æ–‡æœ¬ä¸­æå–äº† {len(gaps_data)} ä¸ªå¯¹è±¡")
                    else:
                        gaps_data = self._parse_partial_json(truncated_text)
                        if gaps_data:
                            print(f"âœ… é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼ä»æˆªæ–­æ–‡æœ¬ä¸­æå–äº† {len(gaps_data)} ä¸ªå¯¹è±¡")
                else:
                    # å…¶ä»–æƒ…å†µï¼Œå°è¯•æ‰‹åŠ¨è§£æ
                    gaps_data = self._parse_partial_json(response_text)
                    if gaps_data:
                        print(f"âœ… é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–äº† {len(gaps_data)} ä¸ªå¯¹è±¡")
            
            # å¦‚æœè¿˜æ˜¯Noneï¼Œè®¾ä¸ºç©ºåˆ—è¡¨
            if gaps_data is None:
                gaps_data = []
            
            # ç¡®ä¿æ˜¯åˆ—è¡¨
            if not isinstance(gaps_data, list):
                gaps_data = [gaps_data] if gaps_data else []
            
            knowledge_gaps = []
            for g in gaps_data[:5]:  # æœ€å¤š5ä¸ª
                if not isinstance(g, dict):
                    continue
                    
                concept = g.get("concept", "").strip()
                gap_type = g.get("gap_type", "").strip()
                priority = g.get("priority", 3)
                
                # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                if concept and gap_type:
                    # ç¡®ä¿priorityæ˜¯æ•°å­—ä¸”åœ¨1-5èŒƒå›´å†…
                    try:
                        priority = int(priority)
                        priority = max(1, min(5, priority))
                    except:
                        priority = 3
                    
                    knowledge_gaps.append(KnowledgeGap(
                        concept=concept,
                        gap_types=[gap_type],
                        priority=priority
                    ))
            
            print(f"âœ… æˆåŠŸè¯†åˆ« {len(knowledge_gaps)} ä¸ªçŸ¥è¯†ç¼ºå£")
            if knowledge_gaps:
                for gap in knowledge_gaps:
                    print(f"   - {gap.concept} (ä¼˜å…ˆçº§: {gap.priority}, ç±»å‹: {gap.gap_types[0]})")
        except Exception as e:
            print(f"âš ï¸  çŸ¥è¯†ç¼ºå£è¯†åˆ«JSONè§£æå¤±è´¥: {e}")
            print(f"   LLMåŸå§‹å“åº”å‰500å­—ç¬¦: {response.content[:500]}")
            # å°è¯•æ‰‹åŠ¨è§£æ
            try:
                gaps_data = self._parse_partial_json(response.content)
                knowledge_gaps = []
                for g in gaps_data[:5]:
                    if isinstance(g, dict) and g.get("concept") and g.get("gap_type"):
                        knowledge_gaps.append(KnowledgeGap(
                            concept=g["concept"].strip(),
                            gap_types=[g["gap_type"].strip()],
                            priority=max(1, min(5, int(g.get("priority", 3))))
                        ))
                if knowledge_gaps:
                    print(f"âœ… é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–äº† {len(knowledge_gaps)} ä¸ªçŸ¥è¯†ç¼ºå£")
                    for gap in knowledge_gaps:
                        print(f"   - {gap.concept} (ä¼˜å…ˆçº§: {gap.priority}, ç±»å‹: {gap.gap_types[0]})")
                else:
                    print(f"âš ï¸  æ­£åˆ™è¡¨è¾¾å¼è§£ææœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
            except Exception as e2:
                print(f"âš ï¸  æ­£åˆ™è¡¨è¾¾å¼è§£æä¹Ÿå¤±è´¥: {e2}")
                knowledge_gaps = []
        
        state["knowledge_gaps"] = knowledge_gaps
        return state


# ==================== Step 3: å®šå‘çŸ¥è¯†æ‰©å±• Agent (ç²¾ç®€) ====================
class KnowledgeExpansionAgent:
    """å®šå‘çŸ¥è¯†æ‰©å±• Agent - ç”Ÿæˆè¡¥å……è¯´æ˜"""
    
    def __init__(self, llm_config: LLMConfig):
        self.llm = llm_config.create_llm(temperature=0.6)
    
    def run(self, state: GraphState) -> GraphState:
        """ç”Ÿæˆæ‰©å±•å†…å®¹"""
        expanded_contents = []
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº,åªå¤„ç†å‰3ä¸ª
        sorted_gaps = sorted(state["knowledge_gaps"], key=lambda x: x.priority, reverse=True)[:3]
        
        for gap in sorted_gaps:
            gap_type = gap.gap_types[0] if gap.gap_types else "è§£é‡Š"
            
            # ç²¾ç®€ prompt,æ˜ç¡®è¦æ±‚
            template = """ä¸ºå­¦ç”Ÿè¡¥å……è¯´æ˜(150å­—å†…,é€šä¿—æ˜“æ‡‚):

æ¦‚å¿µ: {concept}
éœ€è¦: {gap_type}
PPTåŸæ–‡: {raw_text}

è¡¥å……è¯´æ˜:"""
            
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.llm
            
            response = chain.invoke({
                "concept": gap.concept,
                "gap_type": gap_type,
                "raw_text": state["raw_text"][:500]
            })
            
            expanded_contents.append(ExpandedContent(
                concept=gap.concept,
                gap_type=gap_type,
                content=response.content[:300],  # é™åˆ¶é•¿åº¦
                sources=["AIç”Ÿæˆ"]
            ))
        
        state["expanded_content"] = expanded_contents
        return state


# ==================== Step 4: å¤–éƒ¨æ£€ç´¢å¢å¼º Agent (ä¼˜åŒ–ç­–ç•¥) ====================
class RetrievalAgent:
    """å¤–éƒ¨æ£€ç´¢å¢å¼º Agent - æ™ºèƒ½å¤šæºæ£€ç´¢"""
    
    def __init__(self, llm_config: LLMConfig, vector_db_path: str = "./knowledge_base"):
        self.llm = llm_config.create_llm(temperature=0)
        self.embeddings = OpenAIEmbeddings(
            api_key=llm_config.api_key,
            base_url=llm_config.base_url
        )
        self.vector_db_path = vector_db_path
        self.vectorstore = None
        
        # å¤šæºæ£€ç´¢é…ç½®
        self.sources = {
            "baidu_baike": {"url": "https://baike.baidu.com", "available": False},
            "wikipedia": {"url": "https://zh.wikipedia.org", "available": False},
            "arxiv": {"url": "https://arxiv.org", "available": False},
        }
        self._test_sources()
    
    def _test_sources(self):
        """æµ‹è¯•å¤–éƒ¨æºè¿é€šæ€§"""
        for name, config in self.sources.items():
            config["available"] = test_url_connectivity(config["url"])
    
    def initialize_vectorstore(self, documents: List[Document] = None):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            self.vectorstore = Chroma(
                persist_directory=self.vector_db_path,
                embedding_function=self.embeddings
            )
            if documents:
                self.vectorstore.add_documents(documents)
        except:
            if documents:
                self.vectorstore = Chroma.from_documents(
                    documents=documents,
                    embedding=self.embeddings,
                    persist_directory=self.vector_db_path
                )
    
    def retrieve_local(self, query: str, k: int = 2) -> List[Document]:
        """æœ¬åœ° RAG æ£€ç´¢"""
        if not self.vectorstore:
            return []
        return self.vectorstore.similarity_search(query, k=k)
    
    def retrieve_external(self, query: str, preferred_sources: List[str] = None) -> List[Document]:
        """å¤–éƒ¨æ£€ç´¢ - ä½¿ç”¨MCPRouterï¼ˆåˆå¹¶æ‰€æœ‰å¤–éƒ¨èµ„æºæœç´¢ï¼‰"""
        from ..services.mcp_tools import MCPRouter
        
        docs = []
        
        # å¦‚æœæŒ‡å®šäº†ä¼˜å…ˆæºï¼Œä½¿ç”¨å®ƒä»¬ï¼›å¦åˆ™ä½¿ç”¨æ‰€æœ‰å¯ç”¨æº
        if preferred_sources:
            # æ˜ å°„æºåç§°åˆ°MCPRouterçš„æºåç§°
            source_mapping = {
                "baidu_baike": "baike",
                "baike": "baike",
                "wikipedia": "wikipedia",
                "arxiv": "arxiv"
            }
            
            # è½¬æ¢æºåç§°
            mcp_sources = []
            for source in preferred_sources:
                if source in source_mapping:
                    mcp_sources.append(source_mapping[source])
                elif source in ["baike", "wikipedia", "arxiv"]:
                    mcp_sources.append(source)
            
            if not mcp_sources:
                print(f"   âš ï¸  æ²¡æœ‰å¯æ˜ å°„çš„æº")
                return docs
            
            print(f"   ğŸ” ä½¿ç”¨MCPRouteræœç´¢ï¼ŒæŸ¥è¯¢: '{query}', æº: {mcp_sources}")
        else:
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æº
            available_sources = [name for name, config in self.sources.items() if config["available"]]
            
            if not available_sources:
                print(f"   âš ï¸  æ²¡æœ‰å¯ç”¨çš„å¤–éƒ¨æº")
                return docs
            
            # æ˜ å°„æºåç§°åˆ°MCPRouterçš„æºåç§°
            source_mapping = {
                "baidu_baike": "baike",
                "wikipedia": "wikipedia",
                "arxiv": "arxiv"
            }
            
            # è½¬æ¢æºåç§°
            mcp_sources = []
            for source in available_sources:
                if source in source_mapping:
                    mcp_sources.append(source_mapping[source])
            
            if not mcp_sources:
                print(f"   âš ï¸  æ²¡æœ‰å¯æ˜ å°„çš„æº")
                return docs
            
            print(f"   ğŸ” ä½¿ç”¨MCPRouteræœç´¢ï¼ŒæŸ¥è¯¢: '{query}', æº: {mcp_sources}")
        
        try:
            mcp_router = MCPRouter()
            docs = mcp_router.search(query, preferred_sources=mcp_sources)
            print(f"   âœ… MCPRouterè¿”å› {len(docs)} æ¡ç»“æœ")
            
            # è¿‡æ»¤æ‰å ä½ç¬¦æ–‡æ¡£
            filtered_docs = []
            for doc in docs:
                if "æœªæ‰¾åˆ°" not in doc.page_content:
                    # å…è®¸æœ‰URLæˆ–æ²¡æœ‰URLçš„æ–‡æ¡£ï¼Œåªè¦å†…å®¹æœ‰æ•ˆ
                    filtered_docs.append(doc)
            
            print(f"   âœ… è¿‡æ»¤åå‰©ä½™ {len(filtered_docs)} æ¡æœ‰æ•ˆç»“æœ")
            return filtered_docs
        except Exception as e:
            print(f"   âŒ MCPRouteræœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def run(self, state: GraphState) -> GraphState:
        """æ‰§è¡Œæ£€ç´¢å¢å¼ºï¼ˆåˆå¹¶æ‰€æœ‰å¤–éƒ¨èµ„æºæœç´¢ï¼ŒåŒ…æ‹¬æ ‡é¢˜å’Œæ ¸å¿ƒæ¦‚å¿µï¼Œç™¾åº¦ä½œä¸ºä¿åº•ï¼‰"""
        retrieved_docs = []
        seen_urls = set()  # ç”¨äºå»é‡
        
        # 1. å‡†å¤‡æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼ˆåŒ…æ‹¬æ ‡é¢˜ã€çŸ¥è¯†ç¼ºå£ã€çŸ¥è¯†èšç±»ï¼‰
        search_queries = []
        
        # æ·»åŠ é¡µé¢æ ‡é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
        raw_text = state.get("raw_text", "")
        if raw_text:
            # å°è¯•ä»åŸå§‹æ–‡æœ¬ä¸­æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œæˆ–å‰50å­—ç¬¦ï¼‰
            lines = raw_text.split('\n')
            if lines:
                title = lines[0].strip()[:50]
                if title and len(title) > 2:
                    search_queries.append(title)
        
        # æ·»åŠ çŸ¥è¯†ç¼ºå£æ¦‚å¿µ
        gaps = state.get("knowledge_gaps", [])
        if gaps:
            # ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§ç¼ºå£
            high_priority_gaps = [g for g in gaps if hasattr(g, 'priority') and g.priority >= 4]
            gaps_to_use = high_priority_gaps if high_priority_gaps else gaps[:3]
            
            for gap in gaps_to_use:
                concept = gap.concept if hasattr(gap, 'concept') else gap.get("concept", "")
                if concept and len(concept) <= 50:  # é™åˆ¶é•¿åº¦
                    search_queries.append(concept)
        
        # æ·»åŠ çŸ¥è¯†èšç±»æ¦‚å¿µï¼ˆå¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æŸ¥è¯¢ï¼‰
        if len(search_queries) < 3:
            clusters = state.get("knowledge_clusters", [])
            for cluster in clusters[:3]:
                concept = cluster.get("concept", "") if isinstance(cluster, dict) else ""
                if concept and concept not in search_queries and len(concept) <= 50:
                    search_queries.append(concept)
                    if len(search_queries) >= 5:  # æœ€å¤š5ä¸ªæŸ¥è¯¢
                        break
        
        if not search_queries:
            print("âš ï¸  æ²¡æœ‰å¯æœç´¢çš„æŸ¥è¯¢ï¼Œè·³è¿‡æ£€ç´¢")
            state["retrieved_docs"] = []
            return state
        
        print(f"ğŸ” ä¸º {len(search_queries)} ä¸ªæŸ¥è¯¢æ£€ç´¢å‚è€ƒèµ„æ–™: {search_queries[:3]}")
        
        # 2. æ£€æŸ¥å¤–éƒ¨æºå¯ç”¨æ€§
        self._test_sources()
        available_external = any(s["available"] for s in self.sources.values())
        print(f"   ğŸŒ å¤–éƒ¨æºå¯ç”¨æ€§: {available_external}")
        if available_external:
            for name, config in self.sources.items():
                if config["available"]:
                    print(f"      - {name}: âœ…")
                else:
                    print(f"      - {name}: âŒ")
        
        # 3. ç¡®å®šæœç´¢é¡ºåºï¼šä¼˜å…ˆ arxiv å’Œ wikipediaï¼Œç™¾åº¦ä½œä¸ºä¿åº•
        preferred_sources_order = []
        if self.sources.get("arxiv", {}).get("available"):
            preferred_sources_order.append("arxiv")
        if self.sources.get("wikipedia", {}).get("available"):
            preferred_sources_order.append("wikipedia")
        if self.sources.get("baidu_baike", {}).get("available"):
            preferred_sources_order.append("baike")
        
        # 4. å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œæœç´¢ï¼ˆåˆå¹¶æ‰€æœ‰å¤–éƒ¨èµ„æºï¼‰
        for query in search_queries[:5]:  # æœ€å¤š5ä¸ªæŸ¥è¯¢
            if len(retrieved_docs) >= 10:  # æœ€å¤š10æ¡ç»“æœ
                break
            
            print(f"   ğŸ” æœç´¢æŸ¥è¯¢: '{query}'")
            
            # 4.1 ä¼˜å…ˆæœ¬åœ° RAG
            try:
                local_docs = self.retrieve_local(query, k=2)
                for doc in local_docs:
                    url = doc.metadata.get("url", "")
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        retrieved_docs.append(doc)
                if local_docs:
                    print(f"      ğŸ“š æœ¬åœ°RAGæ‰¾åˆ° {len(local_docs)} æ¡")
            except Exception as e:
                print(f"      âš ï¸  æœ¬åœ°RAGæ£€ç´¢å¤±è´¥: {e}")
            
            # 4.2 å¤–éƒ¨æ£€ç´¢ï¼ˆæ‰€æœ‰å¯ç”¨æºï¼‰
            if preferred_sources_order:
                try:
                    external_docs = self.retrieve_external(query, preferred_sources=preferred_sources_order)
                    for doc in external_docs:
                        url = doc.metadata.get("url", "")
                        # å…è®¸æ²¡æœ‰URLçš„æ–‡æ¡£ï¼ˆå¦‚ç™¾åº¦ç™¾ç§‘ï¼‰ï¼Œä½†è¦å»é‡
                        doc_id = url or doc.page_content[:50]
                        if doc_id not in seen_urls:
                            seen_urls.add(doc_id)
                            retrieved_docs.append(doc)
                    if external_docs:
                        print(f"      ğŸŒ å¤–éƒ¨æ£€ç´¢æ‰¾åˆ° {len(external_docs)} æ¡")
                except Exception as e:
                    print(f"      âš ï¸  å¤–éƒ¨æ£€ç´¢å¤±è´¥: {e}")
        
        # 5. ä¿åº•ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œå°è¯•ç™¾åº¦ä¿åº•æœç´¢
        if len(retrieved_docs) == 0 and self.sources.get("baidu_baike", {}).get("available"):
            print(f"   ğŸ”„ æœªæ‰¾åˆ°ç»“æœï¼Œå°è¯•ç™¾åº¦ä¿åº•æœç´¢...")
            for query in search_queries[:2]:  # æœ€å¤šå°è¯•2ä¸ªæŸ¥è¯¢
                try:
                    from ..services.mcp_tools import MCPRouter
                    mcp_router = MCPRouter()
                    baike_docs = mcp_router.search(query, preferred_sources=["baike"])
                    for doc in baike_docs:
                        url = doc.metadata.get("url", "")
                        doc_id = url or doc.page_content[:50]
                        if doc_id not in seen_urls:
                            seen_urls.add(doc_id)
                            retrieved_docs.append(doc)
                    if baike_docs:
                        print(f"      âœ… ç™¾åº¦ä¿åº•æœç´¢æ‰¾åˆ° {len(baike_docs)} æ¡")
                        break
                except Exception as e:
                    print(f"      âš ï¸  ç™¾åº¦ä¿åº•æœç´¢å¤±è´¥: {e}")
        
        state["retrieved_docs"] = retrieved_docs[:10]  # æœ€å¤š10æ¡
        print(f"âœ… æ£€ç´¢å®Œæˆï¼Œå…± {len(state['retrieved_docs'])} æ¡å‚è€ƒèµ„æ–™")
        return state


# ==================== Step 5: å†…å®¹ä¸€è‡´æ€§æ ¡éªŒ Agent (é˜²å¹»è§‰) ====================
class ConsistencyCheckAgent:
    """å†…å®¹ä¸€è‡´æ€§æ ¡éªŒ Agent - é˜²æ­¢å¹»è§‰"""
    
    def __init__(self, llm_config: LLMConfig):
        self.llm = llm_config.create_llm(temperature=0)
    
    def run(self, state: GraphState) -> GraphState:
        """æ‰§è¡Œä¸€è‡´æ€§æ ¡éªŒï¼ˆä¸æœç´¢å¤–éƒ¨èµ„æºï¼Œåªåšå†…å®¹æ ¡éªŒå’Œä¿®æ­£ï¼‰"""
        # å¦‚æœæ²¡æœ‰è¡¥å……å†…å®¹ï¼Œè·³è¿‡æ ¡éªŒ
        expanded_content = state.get("expanded_content", [])
        if not expanded_content:
            print("âš ï¸  æ²¡æœ‰è¡¥å……å†…å®¹ï¼Œè·³è¿‡ä¸€è‡´æ€§æ ¡éªŒ")
            state["check_result"] = CheckResult(status="pass", issues=[], suggestions=[])
            return state
        
        print("â³ è¿›è¡Œä¸€è‡´æ€§æ ¡éªŒå’Œå†…å®¹æ•´ç†...")
        
        # ä¼˜åŒ–: æ˜ç¡®é˜²å¹»è§‰è¦æ±‚ï¼Œç¡®ä¿ä¸åç¦»æºæ–‡æœ¬
        template = """ä½ æ˜¯å†…å®¹å®¡æ ¸å‘˜ï¼Œè´Ÿè´£æ ¡éªŒå’Œä¿®æ­£è¡¥å……å†…å®¹ï¼Œç¡®ä¿ä¸åç¦»PPTåŸæ–‡ã€‚

PPTåŸæ–‡:
{raw_text}

è¡¥å……å†…å®¹:
{expanded_content}

å‚è€ƒèµ„æ–™ï¼ˆå·²åœ¨æ£€ç´¢é˜¶æ®µè·å–ï¼‰:
{retrieved_docs}

ä»»åŠ¡: ä¸¥æ ¼æ ¡éªŒè¡¥å……å†…å®¹ï¼Œç¡®ä¿ï¼š
1. æ‰€æœ‰å†…å®¹å¿…é¡»åŸºäºPPTåŸæ–‡æˆ–å‚è€ƒèµ„æ–™ï¼Œä¸èƒ½ç¼–é€ 
2. ä¸èƒ½åç¦»PPTåŸæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œä¸»é¢˜
3. è¡¥å……å†…å®¹åº”è¯¥æ˜¯å¯¹åŸæ–‡çš„æ‰©å±•å’Œè§£é‡Šï¼Œä¸èƒ½å¼•å…¥æ— å…³æ¦‚å¿µ
4. å¦‚æœå‘ç°åç¦»æˆ–é”™è¯¯ï¼Œå¿…é¡»ä¿®æ­£

ä¸¥æ ¼æ ¡éªŒå¹¶ä¿®æ­£(JSONæ ¼å¼):
{{
  "status": "passæˆ–revise",
  "issues": ["å…·ä½“é—®é¢˜åˆ—è¡¨ï¼Œå¦‚ï¼š'å¼•å…¥äº†PPTæœªæåŠçš„æ¦‚å¿µX'ã€'åç¦»äº†åŸæ–‡ä¸»é¢˜'ç­‰"],
  "suggestions": ["å…·ä½“ä¿®æ­£å»ºè®®ï¼Œå¦‚ï¼š'åˆ é™¤æ¦‚å¿µXï¼Œæ”¹ä¸ºåŸºäºåŸæ–‡çš„Y'ã€'ä¿®æ­£ä¸ºä¸åŸæ–‡ä¸€è‡´çš„è§‚ç‚¹'ç­‰"],
  "revised_content": ["ä¿®æ­£åçš„è¡¥å……å†…å®¹ï¼Œå¦‚æœstatusæ˜¯passåˆ™ä¿æŒåŸæ ·"]
}}

åŸåˆ™:
1. ç¦æ­¢ç¼–é€ PPTæœªæåŠçš„æ¦‚å¿µ
2. æ‰€æœ‰é™ˆè¿°å¿…é¡»æœ‰ä¾æ®(PPTåŸæ–‡æˆ–å‚è€ƒèµ„æ–™)
3. ä¸ç¡®å®šçš„å†…å®¹å¿…é¡»æ ‡è®°ä¸º"æ¨æµ‹"æˆ–åˆ é™¤
4. å‘ç°åç¦»åŸæ–‡å¿…é¡»reviseå¹¶ä¿®æ­£
5. ä¿®æ­£åçš„å†…å®¹å¿…é¡»ä¸åŸæ–‡ä¿æŒä¸€è‡´
"""
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.llm
        
        # å¤„ç†expanded_contentå¯èƒ½æ˜¯å¯¹è±¡æˆ–å­—å…¸
        expanded_text = "\n".join([
            f"**{ec.concept if hasattr(ec, 'concept') else ec.get('concept', '')}**: {ec.content if hasattr(ec, 'content') else ec.get('content', '')}" 
            for ec in expanded_content
        ])
        
        retrieved_docs = state.get("retrieved_docs", [])
        retrieved_text = "\n".join([
            f"[å‚è€ƒ{i+1}] {doc.page_content[:200] if hasattr(doc, 'page_content') else str(doc)[:200]}"
            for i, doc in enumerate(retrieved_docs[:3])
        ]) if retrieved_docs else "æ— å‚è€ƒèµ„æ–™"
        
        response = chain.invoke({
            "raw_text": state["raw_text"][:1000],  # å¢åŠ åŸæ–‡é•¿åº¦
            "expanded_content": expanded_text or "æ— è¡¥å……å†…å®¹",
            "retrieved_docs": retrieved_text
        })
        
        try:
            # å°è¯•è§£æJSONï¼Œæ”¯æŒmarkdownä»£ç å—
            response_text = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            
            if not response_text:
                print("âš ï¸  ä¸€è‡´æ€§æ ¡éªŒå“åº”ä¸ºç©º")
                check_result = CheckResult(status="pass", issues=[], suggestions=[])
            else:
                # ç§»é™¤markdownä»£ç å—
                if response_text.startswith("```"):
                    lines = response_text.split("\n")
                    json_lines = []
                    in_code_block = False
                    for line in lines:
                        if line.strip().startswith("```"):
                            in_code_block = not in_code_block
                            continue
                        if in_code_block:
                            json_lines.append(line)
                    response_text = "\n".join(json_lines).strip()
                
                # å°è¯•æå–JSONå¯¹è±¡
                import re
                json_match = re.search(r'\{[\s\S]*\}', response_text)
                if json_match:
                    response_text = json_match.group(0)
                
                try:
                    result = json.loads(response_text)
                    check_result = CheckResult(
                        status=result.get("status", "pass"),
                        issues=result.get("issues", []),
                        suggestions=result.get("suggestions", [])
                    )
                    
                    # å¦‚æœæœ‰ä¿®æ­£å»ºè®®ï¼Œæ›´æ–°expanded_content
                    if result.get("status") == "revise" and result.get("revised_content"):
                        print("âœ… æ£€æµ‹åˆ°éœ€è¦ä¿®æ­£çš„å†…å®¹ï¼Œåº”ç”¨ä¿®æ­£...")
                        # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥å¤„ç†ä¿®æ­£åçš„å†…å®¹
                        # æš‚æ—¶ä¿ç•™åŸå†…å®¹ï¼Œä½†è®°å½•ä¿®æ­£å»ºè®®
                except json.JSONDecodeError as je:
                    print(f"âš ï¸  ä¸€è‡´æ€§æ ¡éªŒJSONè§£æå¤±è´¥: {je}")
                    print(f"   å“åº”å†…å®¹å‰200å­—ç¬¦: {response_text[:200]}")
                    check_result = CheckResult(status="pass", issues=[], suggestions=[])
        except Exception as e:
            print(f"âš ï¸  ä¸€è‡´æ€§æ ¡éªŒå¤„ç†å¤±è´¥: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            check_result = CheckResult(status="pass", issues=[], suggestions=[])
        
        state["check_result"] = check_result
        return state


# ==================== Step 6: å†…å®¹ç»“æ„åŒ–æ•´ç† Agent (ç²¾ç®€ç‰ˆ) ====================
class StructuredOrganizationAgent:
    """å†…å®¹ç»“æ„åŒ–æ•´ç† Agent - ç”Ÿæˆå­¦ä¹ ç¬”è®°"""
    
    def __init__(self, llm_config: LLMConfig):
        self.llm = llm_config.create_llm(temperature=0.5)
    
    def run(self, state: GraphState) -> GraphState:
        """æ•´ç†æœ€ç»ˆç¬”è®°ï¼ˆç¡®ä¿ä¸åç¦»æºæ–‡æœ¬ï¼‰"""
        # ä¼˜åŒ–: æ˜ç¡®æ˜¯å­¦ä¹ ç¬”è®°,ä¸èƒ½åç¦»æºæ–‡æœ¬
        template = """æ•´ç†å­¦ä¹ ç¬”è®°(Markdownæ ¼å¼,300å­—å†…)ï¼Œå¿…é¡»ä¸¥æ ¼åŸºäºPPTåŸæ–‡ï¼Œä¸èƒ½åç¦»ã€‚

PPTåŸæ–‡:
{raw_text}

è¡¥å……è¯´æ˜ï¼ˆå·²æ ¡éªŒï¼‰:
{expanded_content}

å‚è€ƒèµ„æ–™:
{references}

ä¸€è‡´æ€§æ ¡éªŒç»“æœ:
{check_result}

æ ¼å¼è¦æ±‚:
## [é¡µé¢æ ‡é¢˜]

### æ ¸å¿ƒæ¦‚å¿µ
- æ¦‚å¿µ1: ç®€è¦è¯´æ˜ï¼ˆå¿…é¡»æ¥è‡ªPPTåŸæ–‡ï¼‰
- æ¦‚å¿µ2: ç®€è¦è¯´æ˜ï¼ˆå¿…é¡»æ¥è‡ªPPTåŸæ–‡ï¼‰

### è¡¥å……ç†è§£
[è¡¥å……å†…å®¹,ç®€æ´æ˜“æ‡‚ï¼Œå¿…é¡»ä¸PPTåŸæ–‡ä¸€è‡´]

### å‚è€ƒ
[å¦‚æœ‰å‚è€ƒèµ„æ–™åˆ—å‡º]

ä¸¥æ ¼åŸåˆ™:
1. æ‰€æœ‰å†…å®¹å¿…é¡»åŸºäºPPTåŸæ–‡ï¼Œä¸èƒ½åç¦»
2. è¡¥å……å†…å®¹åªèƒ½æ˜¯å¯¹åŸæ–‡çš„è§£é‡Šå’Œæ‰©å±•ï¼Œä¸èƒ½å¼•å…¥æ–°æ¦‚å¿µ
3. å¦‚æœä¸€è‡´æ€§æ ¡éªŒå‘ç°é—®é¢˜ï¼Œå¿…é¡»ä¿®æ­£
4. ç®€æ´,çªå‡ºé‡ç‚¹
5. ä¸é‡å¤PPTåŸæ–‡ï¼Œä½†å¿…é¡»ä¸åŸæ–‡ä¿æŒä¸€è‡´
"""
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.llm
        
        expanded_text = "\n".join([
            f"**{ec.concept if hasattr(ec, 'concept') else ec.get('concept', '')}**: {ec.content if hasattr(ec, 'content') else ec.get('content', '')}" 
            for ec in state.get("expanded_content", [])
        ])
        
        retrieved_docs = state.get("retrieved_docs", [])
        references_text = "\n".join([
            f"- {doc.metadata.get('title', '')} ({doc.metadata.get('source', '')})" 
            for doc in retrieved_docs[:3]
        ]) if retrieved_docs else "æ— å‚è€ƒèµ„æ–™"
        
        check_result = state.get("check_result", CheckResult(status="pass", issues=[], suggestions=[]))
        check_text = f"çŠ¶æ€: {check_result.status}\né—®é¢˜: {', '.join(check_result.issues) if check_result.issues else 'æ— '}\nå»ºè®®: {', '.join(check_result.suggestions) if check_result.suggestions else 'æ— '}"
        
        response = chain.invoke({
            "raw_text": state["raw_text"][:1500],  # å¢åŠ åŸæ–‡é•¿åº¦ä»¥ç¡®ä¿ä¸åç¦»
            "expanded_content": expanded_text or "æ— è¡¥å……å†…å®¹",
            "references": references_text,
            "check_result": check_text
        })
        
        state["final_notes"] = response.content.strip()
        return state
        
        response = chain.invoke({
            "raw_text": state["raw_text"][:500],
            "expanded_content": expanded_text or "æ— è¡¥å……å†…å®¹"
        })
        
        state["final_notes"] = response.content
        state["streaming_chunks"] = [response.content]
        return state