import os
import tempfile
import json
from typing import Any, Dict, List, Optional, Union
from datetime import datetime
import uuid

from fastapi import FastAPI, File, UploadFile, HTTPException, Depends, WebSocket, Query, Body, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse

from src.utils.helpers import ensure_supported_ext, save_upload_to_temp, download_to_temp
from src.services.ppt_parser_service import DocumentParserService
from src.services.ppt_expansion_service import PPTExpansionService
from src.services.page_analysis_service import PageDeepAnalysisService
from src.services.ai_tutor_service import AITutorService, ChatMessage
from src.services.reference_search_service import ReferenceSearchService
from src.services.vector_store_service import VectorStoreService
from src.agents.base import LLMConfig
from pydantic import BaseModel, Field

from src.services.mindmap_service import MindmapService
from src.services.persistence_service import PersistenceService

app = FastAPI(title="PPTAS Backend", version="0.2.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

_ai_tutor_service = None
_page_analysis_service = None
_persistence_service = None

def get_ai_tutor():
    """è·å– AI åŠ©æ•™æœåŠ¡å•ä¾‹"""
    global _ai_tutor_service
    if _ai_tutor_service is None:
        config = load_config()
        llm_config = LLMConfig(
            api_key=config["llm"]["api_key"],
            base_url=config["llm"]["base_url"],
            model=config["llm"]["model"]
        )
        _ai_tutor_service = AITutorService(llm_config)
    return _ai_tutor_service

def get_page_analysis():
    """è·å–é¡µé¢åˆ†ææœåŠ¡å•ä¾‹"""
    global _page_analysis_service
    if _page_analysis_service is None:
        config = load_config()
        llm_config = LLMConfig(
            api_key=config["llm"]["api_key"],
            base_url=config["llm"]["base_url"],
            model=config["llm"]["model"]
        )
        _page_analysis_service = PageDeepAnalysisService(llm_config)
    return _page_analysis_service
# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================
class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    page_id: int
    message: str


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    page_id: int
    response: str
    timestamp: str


class PageAnalysisRequest(BaseModel):
    """é¡µé¢åˆ†æè¯·æ±‚"""
    doc_id: Optional[str] = None  # å…³è”æ–‡æ¡£IDï¼Œç”¨äºç¼“å­˜/æŒä¹…åŒ–
    page_id: int
    title: str
    content: str
    raw_points: Optional[list] = None
    key_concepts: Optional[list] = None  # å…³é”®æ¦‚å¿µåˆ—è¡¨
    analysis: Optional[str] = None  # æ·±åº¦åˆ†æå†…å®¹
    force: Optional[bool] = False  # å¼ºåˆ¶é‡æ–°åˆ†æï¼Œå¿½ç•¥ç¼“å­˜


class ReferenceSearchRequest(BaseModel):
    """å‚è€ƒæ–‡çŒ®æœç´¢è¯·æ±‚"""
    query: str
    max_results: int = 10
    search_type: Optional[str] = None  # "academic" | "general" | None


class SemanticSearchRequest(BaseModel):
    """è¯­ä¹‰æœç´¢è¯·æ±‚"""
    query: str
    top_k: int = 5
    file_name: Optional[str] = None
    file_type: Optional[str] = None  # "pdf" æˆ– "pptx"
    min_score: float = 0.0


# ==================== é…ç½®åŠ è½½ ====================
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(__file__), "..", "..", "config.json")
    
    if not os.path.exists(config_path):
        config_path = os.path.join(os.path.dirname(__file__), "..", "config.json")
    
    if os.path.exists(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    # é»˜è®¤é…ç½®
    return {
        "llm": {
            "api_key": os.getenv("api_key", ""),
            "base_url": os.getenv("base_url", "https://api.openai.com/v1"),
            "model": os.getenv("model", "gpt-4")
        },
        "retrieval": {
            "preferred_sources": ["arxiv", "wikipedia"],
            "max_results": 3,
            "local_rag_priority": True
        },
        "expansion": {
            "max_revisions": 2,
            "min_gap_priority": 3,
            "temperature": 0.7
        }
    }


def get_persistence_service() -> PersistenceService:
    """è·å– SQLite æŒä¹…åŒ–æœåŠ¡å•ä¾‹"""
    global _persistence_service
    if _persistence_service is None:
        backend_root = os.path.join(os.path.dirname(__file__), "..")  # backend/src -> backend/
        db_path = os.path.abspath(os.path.join(backend_root, "pptas_cache.sqlite3"))
        _persistence_service = PersistenceService(db_path=db_path)
        print(f"ğŸ—„ï¸  SQLite æŒä¹…åŒ–å¯ç”¨: {db_path}")
    return _persistence_service


_persistence_service = None


def get_persistence_service() -> PersistenceService:
    """è·å– SQLite æŒä¹…åŒ–æœåŠ¡å•ä¾‹"""
    global _persistence_service
    if _persistence_service is None:
        backend_root = os.path.join(os.path.dirname(__file__), "..")  # backend/src -> backend/
        db_path = os.path.abspath(os.path.join(backend_root, "pptas_cache.sqlite3"))
        _persistence_service = PersistenceService(db_path=db_path)
        print(f"ğŸ—„ï¸  SQLite æŒä¹…åŒ–å¯ç”¨: {db_path}")
    return _persistence_service


def get_parser_service():
    return DocumentParserService()

def get_mindmap_service():
    return MindmapService()


class MindmapRequest(BaseModel):
    title: str = Field(default="", description="Slide title")
    raw_points: Optional[List[Union[str, Dict[str, Any]]]] = Field(
        default=None,
        description="Slide points; supports plain strings or objects like {text, level}.",
    )
    max_depth: int = Field(default=4, ge=1, le=8)
    max_children_per_node: int = Field(default=20, ge=1, le=100)


class SlidePoint(BaseModel):
    text: str
    level: int = Field(default=0, ge=0)


class SlideItem(BaseModel):
    title: str
    page_num: Optional[int] = None
    raw_points: Optional[List[Union[str, Dict[str, Any], SlidePoint]]] = None


class MindmapFromSlidesRequest(BaseModel):
    title: Optional[str] = Field(default=None, description="æ•´ä½“ PPT æ ‡é¢˜ï¼Œå¯é€‰")
    slides: List[SlideItem]
    max_depth: int = Field(default=4, ge=1, le=8)
    max_children_per_node: int = Field(default=20, ge=1, le=100)


class MindmapFromGlobalAnalysisRequest(BaseModel):
    doc_id: str = Field(description="æ–‡æ¡£IDï¼Œç”¨äºè·å–å…¨å±€åˆ†æç»“æœ")
    title: Optional[str] = Field(default=None, description="æ•´ä½“ PPT æ ‡é¢˜ï¼Œå¯é€‰ï¼ˆé»˜è®¤ä½¿ç”¨å…¨å±€åˆ†æçš„ä¸»é¢˜ï¼‰")
    max_depth: int = Field(default=4, ge=1, le=8)
    max_children_per_node: int = Field(default=20, ge=1, le=100)


@app.post("/api/v1/mindmap")
async def build_mindmap(
    payload: MindmapRequest,
    svc: MindmapService = Depends(get_mindmap_service),
):
    """
    Build a mindmap tree for the frontend "æ€ç»´å¯¼å›¾" tab.
    Returns: { root: {id,label,children:[...] } }
    """
    return svc.build_mindmap(
        title=payload.title,
        raw_points=payload.raw_points,
        max_depth=payload.max_depth,
        max_children_per_node=payload.max_children_per_node,
    )


@app.post("/api/v1/mindmap/from-slides")
async def build_mindmap_from_slides(
    payload: MindmapFromSlidesRequest,
    svc: MindmapService = Depends(get_mindmap_service),
):
    """
    Build a mindmap for the entire PPT (all slides).
    Expects slides from /api/v1/expand-ppt output.
    """
    return svc.build_mindmap_for_ppt(
        slides=[s.model_dump() for s in payload.slides],
        deck_title=payload.title or "PPT Mindmap",
        max_depth=payload.max_depth,
        max_children_per_node=payload.max_children_per_node,
    )


@app.post("/api/v1/mindmap/from-global-analysis")
async def build_mindmap_from_global_analysis(
    payload: MindmapFromGlobalAnalysisRequest,
    svc: MindmapService = Depends(get_mindmap_service),
    persistence: PersistenceService = Depends(get_persistence_service),
):
    """
    Build a mindmap from global analysis results.
    Uses the global_analysis_json stored in the database for the given doc_id.
    """
    # è·å–æ–‡æ¡£å’Œå…¨å±€åˆ†æç»“æœ
    doc = persistence.get_document_by_id(payload.doc_id)
    if not doc:
        raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ä¸å­˜åœ¨: {payload.doc_id}")
    
    global_analysis = doc.get("global_analysis")
    if not global_analysis:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡æ¡£ {payload.doc_id} å°šæœªè¿›è¡Œå…¨å±€åˆ†æï¼Œè¯·å…ˆæ‰§è¡Œå…¨å±€åˆ†æ"
        )
    
    # ä½¿ç”¨å…¨å±€åˆ†æç»“æœç”Ÿæˆæ€ç»´å¯¼å›¾
    return svc.build_mindmap_from_global_analysis(
        global_analysis=global_analysis,
        deck_title=payload.title,
        max_depth=payload.max_depth,
        max_children_per_node=payload.max_children_per_node,
    )

def get_expansion_service():
    config = load_config()
    llm_config = LLMConfig(
        api_key=config["llm"]["api_key"],
        base_url=config["llm"]["base_url"],
        model=config["llm"]["model"]
    )
    return PPTExpansionService(llm_config)


def get_page_analysis_service():
    config = load_config()
    llm_config = LLMConfig(
        api_key=config["llm"]["api_key"],
        base_url=config["llm"]["base_url"],
        model=config["llm"]["model"]
    )
    return PageDeepAnalysisService(llm_config)


def get_ai_tutor_service():
    config = load_config()
    llm_config = LLMConfig(
        api_key=config["llm"]["api_key"],
        base_url=config["llm"]["base_url"],
        model=config["llm"]["model"]
    )
    return AITutorService(llm_config)


def get_reference_search_service():
    return ReferenceSearchService()


def get_vector_store_service():
    """è·å–å‘é‡å­˜å‚¨æœåŠ¡å®ä¾‹"""
    config = load_config()
    llm_config = LLMConfig(
        api_key=config["llm"]["api_key"],
        base_url=config["llm"]["base_url"],
        model=config["llm"]["model"]
    )
    # ä¼˜å…ˆä½¿ç”¨ vector_store é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ knowledge_base è·¯å¾„
    vector_db_path = config.get("vector_store", {}).get("path") or config.get("knowledge_base", {}).get("path", "./ppt_vector_db")
    return VectorStoreService(llm_config, vector_db_path)


# ==================== API ç«¯ç‚¹ ====================

@app.post("/api/v1/expand-ppt")
async def expand_ppt(
    file: Optional[UploadFile] = File(None),
    url_query: Optional[str] = Query(None, alias="url"),
    url_body: Optional[str] = Body(None),
    url_form: Optional[str] = Form(None, alias="url"),
    parser: DocumentParserService = Depends(get_parser_service),
    vector_store: VectorStoreService = Depends(get_vector_store_service),
    persistence: PersistenceService = Depends(get_persistence_service),
):
    """æ¥æ”¶ PPTX/PDF æ–‡ä»¶æˆ– URLï¼Œè¿”å›è§£æåçš„é€»è¾‘ç»“æ„ï¼Œå¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ã€‚"""
    incoming_url = (url_form or url_body or url_query or "").strip() if url_form or url_body or url_query else None

    if not file and not incoming_url:
        raise HTTPException(status_code=400, detail="éœ€è¦ä¸Šä¼ æ–‡ä»¶æˆ–æä¾› url")

    tmp_path = None
    try:
        if incoming_url:
            tmp_path, filename = download_to_temp(incoming_url)
        else:
            tmp_path, filename = await save_upload_to_temp(file)

        ext = ensure_supported_ext(filename)

        file_hash = persistence.sha256_file(tmp_path)
        existing_doc = persistence.get_document_by_hash(file_hash)
        if existing_doc:
            print(f"â™»ï¸  å‘½ä¸­æ–‡æ¡£ç¼“å­˜: {filename} hash={file_hash[:12]} doc_id={existing_doc['doc_id']}")
            return {
                "doc_id": existing_doc["doc_id"],
                "file_hash": file_hash,
                "slides": existing_doc.get("slides", []),
                "cached": True,
            }

        slides = parser.parse_document(tmp_path, ext)
        
        try:
            file_type = ext[1:] if ext.startswith('.') else ext  # ç§»é™¤ç‚¹å·
            store_result = vector_store.store_document_slides(
                file_name=filename,
                file_type=file_type,
                slides=slides
            )
            print(f"âœ… å·²å­˜å‚¨ {store_result['total_chunks']} ä¸ªåˆ‡ç‰‡åˆ°å‘é‡æ•°æ®åº“")
        except Exception as e:
            print(f"âš ï¸  å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")

        # æ¯æ¬¡ä¸Šä¼ åéƒ½ä¿å­˜è§£æç»“æœï¼ˆä¾›ä¸‹æ¬¡åŒ PPT å¤ç”¨ï¼‰
        doc_id = str(uuid.uuid4())
        file_type = ext[1:] if ext.startswith('.') else ext
        persistence.upsert_document(
            doc_id=doc_id,
            file_name=filename,
            file_type=file_type,
            file_hash=file_hash,
            slides=slides,
        )
        
        return {
            "doc_id": doc_id,
            "file_hash": file_hash,
            "slides": slides,
            "vector_store": {
                "stored": True,
                "total_chunks": store_result.get("total_chunks", 0) if 'store_result' in locals() else 0
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})
    finally:
        if tmp_path and os.path.exists(tmp_path):
            os.remove(tmp_path)


@app.post("/api/v1/analyze-page")
async def analyze_page(
    request: PageAnalysisRequest,
    service: PageDeepAnalysisService = Depends(get_page_analysis_service),
    persistence: PersistenceService = Depends(get_persistence_service),
):
    """å¯¹å•ä¸ªé¡µé¢è¿›è¡Œæ·±åº¦åˆ†æ - ä¼˜åŒ–çš„ Agent æµç¨‹
    
    Args:
        request: åˆ†æè¯·æ±‚
        service: åˆ†ææœåŠ¡
    
    Returns:
        é¡µé¢æ·±åº¦åˆ†æç»“æœï¼ˆç»“æ„åŒ–åˆ†æã€çŸ¥è¯†ç¼ºå£ã€è¡¥å……è¯´æ˜ç­‰ï¼‰
    """
    try:
        # å¦‚æœ force=Trueï¼Œåˆ™å¿½ç•¥ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°åˆ†æ
        if request.doc_id and not request.force:
            cached = persistence.get_page_analysis(request.doc_id, request.page_id)
            if cached:
                return {"success": True, "cached": True, "data": cached}

        result = service.analyze_page(
            page_id=request.page_id,
            title=request.title,
            content=request.content,
            raw_points=request.raw_points
        )
        payload = {
            "success": True,
            "data": {
                "page_id": result.page_id,
                "title": result.title,
                "raw_content": result.raw_content,
                "page_structure": result.page_structure,
                "knowledge_clusters": result.knowledge_clusters,
                "understanding_notes": result.understanding_notes,
                "knowledge_gaps": result.knowledge_gaps,
                "expanded_content": result.expanded_content,
                "references": result.references,
                "raw_points": result.raw_points
            }
        }

        if request.doc_id:
            persistence.upsert_page_analysis(request.doc_id, request.page_id, payload["data"])
        return payload
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"âŒ é¡µé¢åˆ†æé”™è¯¯: {error_trace}")
        return JSONResponse(
            status_code=500,
            content={"error": str(e), "details": error_trace}
        )


@app.get("/api/v1/page-analysis")
async def get_page_analysis_api(
    doc_id: str = Query(..., description="ä¸Šä¼ è¿”å›çš„æ–‡æ¡£ID"),
    page_id: int = Query(..., description="é¡µé¢ç¼–å·ï¼Œä»1å¼€å§‹"),
    persistence: PersistenceService = Depends(get_persistence_service),
):
    """è·å–å•é¡µå†å²åˆ†æï¼ˆè‹¥å­˜åœ¨ï¼‰ã€‚"""
    cached = persistence.get_page_analysis(doc_id, page_id)
    return {"success": True, "data": cached}


@app.get("/api/v1/page-analysis/all")
async def get_all_page_analysis(
    doc_id: str = Query(..., description="ä¸Šä¼ è¿”å›çš„æ–‡æ¡£ID"),
    persistence: PersistenceService = Depends(get_persistence_service),
):
    """è·å–æ–‡æ¡£æ‰€æœ‰å·²ä¿å­˜çš„é¡µåˆ†æï¼ˆå­—å…¸ï¼Œkey ä¸º page_idï¼‰ã€‚"""
    data = persistence.list_page_analyses(doc_id)
    return {"success": True, "data": data}


class GlobalAnalysisRequest(BaseModel):
    """å…¨å±€åˆ†æè¯·æ±‚"""
    doc_id: str
    force: Optional[bool] = False  # å¼ºåˆ¶é‡æ–°åˆ†æï¼Œå¿½ç•¥ç¼“å­˜


@app.post("/api/v1/analyze-document-global")
async def analyze_document_global(
    request: GlobalAnalysisRequest,
    service: PageDeepAnalysisService = Depends(get_page_analysis_service),
    persistence: PersistenceService = Depends(get_persistence_service),
):
    """å¯¹æ•´ä¸ªæ–‡æ¡£è¿›è¡Œå…¨å±€åˆ†æï¼Œè·å–ä¸»é¢˜å’ŒçŸ¥è¯†ç‚¹æ¡†æ¶
    
    è¿™ä¸ªæ¥å£åº”è¯¥åœ¨æ–‡æ¡£ä¸Šä¼ åè°ƒç”¨ï¼Œç”¨äºï¼š
    1. åˆ†ææ•´ä¸ªæ–‡æ¡£çš„ä¸»é¢˜å’Œç»“æ„
    2. æå–å…¨å±€çŸ¥è¯†ç‚¹æ¡†æ¶
    3. è¯†åˆ«çŸ¥è¯†é€»è¾‘æµç¨‹
    
    Args:
        request: å…¨å±€åˆ†æè¯·æ±‚ï¼ŒåŒ…å« doc_id å’Œå¯é€‰çš„ force å‚æ•°
    """
    try:
        doc = persistence.get_document_by_id(request.doc_id)
        if not doc:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ–‡æ¡£")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å…¨å±€åˆ†æï¼ˆé™¤éå¼ºåˆ¶é‡æ–°åˆ†æï¼‰
        if not request.force and doc.get("global_analysis"):
            print(f"â™»ï¸  æ–‡æ¡£ {request.doc_id} å·²æœ‰å…¨å±€åˆ†æï¼Œç›´æ¥è¿”å›")
            return {
                "success": True,
                "doc_id": request.doc_id,
                "global_analysis": doc["global_analysis"],
                "cached": True
            }
        
        if request.force:
            print(f"ğŸ”„ å¼ºåˆ¶é‡æ–°è¿›è¡Œå…¨å±€åˆ†æï¼Œå¿½ç•¥ç¼“å­˜ (doc_id={request.doc_id})")
        
        slides = doc.get("slides", [])
        if not slides:
            raise HTTPException(status_code=400, detail="æ–‡æ¡£æ²¡æœ‰slidesæ•°æ®")
        
        # æå–æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬å†…å®¹
        ppt_texts = []
        for slide in slides:
            # æå–æ–‡æœ¬å†…å®¹
            content_parts = []
            if slide.get("title"):
                content_parts.append(f"æ ‡é¢˜: {slide['title']}")
            if slide.get("raw_points"):
                for point in slide["raw_points"]:
                    if isinstance(point, dict):
                        content_parts.append(point.get("text", ""))
                    elif isinstance(point, str):
                        content_parts.append(point)
            if slide.get("raw_content"):
                content_parts.append(slide["raw_content"])
            
            slide_text = "\n".join(content_parts)
            if slide_text.strip():
                ppt_texts.append(slide_text)
        
        print(f"ğŸ“Š å¼€å§‹å…¨å±€åˆ†æï¼Œæ–‡æ¡£ {request.doc_id}ï¼Œå…± {len(ppt_texts)} é¡µ")
        
        # ä½¿ç”¨ GlobalStructureAgent è¿›è¡Œå…¨å±€åˆ†æ
        from src.agents.models import CheckResult
        state = {
            "ppt_texts": ppt_texts,
            "global_outline": {},
            "knowledge_units": [],
            "current_unit_id": "global",
            "current_page_id": 0,
            "raw_text": "\n\n".join([f"ç¬¬{i+1}é¡µ:\n{text}" for i, text in enumerate(ppt_texts)]),
            "page_structure": {},
            "knowledge_clusters": [],
            "understanding_notes": "",
            "knowledge_gaps": [],
            "expanded_content": [],
            "retrieved_docs": [],
            "check_result": CheckResult(status="pass", issues=[], suggestions=[]),
            "final_notes": "",
            "revision_count": 0,
            "max_revisions": 1,
            "streaming_chunks": []
        }
        
        # æ­¥éª¤1: å…¨å±€ç»“æ„è§£æ
        print("â³ å¼€å§‹å…¨å±€ç»“æ„è§£æ...")
        state = service.structure_agent.run(state)
        global_outline = state.get("global_outline", {})
        print(f"âœ… å…¨å±€ç»“æ„è§£æå®Œæˆ: {global_outline.get('main_topic', 'æœªçŸ¥ä¸»é¢˜')}")
        
        # æ­¥éª¤2: å…¨å±€çŸ¥è¯†ç‚¹èšç±»
        print("â³ å¼€å§‹å…¨å±€çŸ¥è¯†ç‚¹èšç±»...")
        from src.agents.base import KnowledgeClusteringAgent
        clustering_agent = KnowledgeClusteringAgent(service.llm_config)
        state = clustering_agent.run(state)
        knowledge_units = state.get("knowledge_units", [])
        print(f"âœ… å…¨å±€çŸ¥è¯†ç‚¹èšç±»å®Œæˆ: {len(knowledge_units)} ä¸ªçŸ¥è¯†ç‚¹å•å…ƒ")
        
        # æ„å»ºå…¨å±€åˆ†æç»“æœ
        global_analysis = {
            "main_topic": global_outline.get("main_topic", ""),
            "chapters": global_outline.get("chapters", []),
            "knowledge_flow": global_outline.get("knowledge_flow", ""),
            "knowledge_units": [
                {
                    "unit_id": unit.unit_id,
                    "title": unit.title,
                    "pages": unit.pages,
                    "core_concepts": unit.core_concepts
                } for unit in knowledge_units
            ],
            "total_pages": len(ppt_texts)
        }
        
        # ä¿å­˜å…¨å±€åˆ†æç»“æœ
        persistence.update_global_analysis(request.doc_id, global_analysis)
        print(f"âœ… å…¨å±€åˆ†æå®Œæˆå¹¶å·²ä¿å­˜: {request.doc_id}")
        
        return {
            "success": True,
            "doc_id": request.doc_id,
            "global_analysis": global_analysis,
            "cached": False
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å…¨å±€åˆ†æå¤±è´¥: {str(e)}")

@app.post("/api/v1/analyze-page-stream")
async def analyze_page_stream(
    request: PageAnalysisRequest,
    service: PageDeepAnalysisService = Depends(get_page_analysis_service),
    persistence: PersistenceService = Depends(get_persistence_service),
):
    """å¯¹å•ä¸ªé¡µé¢è¿›è¡Œæµå¼æ·±åº¦åˆ†æ - å®æ—¶è¿”å›å„ Agent çš„ç»“æœ
    
    Args:
        request: åˆ†æè¯·æ±‚
        service: åˆ†ææœåŠ¡
    
    Returns:
        Server-Sent Events æµå¼å“åº”
    """
    async def event_generator():
        try:
            # å¦‚æœ force=False ä¸”æœ‰ç¼“å­˜ï¼Œåˆ™ç›´æ¥å›æ”¾ç¼“å­˜
            print(f"ğŸ” æµå¼åˆ†æè¯·æ±‚: doc_id={request.doc_id}, page_id={request.page_id}, force={request.force}")
            if request.doc_id and not request.force:
                cached = persistence.get_page_analysis(request.doc_id, request.page_id)
                if cached:
                    print(f"âœ… æ‰¾åˆ°ç¼“å­˜åˆ†æç»“æœï¼Œç›´æ¥è¿”å› (doc_id={request.doc_id}, page_id={request.page_id})")
                    yield f"data: {json.dumps({'stage': 'clustering', 'data': cached.get('knowledge_clusters', []), 'message': 'å·²åŠ è½½å†å²åˆ†æï¼šçŸ¥è¯†èšç±»', 'cached': True})}\n\n"
                    yield f"data: {json.dumps({'stage': 'understanding', 'data': cached.get('understanding_notes', ''), 'message': 'å·²åŠ è½½å†å²åˆ†æï¼šå­¦ä¹ ç¬”è®°', 'cached': True})}\n\n"
                    yield f"data: {json.dumps({'stage': 'gaps', 'data': cached.get('knowledge_gaps', []), 'message': 'å·²åŠ è½½å†å²åˆ†æï¼šçŸ¥è¯†ç¼ºå£', 'cached': True})}\n\n"
                    yield f"data: {json.dumps({'stage': 'expansion', 'data': cached.get('expanded_content', []), 'message': 'å·²åŠ è½½å†å²åˆ†æï¼šè¡¥å……è¯´æ˜', 'cached': True})}\n\n"
                    yield f"data: {json.dumps({'stage': 'retrieval', 'data': cached.get('references', []), 'message': 'å·²åŠ è½½å†å²åˆ†æï¼šå‚è€ƒèµ„æ–™', 'cached': True})}\n\n"
                    yield f"data: {json.dumps({'stage': 'complete', 'data': cached, 'message': 'å†å²åˆ†æåŠ è½½å®Œæˆ', 'cached': True})}\n\n"
                    return
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°ç¼“å­˜åˆ†æç»“æœ (doc_id={request.doc_id}, page_id={request.page_id})")
            elif not request.doc_id:
                print(f"âš ï¸ doc_id ä¸ºç©ºï¼Œæ— æ³•æ£€æŸ¥ç¼“å­˜ (page_id={request.page_id})")
            elif request.force:
                print(f"ğŸ”„ å¼ºåˆ¶é‡æ–°åˆ†æï¼Œå¿½ç•¥ç¼“å­˜ (doc_id={request.doc_id}, page_id={request.page_id})")
            
            # å¦‚æœæ˜¯å¼ºåˆ¶é‡æ–°åˆ†æï¼Œè¾“å‡ºæç¤º
            if request.force:
                yield f"data: {json.dumps({'stage': 'info', 'data': {}, 'message': 'ğŸ”„ å¼ºåˆ¶é‡æ–°åˆ†æï¼Œå¿½ç•¥ç¼“å­˜...'})}\n\n"

            # è·å–å…¨å±€åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            global_analysis = None
            if request.doc_id:
                doc = persistence.get_document_by_id(request.doc_id)
                if doc and doc.get("global_analysis"):
                    global_analysis = doc["global_analysis"]
                    print(f"ğŸ“š åŠ è½½å…¨å±€åˆ†æç»“æœ: ä¸»é¢˜={global_analysis.get('main_topic', 'æœªçŸ¥')}, çŸ¥è¯†ç‚¹å•å…ƒ={len(global_analysis.get('knowledge_units', []))}")
                else:
                    print(f"âš ï¸  æ–‡æ¡£ {request.doc_id} æ²¡æœ‰å…¨å±€åˆ†æç»“æœï¼Œå°†ä»…åŸºäºå½“å‰é¡µé¢åˆ†æ")
            
            # æ­¥éª¤1: çŸ¥è¯†èšç±»ï¼ˆåŸºäºå…¨å±€ä¸Šä¸‹æ–‡ï¼‰
            print("â³ å¼€å§‹çŸ¥è¯†èšç±»...")
            yield f"data: {json.dumps({'stage': 'clustering', 'data': [], 'message': 'æ­£åœ¨åˆ†æéš¾ç‚¹æ¦‚å¿µ...'})}\n\n"
            
            # å¦‚æœæœ‰å…¨å±€åˆ†æï¼Œå°†å…¨å±€çŸ¥è¯†ç‚¹å•å…ƒä¼ é€’ç»™èšç±»agent
            knowledge_clusters = service.clustering_agent.run(
                request.content,
                global_context=global_analysis
            )
            print(f"âœ… çŸ¥è¯†èšç±»å®Œæˆ: {len(knowledge_clusters)} ä¸ªæ¦‚å¿µ")
            clustering_msg = f'è¯†åˆ«äº† {len(knowledge_clusters)} ä¸ªéš¾ç‚¹æ¦‚å¿µ'
            yield f"data: {json.dumps({'stage': 'clustering', 'data': knowledge_clusters, 'message': clustering_msg})}\n\n"
            
            # æ­¥éª¤2: å­¦ä¹ ç¬”è®°
            print("â³ å¼€å§‹ç”Ÿæˆå­¦ä¹ ç¬”è®°...")
            yield f"data: {json.dumps({'stage': 'understanding', 'data': '', 'message': 'æ­£åœ¨ç”Ÿæˆå­¦ä¹ ç¬”è®°...'})}\n\n"
            
            from src.agents.models import CheckResult
            
            # æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡æ•°æ®
            global_outline = {}
            knowledge_units = []
            if global_analysis:
                global_outline = {
                    "main_topic": global_analysis.get("main_topic", ""),
                    "chapters": global_analysis.get("chapters", []),
                    "knowledge_flow": global_analysis.get("knowledge_flow", "")
                }
                # è½¬æ¢knowledge_unitsæ ¼å¼
                for unit in global_analysis.get("knowledge_units", []):
                    from src.agents.models import KnowledgeUnit
                    knowledge_units.append(KnowledgeUnit(
                        unit_id=unit.get("unit_id", ""),
                        title=unit.get("title", ""),
                        pages=unit.get("pages", []),
                        core_concepts=unit.get("core_concepts", []),
                        raw_texts=[]
                    ))
            
            state = {
                "ppt_texts": [request.content],
                "global_outline": global_outline,
                "knowledge_units": knowledge_units,
                "current_unit_id": f"page_{request.page_id}",
                "current_page_id": request.page_id,
                "raw_text": request.content,
                "page_structure": {},
                "knowledge_clusters": knowledge_clusters,
                "understanding_notes": "",
                "knowledge_gaps": [],
                "expanded_content": [],
                "retrieved_docs": [],
                "check_result": CheckResult(status="pass", issues=[], suggestions=[]),
                "final_notes": "",
                "revision_count": 0,
                "max_revisions": 1,
                "streaming_chunks": []
            }
            
            state = service.understanding_agent.run(state)
            understanding_notes = state.get("understanding_notes", "")
            print(f"âœ… å­¦ä¹ ç¬”è®°å®Œæˆ")
            yield f"data: {json.dumps({'stage': 'understanding', 'data': understanding_notes, 'message': 'å­¦ä¹ ç¬”è®°å·²ç”Ÿæˆ'})}\n\n"
            
            # æ­¥éª¤3: çŸ¥è¯†ç¼ºå£
            print("â³ å¼€å§‹è¯†åˆ«çŸ¥è¯†ç¼ºå£...")
            yield f"data: {json.dumps({'stage': 'gaps', 'data': [], 'message': 'æ­£åœ¨è¯†åˆ«çŸ¥è¯†ç¼ºå£...'})}\n\n"
            
            state = service.gap_agent.run(state)
            gaps_data = [
                {
                    "concept": gap.concept,
                    "gap_types": gap.gap_types,
                    "priority": gap.priority
                } for gap in state.get("knowledge_gaps", [])
            ]
            print(f"âœ… ç¼ºå£è¯†åˆ«å®Œæˆ: {len(gaps_data)} ä¸ªç¼ºå£")
            gaps_msg = f'è¯†åˆ«äº† {len(gaps_data)} ä¸ªç†è§£ç¼ºå£'
            yield f"data: {json.dumps({'stage': 'gaps', 'data': gaps_data, 'message': gaps_msg})}\n\n"
            
            # æ­¥éª¤4: çŸ¥è¯†æ‰©å±•
            print("â³ å¼€å§‹ç”Ÿæˆè¡¥å……è¯´æ˜...")
            yield f"data: {json.dumps({'stage': 'expansion', 'data': [], 'message': 'æ­£åœ¨ç”Ÿæˆè¡¥å……è¯´æ˜...'})}\n\n"
            
            state = service.expansion_agent.run(state)
            expanded_data = []
            if state.get("expanded_content"):
                for ec in state["expanded_content"]:
                    if hasattr(ec, 'concept'):
                        expanded_data.append({
                            "concept": ec.concept,
                            "gap_type": ec.gap_type,
                            "content": ec.content,
                            "sources": ec.sources
                        })
                    else:
                        expanded_data.append(ec)
            print(f"âœ… è¡¥å……è¯´æ˜å®Œæˆ: {len(expanded_data)} æ¡")
            expansion_msg = f'ç”Ÿæˆäº† {len(expanded_data)} æ¡è¡¥å……è¯´æ˜'
            yield f"data: {json.dumps({'stage': 'expansion', 'data': expanded_data, 'message': expansion_msg})}\n\n"
            
            # æ­¥éª¤5: å¤–éƒ¨æ£€ç´¢
            print("â³ å¼€å§‹æœç´¢å‚è€ƒèµ„æ–™...")
            yield f"data: {json.dumps({'stage': 'retrieval', 'data': [], 'message': 'æ­£åœ¨æœç´¢å‚è€ƒèµ„æ–™...'})}\n\n"
            
            state = service.retrieval_agent.run(state)
            retrieved_count = len(state.get('retrieved_docs', []))
            print(f"âœ… æ£€ç´¢å®Œæˆ: {retrieved_count} æ¡å‚è€ƒ")
            retrieval_msg = f'æ‰¾åˆ°äº† {retrieved_count} æ¡å‚è€ƒèµ„æ–™'
            yield f"data: {json.dumps({'stage': 'retrieval', 'data': [], 'message': retrieval_msg})}\n\n"
            
            # æ­¥éª¤6-7: æ ¡éªŒå’Œæ•´ç†
            print("â³ è¿›è¡Œä¸€è‡´æ€§æ ¡éªŒå’Œå†…å®¹æ•´ç†...")
            state = service.consistency_agent.run(state)
            state = service.organization_agent.run(state)
            
            # æœ€ç»ˆç»“æœ
            references = service._search_references(
                request.title,
                [c["concept"] for c in knowledge_clusters[:3]]
            )
            
            print("âœ… åˆ†æå®Œå…¨å®Œæˆ")
            complete_data = {
                "page_id": request.page_id,
                "title": request.title,
                "raw_content": request.content,
                "page_structure": state.get('page_structure', {}),
                "knowledge_clusters": knowledge_clusters,
                "understanding_notes": state.get("understanding_notes", ""),
                "knowledge_gaps": gaps_data,
                "expanded_content": expanded_data,
                "references": references,
                "raw_points": request.raw_points or [],
            }

            if request.doc_id:
                persistence.upsert_page_analysis(request.doc_id, request.page_id, complete_data)

            yield f"data: {json.dumps({'stage': 'complete', 'data': complete_data, 'message': 'åˆ†æå®Œæˆï¼'})}\n\n"
            
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            print(f"âŒ æµå¼åˆ†æé”™è¯¯: {error_trace}")
            error_msg = f'é”™è¯¯: {str(e)}'
            yield f"data: {json.dumps({'stage': 'error', 'data': {}, 'message': error_msg})}\n\n"
    
    return StreamingResponse(event_generator(), media_type="text/event-stream")


@app.post("/api/v1/chat")
async def chat(
    request: ChatRequest,
):
    """ä¸ AI åŠ©æ•™å¯¹è¯"""
    try:
        if not request.message or not request.message.strip():
            raise HTTPException(status_code=400, detail="æ¶ˆæ¯ä¸èƒ½ä¸ºç©º")
        
        # è·å–æœåŠ¡å®ä¾‹
        service = get_ai_tutor()
        
        print(f"ğŸ“ èŠå¤©è¯·æ±‚: page_id={request.page_id}, å·²æœ‰ä¸Šä¸‹æ–‡: {list(service.page_context.keys())}")
        
        # æ£€æŸ¥æ˜¯å¦å·²è®¾ç½®ä¸Šä¸‹æ–‡
        if request.page_id not in service.page_context:
            print(f"âš ï¸ é¡µé¢ {request.page_id} æœªåœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œå½“å‰å·²çŸ¥é¡µé¢: {list(service.page_context.keys())}")
            return {
                "status": "error",
                "response": f"âš ï¸ é¡µé¢å†…å®¹æœªåŠ è½½ã€‚è¯·ç¡®ä¿ï¼š\n1. å·²åˆ‡æ¢åˆ°èŠå¤©æ ‡ç­¾é¡µ\n2. å·²åŠ è½½ PPT å†…å®¹\n3. é¡µé¢å·²å®Œå…¨åˆå§‹åŒ–ï¼ˆpage_id={request.page_id}ï¼‰\n\nå¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·ï¼š\nâ€¢ åˆ·æ–°é¡µé¢\nâ€¢ é‡æ–°ä¸Šä¼  PPT\nâ€¢ æŸ¥çœ‹åç«¯æ—¥å¿—",
                "need_context": True
            }
        
        # è°ƒç”¨åŠ©æ•™æœåŠ¡
        response_text = service.chat(request.page_id, request.message)
        
        return {
            "status": "ok",
            "response": response_text,
            "page_id": request.page_id,
            "timestamp": datetime.now().isoformat()
        }
    
    except Exception as e:
        print(f"âŒ èŠå¤©å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return {
            "status": "error",
            "response": f"âŒ æŠ±æ­‰,AI æš‚æ—¶æ— æ³•å›ç­”ã€‚é”™è¯¯: {str(e)}",
            "error": str(e)
        }

@app.post("/api/v1/tutor/set-context")
async def set_tutor_context(
    request: PageAnalysisRequest,
):
    """è®¾ç½® AI åŠ©æ•™çš„é¡µé¢ä¸Šä¸‹æ–‡ - ä¸ä¼˜åŒ–çš„çŸ¥è¯†åˆ†æç»“æ„å¯¹é½"""
    try:
        # è·å–æœåŠ¡å®ä¾‹
        service = get_ai_tutor()
        
        # ç»„è£…å†…å®¹æ–‡æœ¬
        content_text = request.content
        if not content_text and request.raw_points:
            content_text = "\n".join([
                point.get('text', '') 
                for point in request.raw_points 
                if point.get('type') == 'text'
            ])
        
        # ç¡®ä¿ page_id æ˜¯æ•´æ•°
        page_id = int(request.page_id)
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ‰¹é‡è®¾ç½®ååº”è¯¥å·²å­˜åœ¨ï¼‰
        if page_id in service.page_context:
            print(f"âœ… ä¸Šä¸‹æ–‡å·²å­˜åœ¨ï¼ˆæ‰¹é‡è®¾ç½®å·²å®Œæˆï¼‰ï¼Œè·³è¿‡é‡å¤è®¾ç½®: page_id={page_id}")
            greeting = service.get_assistant_greeting(page_id)
            return {
                "status": "ok",
                "page_id": page_id,
                "greeting": greeting,
                "message": "é¡µé¢ä¸Šä¸‹æ–‡å·²å­˜åœ¨ï¼ˆæ‰¹é‡è®¾ç½®ï¼‰",
                "cached": True
            }
        
        print(f"ğŸ”§ è®¾ç½®ä¸Šä¸‹æ–‡: page_id={page_id}, title={request.title}")
        
        # æå–çŸ¥è¯†é›†ç¾¤ä¿¡æ¯ï¼ˆå¦‚æœå·²åˆ†æè¿‡ï¼‰
        knowledge_clusters = request.key_concepts or []
        if isinstance(knowledge_clusters, list) and len(knowledge_clusters) > 0:
            # å¦‚æœ key_concepts æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            if isinstance(knowledge_clusters[0], str):
                knowledge_clusters = [
                    {"concept": c, "difficulty_level": 2} 
                    for c in knowledge_clusters
                ]
        
        # è®¾ç½®é¡µé¢ä¸Šä¸‹æ–‡ - ä½¿ç”¨æ–°çš„å‚æ•°æ ¼å¼
        service.set_page_context(
            page_id=page_id,
            title=request.title,
            content=content_text,
            knowledge_clusters=knowledge_clusters or [],
            understanding_notes=request.analysis or "",  # ä½¿ç”¨ analysis å­—æ®µä½œä¸ºç†è§£ç¬”è®°
            knowledge_gaps=getattr(request, 'knowledge_gaps', []),
            expanded_content=getattr(request, 'expanded_content', [])
        )
        
        print(f"âœ… ä¸Šä¸‹æ–‡å·²ä¿å­˜ï¼Œå½“å‰å·²çŸ¥é¡µé¢: {list(service.page_context.keys())}")
        
        # è¿”å›æ¬¢è¿è¯­
        greeting = service.get_assistant_greeting(page_id)
        
        return {
            "status": "ok",
            "page_id": page_id,
            "greeting": greeting,
            "message": "é¡µé¢ä¸Šä¸‹æ–‡å·²è®¾ç½®",
            "cached": False
        }
    
    except Exception as e:
        print(f"âŒ è®¾ç½®ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


class BulkContextRequest(BaseModel):
    """æ‰¹é‡ä¸Šä¸‹æ–‡è¯·æ±‚"""
    doc_id: str

@app.post("/api/v1/tutor/set-context-bulk")
async def set_tutor_context_bulk(
    request: BulkContextRequest,
    persistence: PersistenceService = Depends(get_persistence_service),
):
    """ä¸ºæ–‡æ¡£çš„æ‰€æœ‰é¡µé¢æ‰¹é‡è®¾ç½®ä¸Šä¸‹æ–‡ï¼ˆä¼˜å…ˆä½¿ç”¨å·²ä¿å­˜çš„åˆ†æç»“æœï¼‰ã€‚"""
    try:
        doc_id = request.doc_id
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡è®¾ç½®ä¸Šä¸‹æ–‡ï¼Œdoc_id={doc_id}")
        service = get_ai_tutor()
        doc = persistence.get_document_by_id(doc_id)
        if not doc:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°æ–‡æ¡£")

        analyses = persistence.list_page_analyses(doc_id)
        slides = doc.get("slides", [])
        set_pages = []
        
        print(f"ğŸ“„ æ–‡æ¡£å…±æœ‰ {len(slides)} é¡µï¼Œå·²ä¿å­˜åˆ†æ {len(analyses)} é¡µ")

        for idx, slide in enumerate(slides):
            page_id = slide.get("page_num") or (idx + 1)
            analysis = analyses.get(page_id, {})

            raw_points = slide.get("raw_points") or []
            content_text = analysis.get("raw_content") or slide.get("raw_content") or ""
            if not content_text and raw_points:
                content_text = "\n".join(
                    [p.get("text", "") if isinstance(p, dict) else str(p) for p in raw_points]
                )

            title = analysis.get("title") or slide.get("title") or f"Page {page_id}"
            print(f"  ğŸ“„ è®¾ç½®é¡µé¢ {page_id}: {title[:30]}... (æœ‰åˆ†æ: {page_id in analyses})")
            
            service.set_page_context(
                page_id=page_id,
                title=title,
                content=content_text,
                knowledge_clusters=analysis.get("knowledge_clusters", []),
                understanding_notes=analysis.get("understanding_notes", ""),
                knowledge_gaps=analysis.get("knowledge_gaps", []),
                expanded_content=analysis.get("expanded_content", []),
            )
            set_pages.append(page_id)

        print(f"âœ… æ‰¹é‡ä¸Šä¸‹æ–‡è®¾ç½®å®Œæˆï¼Œå…± {len(set_pages)} é¡µ: {set_pages}")
        return {
            "status": "ok",
            "doc_id": doc_id,
            "pages": set_pages,
            "message": f"æ‰¹é‡ä¸Šä¸‹æ–‡å·²è®¾ç½®ï¼Œå…± {len(set_pages)} é¡µ"
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/tutor/debug/{page_id}")
async def debug_tutor_context(page_id: int):
    """è°ƒè¯•ï¼šæŸ¥çœ‹å½“å‰é¡µé¢ä¸Šä¸‹æ–‡"""
    service = get_ai_tutor()
    context = service.page_context.get(page_id)
    conversation = service.get_conversation_history(page_id)
    
    return {
        "page_id": page_id,
        "has_context": context is not None,
        "context": context,
        "conversation_count": len(conversation),
        "conversation": conversation[-5:] if conversation else []
    }

@app.post("/api/v1/tutor/conversation")
async def get_conversation_history(
    page_id: int,
    service: AITutorService = Depends(get_ai_tutor_service),
):
    """è·å–å¯¹è¯å†å²
    
    Args:
        page_id: é¡µé¢ ID
        service: AI åŠ©æ•™æœåŠ¡
    
    Returns:
        å¯¹è¯å†å²
    """
    try:
        history = service.get_conversation_history(page_id)
        return {
            "page_id": page_id,
            "messages": history
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )


@app.post("/api/v1/search-references")
async def search_references(
    request: ReferenceSearchRequest,
    service: ReferenceSearchService = Depends(get_reference_search_service),
):
    """æœç´¢å‚è€ƒæ–‡çŒ®
    
    Args:
        request: æœç´¢è¯·æ±‚
        service: æœç´¢æœåŠ¡
    
    Returns:
        æœç´¢ç»“æœ
    """
    try:
        if request.search_type == "academic":
            result = service.search_academic_papers(request.query, request.max_results)
        elif request.search_type == "general":
            result = service.search_general_knowledge(request.query, request.max_results)
        else:
            result = service.search_references(request.query, request.max_results)
        
        return {
            "success": True,
            "query": result.query,
            "total_results": result.total_results,
            "references": [
                {
                    "title": ref.title,
                    "url": ref.url,
                    "source": ref.source,
                    "snippet": ref.snippet
                }
                for ref in result.references
            ]
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )


@app.post("/api/v1/search-by-concepts")
async def search_by_concepts(
    concepts: list,
    max_per_concept: int = 3,
    service: ReferenceSearchService = Depends(get_reference_search_service),
):
    """æŒ‰æ¦‚å¿µæœç´¢å‚è€ƒæ–‡çŒ®
    
    Args:
        concepts: æ¦‚å¿µåˆ—è¡¨
        max_per_concept: æ¯ä¸ªæ¦‚å¿µçš„æœ€å¤§ç»“æœæ•°
        service: æœç´¢æœåŠ¡
    
    Returns:
        æŒ‰æ¦‚å¿µç»„ç»‡çš„æœç´¢ç»“æœ
    """
    try:
        results = service.search_by_concepts(concepts, max_per_concept)
        
        return {
            "success": True,
            "results": {
                concept: {
                    "query": result.query,
                    "total": result.total_results,
                    "references": [
                        {
                            "title": ref.title,
                            "url": ref.url,
                            "source": ref.source,
                            "snippet": ref.snippet
                        }
                        for ref in result.references
                    ]
                }
                for concept, result in results.items()
            }
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )


@app.get("/api/v1/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ - è½»é‡çº§ï¼Œä¸è°ƒç”¨ä»»ä½•æœåŠ¡ä¾èµ–"""
    try:
        # åªè¿”å›é™æ€ä¿¡æ¯ï¼Œä¸æ¶‰åŠä»»ä½•æœåŠ¡åˆå§‹åŒ–
        return {
            "status": "ok",
            "version": "0.2.0",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        # å³ä½¿å‡ºé”™ä¹Ÿå¿«é€Ÿè¿”å›
        return {
            "status": "error",
            "message": str(e),
            "version": "0.2.0"
        }


@app.get("/api/v1/health/complete")
async def complete_health_check():
    """è”åˆå¥åº·æ£€æŸ¥ - åŒæ—¶æ£€æŸ¥åç«¯å’Œ LLM è¿æ¥ï¼ˆå¿«é€Ÿè¯Šæ–­ï¼‰"""
    import asyncio
    import aiohttp
    
    config = load_config()
    llm_config = LLMConfig(
        api_key=config["llm"]["api_key"],
        base_url=config["llm"]["base_url"],
        model=config["llm"]["model"]
    )
    
    # åç«¯æ£€æŸ¥ï¼ˆæå¿«é€Ÿï¼‰
    backend_status = {
        "status": "ok",
        "version": "0.2.0",
        "timestamp": datetime.now().isoformat()
    }
    
    # LLM æ£€æŸ¥ï¼ˆå¿«é€Ÿé¢„æ£€æŸ¥ï¼Œä¸è°ƒç”¨ LLM APIï¼‰
    llm_status = {
        "status": "unknown",
        "message": "æ£€æŸ¥ä¸­...",
        "model": llm_config.model,
        "configured": bool(llm_config.api_key)
    }
    
    try:
        # æ£€æŸ¥ API Key
        if not llm_config.api_key:
            llm_status["status"] = "error"
            llm_status["message"] = "API Key æœªé…ç½®"
            llm_status["response_preview"] = "API Key é…ç½®ç¼ºå¤±"
        else:
            # å¼‚æ­¥å¿«é€Ÿç½‘ç»œæ£€æŸ¥
            try:
                timeout = aiohttp.ClientTimeout(total=2, connect=1)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    base_url = llm_config.base_url or "https://api.openai.com/v1"
                    async with session.head(base_url, ssl=False, allow_redirects=True) as resp:
                        if resp.status in [200, 401, 403, 404]:
                            llm_status["status"] = "ok"
                            llm_status["message"] = "LLM æœåŠ¡ç½‘ç»œè¿æ¥æ­£å¸¸"
                            llm_status["response_preview"] = f"LLM æ¨¡å‹: {llm_config.model} âœ“"
                        else:
                            llm_status["status"] = "warning"
                            llm_status["message"] = f"æœåŠ¡è¿”å›å¼‚å¸¸çŠ¶æ€ç  {resp.status}"
                            llm_status["response_preview"] = f"HTTP {resp.status}"
            except (aiohttp.ClientConnectorError, asyncio.TimeoutError):
                llm_status["status"] = "error"
                llm_status["message"] = "æ— æ³•è¿æ¥åˆ° LLM æœåŠ¡"
                llm_status["response_preview"] = "è¿æ¥è¶…æ—¶"
    except Exception as e:
        llm_status["status"] = "error"
        llm_status["message"] = f"æ£€æŸ¥å¤±è´¥: {str(e)}"
        llm_status["response_preview"] = type(e).__name__
    
    return {
        "status": "ok",
        "backend": backend_status,
        "llm": llm_status,
        "timestamp": datetime.now().isoformat()
    }


@app.post("/api/v1/search-semantic")
async def search_semantic(
    request: SemanticSearchRequest,
    vector_store: VectorStoreService = Depends(get_vector_store_service),
):
    """åŸºäºè¯­ä¹‰æœç´¢ PPT/PDF åˆ‡ç‰‡
    
    Args:
        request: æœç´¢è¯·æ±‚
        vector_store: å‘é‡å­˜å‚¨æœåŠ¡
    
    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    try:
        results = vector_store.search_similar_slides(
            query=request.query,
            top_k=request.top_k,
            file_name=request.file_name,
            file_type=request.file_type,
            min_score=request.min_score
        )
        
        return {
            "success": True,
            "query": request.query,
            "total_results": len(results),
            "results": results
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )


@app.get("/api/v1/vector-store/stats")
async def get_vector_store_stats(
    vector_store: VectorStoreService = Depends(get_vector_store_service),
):
    """è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        vector_store: å‘é‡å­˜å‚¨æœåŠ¡
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        stats = vector_store.get_stats()
        return {
            "success": True,
            "stats": stats
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )


@app.get("/api/v1/vector-store/file/{file_name}")
async def get_file_slides(
    file_name: str,
    vector_store: VectorStoreService = Depends(get_vector_store_service),
):
    """è·å–ç‰¹å®šæ–‡ä»¶çš„æ‰€æœ‰åˆ‡ç‰‡
    
    Args:
        file_name: æ–‡ä»¶å
        vector_store: å‘é‡å­˜å‚¨æœåŠ¡
    
    Returns:
        è¯¥æ–‡ä»¶çš„æ‰€æœ‰åˆ‡ç‰‡
    """
    try:
        results = vector_store.search_by_file(file_name)
        return {
            "success": True,
            "file_name": file_name,
            "total_chunks": len(results),
            "chunks": results
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )


@app.delete("/api/v1/vector-store/file/{file_name}")
async def delete_file_slides(
    file_name: str,
    vector_store: VectorStoreService = Depends(get_vector_store_service),
):
    """åˆ é™¤ç‰¹å®šæ–‡ä»¶çš„æ‰€æœ‰åˆ‡ç‰‡
    
    Args:
        file_name: æ–‡ä»¶å
        vector_store: å‘é‡å­˜å‚¨æœåŠ¡
    
    Returns:
        åˆ é™¤ç»“æœ
    """
    try:
        success = vector_store.delete_file_slides(file_name)
        return {
            "success": success,
            "file_name": file_name,
            "message": "åˆ é™¤æˆåŠŸ" if success else "æœªæ‰¾åˆ°æ–‡ä»¶æˆ–åˆ é™¤å¤±è´¥"
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )


@app.get("/api/v1/health/llm")
async def check_llm_connection():
    """æ£€æŸ¥ LLM è¿æ¥çŠ¶æ€ - è½»é‡çº§å¿«é€Ÿè¯Šæ–­ï¼ˆä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰"""
    import asyncio
    import aiohttp
    from urllib.parse import urljoin
    
    config = load_config()
    llm_config = LLMConfig(
        api_key=config["llm"]["api_key"],
        base_url=config["llm"]["base_url"],
        model=config["llm"]["model"]
    )
    
    # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ API Key é…ç½®
    if not llm_config.api_key:
        return {
            "status": "error",
            "message": "API Key æœªé…ç½®",
            "detail": "è¯·æ£€æŸ¥ config.json ä¸­çš„ llm.api_key å­—æ®µ",
            "configured": False,
            "model": llm_config.model,
            "response_preview": "API Key é…ç½®ç¼ºå¤±"
        }
    
    # ç¬¬äºŒæ­¥ï¼šå¿«é€Ÿç½‘ç»œè¿æ¥æ£€æŸ¥ï¼ˆä¸å®é™…è°ƒç”¨ LLMï¼‰
    try:
        # ä½¿ç”¨è¶…çŸ­è¶…æ—¶æ—¶é—´ï¼ˆ2ç§’ï¼‰åšå¿«é€Ÿè¿æ¥é¢„æ£€
        timeout = aiohttp.ClientTimeout(total=2, connect=1)
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            # åªæµ‹è¯•ç½‘ç»œè¿é€šæ€§ï¼Œä¸è°ƒç”¨å®é™… API
            base_url = llm_config.base_url or "https://api.openai.com/v1"
            
            # å°è¯•è¿æ¥åˆ° base_url
            try:
                async with session.head(base_url, ssl=False, allow_redirects=True) as resp:
                    # å¦‚æœèƒ½è¿æ¥ï¼ˆå³ä½¿æ˜¯ 401/403 ä¹Ÿè¡¨ç¤ºç½‘ç»œé€šï¼‰ï¼Œè¯´æ˜åŸºç¡€è¿æ¥æ­£å¸¸
                    if resp.status in [200, 401, 403, 404]:
                        # ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œå¯èƒ½çš„çŠ¶æ€ï¼ˆå…·ä½“çš„ API éªŒè¯ä¼šåœ¨å®é™…è°ƒç”¨æ—¶è¿›è¡Œï¼‰
                        return {
                            "status": "ok",
                            "message": "LLM æœåŠ¡ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ˆé¢„æ£€æŸ¥ï¼‰",
                            "model": llm_config.model,
                            "configured": True,
                            "base_url": base_url,
                            "response_preview": f"LLM æ¨¡å‹: {llm_config.model} âœ“ ç½‘ç»œè¿æ¥æ­£å¸¸",
                            "note": "å¿«é€Ÿé¢„æ£€æŸ¥åªéªŒè¯ç½‘ç»œï¼Œå®é™… API è°ƒç”¨ä¼šåœ¨ä½¿ç”¨æ—¶è¿›è¡Œ"
                        }
                    else:
                        return {
                            "status": "warning",
                            "message": f"LLM æœåŠ¡è¿”å›å¼‚å¸¸çŠ¶æ€ç  {resp.status}",
                            "model": llm_config.model,
                            "configured": True,
                            "base_url": base_url,
                            "response_preview": f"æœåŠ¡çŠ¶æ€å¼‚å¸¸ (HTTP {resp.status})"
                        }
            except (aiohttp.ClientConnectorError, asyncio.TimeoutError):
                # ç½‘ç»œè¿æ¥å¤±è´¥
                return {
                    "status": "error",
                    "message": "æ— æ³•è¿æ¥åˆ° LLM æœåŠ¡",
                    "detail": f"è¿æ¥è¶…æ—¶æˆ–æœåŠ¡ä¸å¯è¾¾ï¼š{base_url}",
                    "model": llm_config.model,
                    "configured": True,
                    "base_url": base_url,
                    "response_preview": f"è¿æ¥å¤±è´¥: {base_url}"
                }
    except Exception as e:
        error_msg = str(e).lower()
        
        # æ ¹æ®é”™è¯¯ç±»å‹æä¾›è¯Šæ–­
        if "401" in error_msg or "unauthorized" in error_msg:
            detail = "API Key å¯èƒ½æ— æ•ˆ - è¯·æ£€æŸ¥ config.json"
            preview = "è®¤è¯å¤±è´¥: API Key æ— æ•ˆ"
        elif "ssl" in error_msg or "certificate" in error_msg:
            detail = "SSL è¯ä¹¦é—®é¢˜ - è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®"
            preview = "SSL è¯ä¹¦é”™è¯¯"
        elif "connection" in error_msg or "refused" in error_msg:
            detail = "æ— æ³•è¿æ¥åˆ° LLM æœåŠ¡ - æ£€æŸ¥ base_url é…ç½®"
            preview = "æ— æ³•è¿æ¥åˆ°æœåŠ¡"
        else:
            detail = f"å¿«é€Ÿè¯Šæ–­å¤±è´¥ï¼š{str(e)}"
            preview = f"è¯Šæ–­é”™è¯¯: {type(e).__name__}"
        
        return {
            "status": "error",
            "message": "LLM è¿æ¥æ£€æŸ¥å¤±è´¥",
            "detail": detail,
            "model": llm_config.model,
            "configured": True,
            "response_preview": preview,
            "error_type": type(e).__name__
        }

class SetContextRequest(BaseModel):
    """è®¾ç½®é¡µé¢ä¸Šä¸‹æ–‡è¯·æ±‚"""
    page_id: int
    title: str
    content: str
    raw_points: List[dict] = []
    key_concepts: List[str] = []
    analysis: str = ""


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    page_id: int
    message: str


# ===== API ç«¯ç‚¹ =====


@app.get("/api/v1/tutor/conversation")
async def get_conversation_history(page_id: int):
    """è·å–å¯¹è¯å†å²"""
    try:
        history = ai_tutor.get_conversation_history(page_id)
        return {
            "status": "ok",
            "page_id": page_id,
            "history": history,
            "count": len(history)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/v1/tutor/conversation/{page_id}")
async def clear_conversation(page_id: int):
    """æ¸…é™¤å¯¹è¯å†å²"""
    try:
        ai_tutor.clear_conversation(page_id)
        return {
            "status": "ok",
            "page_id": page_id,
            "message": "å¯¹è¯å†å²å·²æ¸…é™¤"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)

