"""
å‘é‡å­˜å‚¨æœåŠ¡ - é‡æ–°è®¾è®¡ç‰ˆæœ¬
ç›®æ ‡ï¼šç®€å•ã€é«˜æ•ˆã€å‡†ç¡®çš„è¯­ä¹‰æœç´¢
"""

import os
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime
from collections import defaultdict

from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings

# ä¼˜å…ˆä½¿ç”¨æ–°çš„ langchain-chroma
try:
    from langchain_chroma import Chroma
except ImportError:
    try:
        from langchain_community.vectorstores import Chroma
    except ImportError:
        raise ImportError("éœ€è¦å®‰è£… langchain-chroma æˆ– langchain-community")

from src.agents.base import LLMConfig


class VectorStoreService:
    """
    å‘é‡å­˜å‚¨æœåŠ¡ - é‡æ–°è®¾è®¡ç‰ˆæœ¬
    
    æ ¸å¿ƒåŸåˆ™ï¼š
    1. ä¿æŒå¹»ç¯ç‰‡å®Œæ•´æ€§ - æ¯ä¸ªå¹»ç¯ç‰‡ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„æ–‡æ¡£å­˜å‚¨
    2. ç®€åŒ–æ–‡æœ¬è½¬æ¢ - ç›´æ¥ä½¿ç”¨PPTè§£æç»“æœï¼Œä¸åšå¤šä½™å¤„ç†
    3. ä¼˜åŒ–æœç´¢ç­–ç•¥ - æé«˜ç›¸å…³æ€§ï¼Œå‡å°‘å™ªéŸ³
    """
    
    def __init__(self, llm_config: LLMConfig, vector_db_path: str = "./ppt_vector_db", embedding_model: Optional[str] = None):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨æœåŠ¡
        
        Args:
            llm_config: LLMé…ç½®
            vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„
            embedding_model: Embeddingæ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
        """
        self.llm_config = llm_config
        self.vector_db_path = vector_db_path

        # åˆå§‹åŒ–Embeddingæ¨¡å‹
        embedding_kwargs = {
            "api_key": llm_config.api_key,
            "base_url": llm_config.base_url
        }
        
        # å¦‚æœæŒ‡å®šäº†embeddingæ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨å®ƒ
        if embedding_model:
            try:
                embedding_kwargs["model"] = embedding_model
                self.embeddings = OpenAIEmbeddings(**embedding_kwargs)
                print(f"âœ… ä½¿ç”¨é…ç½®çš„Embeddingæ¨¡å‹: {embedding_model}")
            except Exception as e:
                print(f"âš ï¸  ä½¿ç”¨é…ç½®çš„Embeddingæ¨¡å‹å¤±è´¥ ({embedding_model}): {e}")
                print(f"ğŸ’¡ å°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹...")
                # ç§»é™¤modelå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
                embedding_kwargs.pop("model", None)
                self.embeddings = OpenAIEmbeddings(**embedding_kwargs)
        else:
            # æ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤
            try:
                # å°è¯•ä½¿ç”¨å¸¸ç”¨çš„ä¸­æ–‡embeddingæ¨¡å‹
                embedding_kwargs["model"] = "BAAI/bge-large-zh-v1.5"
                self.embeddings = OpenAIEmbeddings(**embedding_kwargs)
                print(f"âœ… ä½¿ç”¨é»˜è®¤Embeddingæ¨¡å‹: BAAI/bge-large-zh-v1.5")
            except Exception as e:
                print(f"âš ï¸  é»˜è®¤Embeddingæ¨¡å‹ä¸å¯ç”¨: {e}")
                print(f"ğŸ’¡ å°è¯•ä½¿ç”¨APIé»˜è®¤æ¨¡å‹...")
                # ç§»é™¤modelå‚æ•°ï¼Œè®©APIä½¿ç”¨é»˜è®¤æ¨¡å‹
                embedding_kwargs.pop("model", None)
                self.embeddings = OpenAIEmbeddings(**embedding_kwargs)
                print(f"âœ… ä½¿ç”¨APIé»˜è®¤Embeddingæ¨¡å‹")
        
        self.vectorstore: Optional[Chroma] = None
        try:
            self._initialize_vectorstore()
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.vectorstore = None
    
    def _initialize_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            os.makedirs(self.vector_db_path, exist_ok=True)
            
            # å°è¯•åŠ è½½ç°æœ‰æ•°æ®åº“
            if os.path.exists(self.vector_db_path) and os.listdir(self.vector_db_path):
                try:
                    self.vectorstore = Chroma(
                        persist_directory=self.vector_db_path,
                        embedding_function=self.embeddings
                    )
                    print(f"âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ (è·¯å¾„: {self.vector_db_path})")
                    return
                except Exception as e:
                    print(f"âš ï¸  åŠ è½½ç°æœ‰æ•°æ®åº“å¤±è´¥: {e}")
                    # åˆ é™¤æ—§æ•°æ®åº“ï¼Œé‡æ–°åˆ›å»º
                    import shutil
                    shutil.rmtree(self.vector_db_path)
                    os.makedirs(self.vector_db_path, exist_ok=True)
            
            # åˆ›å»ºæ–°æ•°æ®åº“
            self.vectorstore = Chroma(
                persist_directory=self.vector_db_path,
                embedding_function=self.embeddings
            )
            print(f"âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ (è·¯å¾„: {self.vector_db_path})")
            
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _extract_slide_text(self, slide: Dict[str, Any]) -> str:
        """
        ä»å¹»ç¯ç‰‡ä¸­æå–æ–‡æœ¬
        æ ¸å¿ƒåŸåˆ™ï¼šç®€å•ã€å®Œæ•´ã€ä¿ç•™åŸå§‹ä¿¡æ¯
        """
        text_parts = []
        
        # 1. æ ‡é¢˜ï¼ˆæœ€é‡è¦ï¼‰
        title = slide.get("title", "").strip()
        if title:
            text_parts.append(title)
        
        # 2. å†…å®¹ç‚¹ï¼ˆä¿æŒåŸå§‹é¡ºåºå’Œç»“æ„ï¼‰
        raw_points = slide.get("raw_points", [])
        for point in raw_points:
            if isinstance(point, dict):
                text = point.get("text", "").strip()
                if text:
                    # æ·»åŠ å±‚çº§ç¼©è¿›
                    level = point.get("level", 0)
                    indent = "  " * level
                    text_parts.append(f"{indent}{text}")
            elif isinstance(point, str):
                text = point.strip()
                if text:
                    text_parts.append(text)
        
        # ç»„åˆæ–‡æœ¬
        full_text = "\n".join(text_parts)
        
        return full_text
    
    def _split_text_for_embedding(self, text: str, max_tokens: int = 400) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªchunkï¼Œç¡®ä¿æ¯ä¸ªchunkä¸è¶…è¿‡tokené™åˆ¶
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆä¿å®ˆä¼°è®¡ï¼š1ä¸ªtoken â‰ˆ 3ä¸ªå­—ç¬¦ï¼‰
        
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        # ä¿å®ˆä¼°è®¡ï¼š400 tokens â‰ˆ 1200 å­—ç¬¦
        max_chars = max_tokens * 3
        
        if len(text) <= max_chars:
            return [text]
        
        # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼ŒæŒ‰æ®µè½åˆ†å‰²
        chunks = []
        lines = text.split('\n')
        current_chunk = []
        current_length = 0
        
        for line in lines:
            line_length = len(line) + 1  # +1 for newline
            
            if current_length + line_length > max_chars and current_chunk:
                # å½“å‰chunkå·²æ»¡ï¼Œä¿å­˜
                chunks.append('\n'.join(current_chunk))
                current_chunk = [line]
                current_length = line_length
            else:
                current_chunk.append(line)
                current_length += line_length
        
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        
        # å¦‚æœè¿˜æœ‰è¶…é•¿çš„chunkï¼ˆå•è¡Œè¶…é•¿ï¼‰ï¼Œå¼ºåˆ¶æˆªæ–­
        final_chunks = []
        for chunk in chunks:
            if len(chunk) > max_chars:
                # å¼ºåˆ¶æˆªæ–­
                for i in range(0, len(chunk), max_chars):
                    final_chunks.append(chunk[i:i + max_chars])
            else:
                final_chunks.append(chunk)
        
        return final_chunks
    
    def store_document_slides(
        self,
        file_name: str,
        file_type: str,
        slides: List[Dict[str, Any]],
        metadata: Optional[Dict[str, Any]] = None,
        overwrite: bool = False
    ) -> Dict[str, Any]:
        """
        å­˜å‚¨æ–‡æ¡£çš„æ‰€æœ‰å¹»ç¯ç‰‡åˆ°å‘é‡æ•°æ®åº“
        
        æ ¸å¿ƒç­–ç•¥ï¼š
        1. æ¯ä¸ªå¹»ç¯ç‰‡ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„æ–‡æ¡£ï¼ˆä¸åˆ†å—ï¼‰
        2. ä¿ç•™æ‰€æœ‰åŸå§‹ä¿¡æ¯
        3. æ·»åŠ ä¸°å¯Œçš„å…ƒæ•°æ®ä¾¿äºè¿‡æ»¤
        """
        if not self.vectorstore:
            raise Exception("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
        
        # å¦‚æœéœ€è¦è¦†ç›–ï¼Œå…ˆåˆ é™¤æ—§æ•°æ®
        if overwrite:
            self.delete_file_slides(file_name)
        
        documents = []
        ids = []
        stored_count = 0
        
        print(f"ğŸ“ å¼€å§‹å­˜å‚¨æ–‡æ¡£: {file_name}ï¼Œå…± {len(slides)} é¡µ")
        
        for slide in slides:
            # æå–æ–‡æœ¬
            slide_text = self._extract_slide_text(slide)
            
            # è°ƒè¯•ä¿¡æ¯
            page_num = slide.get('page_num', 0)
            print(f"  ğŸ“„ é¡µé¢ {page_num}: æå–æ–‡æœ¬ {len(slide_text)} å­—ç¬¦")
            
            # è¿‡æ»¤ç©ºå†…å®¹
            if not slide_text or len(slide_text.strip()) < 10:
                print(f"  â­ï¸  è·³è¿‡é¡µé¢ {page_num}ï¼šå†…å®¹è¿‡çŸ­ï¼ˆ{len(slide_text)} å­—ç¬¦ï¼‰")
                continue
            
            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œåˆ†å‰²æˆå¤šä¸ªchunk
            text_chunks = self._split_text_for_embedding(slide_text, max_tokens=400)
            
            if len(text_chunks) > 1:
                print(f"  âœ‚ï¸  é¡µé¢ {page_num} æ–‡æœ¬è¾ƒé•¿ï¼Œåˆ†å‰²ä¸º {len(text_chunks)} ä¸ªchunk")
            
            # ä¸ºæ¯ä¸ªchunkåˆ›å»ºæ–‡æ¡£
            for chunk_idx, chunk_text in enumerate(text_chunks):
                doc_id = f"{file_name}_{page_num}_{chunk_idx}_{uuid.uuid4().hex[:6]}"
                
                doc = Document(
                    page_content=chunk_text,
                    metadata={
                        "file_name": file_name,
                        "file_type": file_type,
                        "page_num": page_num,
                        "slide_title": slide.get("title", ""),
                        "slide_type": slide.get("type", "content"),
                        "chunk_index": chunk_idx,
                        "total_chunks": len(text_chunks),
                        "stored_at": datetime.now().isoformat(),
                        **(metadata or {})
                    }
                )
                
                documents.append(doc)
                ids.append(doc_id)
            
            print(f"  âœ“ é¡µé¢ {page_num} å·²åŠ å…¥å­˜å‚¨é˜Ÿåˆ—ï¼ˆ{len(text_chunks)} ä¸ªchunkï¼‰")
        
        # æ‰¹é‡å­˜å‚¨
        if documents:
            print(f"  ğŸ“¦ å‡†å¤‡å­˜å‚¨ {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
            try:
                # åˆ†æ‰¹å­˜å‚¨ï¼Œé¿å…APIé™åˆ¶
                # ç”±äºAPIé™åˆ¶æ¯ä¸ªæ–‡æ¡£<512 tokensï¼Œéœ€è¦æ›´å°çš„æ‰¹æ¬¡
                batch_size = 5  # å‡å°æ‰¹æ¬¡å¤§å°
                for i in range(0, len(documents), batch_size):
                    batch_docs = documents[i:i + batch_size]
                    batch_ids = ids[i:i + batch_size]
                    
                    print(f"  ğŸ”„ æ­£åœ¨å­˜å‚¨æ‰¹æ¬¡ {i//batch_size + 1}ï¼ŒåŒ…å« {len(batch_docs)} ä¸ªæ–‡æ¡£...")
                    
                    try:
                        self.vectorstore.add_documents(
                            documents=batch_docs,
                            ids=batch_ids
                        )
                        stored_count += len(batch_docs)
                        print(f"  âœ… å·²å­˜å‚¨ {stored_count}/{len(documents)} é¡µ")
                    except Exception as batch_err:
                        # å¦‚æœæ‰¹æ¬¡å¤±è´¥ï¼Œå°è¯•å•ä¸ªå­˜å‚¨
                        print(f"  âš ï¸ æ‰¹æ¬¡å­˜å‚¨å¤±è´¥ï¼Œå°è¯•é€ä¸ªå­˜å‚¨...")
                        for doc, doc_id in zip(batch_docs, batch_ids):
                            try:
                                self.vectorstore.add_documents(
                                    documents=[doc],
                                    ids=[doc_id]
                                )
                                stored_count += 1
                                print(f"    âœ“ é¡µé¢ {doc.metadata.get('page_num')} å·²å­˜å‚¨")
                            except Exception as single_err:
                                error_msg = str(single_err)
                                if "512 tokens" in error_msg:
                                    print(f"    âœ— é¡µé¢ {doc.metadata.get('page_num')} æ–‡æœ¬è¿‡é•¿ï¼Œè·³è¿‡")
                                else:
                                    print(f"    âœ— é¡µé¢ {doc.metadata.get('page_num')} å­˜å‚¨å¤±è´¥: {single_err}")
                
                # æŒä¹…åŒ–ï¼ˆæ–°ç‰ˆ Chroma å¯èƒ½ä¸éœ€è¦æ‰‹åŠ¨ persistï¼‰
                try:
                    if hasattr(self.vectorstore, 'persist'):
                        self.vectorstore.persist()
                        print(f"  ğŸ’¾ æ•°æ®å·²æŒä¹…åŒ–")
                except Exception as persist_err:
                    pass  # æ–°ç‰ˆæœ¬è‡ªåŠ¨æŒä¹…åŒ–ï¼Œå¿½ç•¥æ­¤é”™è¯¯
                
                print(f"âœ… å­˜å‚¨å®Œæˆ: {file_name}ï¼Œå…± {stored_count} é¡µ")
                
            except Exception as e:
                print(f"âŒ å­˜å‚¨å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                raise
        else:
            print(f"âš ï¸  æ²¡æœ‰æ–‡æ¡£éœ€è¦å­˜å‚¨ï¼ˆæ‰€æœ‰é¡µé¢å¯èƒ½éƒ½è¢«è¿‡æ»¤æ‰äº†ï¼‰")
        
        return {
            "file_name": file_name,
            "file_type": file_type,
            "total_slides": len(slides),
            "total_chunks": stored_count,  # ä¿æŒå’Œæ—§ç‰ˆä¸€è‡´çš„å­—æ®µå
            "stored_at": datetime.now().isoformat()
        }
    
    def search_similar_slides(
        self,
        query: str,
        top_k: int = 10,
        file_name: Optional[str] = None,
        file_type: Optional[str] = None,
        min_score: float = 0.0
    ) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢ç›¸ä¼¼çš„å¹»ç¯ç‰‡
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ä½¿ç”¨æ›´å®½æ¾çš„æœç´¢èŒƒå›´ï¼ˆæœç´¢æ›´å¤šç»“æœï¼‰
        2. æŒ‰é¡µé¢å»é‡ï¼ˆæ¯ä¸ªé¡µé¢åªè¿”å›ä¸€æ¬¡ï¼‰
        3. ä½¿ç”¨æ›´åˆç†çš„ç›¸ä¼¼åº¦è®¡ç®—
        4. æŒ‰ç›¸ä¼¼åº¦æ’åº
        5. å¦‚æœå‘é‡æœç´¢å¤±è´¥ï¼Œè‡ªåŠ¨é™çº§åˆ°å…³é”®è¯æœç´¢
        """
        if not self.vectorstore:
            print("âš ï¸  å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return []
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"\nğŸ” å¼€å§‹æœç´¢:")
        print(f"   æŸ¥è¯¢: {query}")
        print(f"   top_k: {top_k}, min_score: {min_score}")
        print(f"   æ–‡ä»¶è¿‡æ»¤: {file_name or 'æ— '}")
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where = {}
        if file_name:
            where["file_name"] = file_name
        if file_type:
            where["file_type"] = file_type
        
        try:
            # æœç´¢æ›´å¤šç»“æœï¼ˆtop_k * 2ï¼‰ï¼Œç„¶åå»é‡
            search_k = max(top_k * 2, 20)
            
            # æ‰§è¡Œå‘é‡æœç´¢
            if where:
                results = self.vectorstore.similarity_search_with_score(
                    query,
                    k=search_k,
                    filter=where
                )
            else:
                results = self.vectorstore.similarity_search_with_score(
                    query,
                    k=search_k
                )
            
            print(f"   åŸå§‹ç»“æœæ•°: {len(results)}")
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºå‰5ä¸ªç»“æœçš„æ–‡ä»¶å
            if results:
                print(f"   å‰5ä¸ªç»“æœçš„æ–‡ä»¶å:")
                for i, (doc, dist) in enumerate(results[:5]):
                    print(f"     {i+1}. {doc.metadata.get('file_name', 'unknown')} - é¡µ {doc.metadata.get('page_num', '?')} (è·ç¦»: {dist:.3f})")
            
            # å¤„ç†ç»“æœå¹¶å»é‡
            # ç­–ç•¥ï¼šåŒä¸€é¡µé¢çš„å¤šä¸ªchunkï¼Œåªä¿ç•™ç›¸ä¼¼åº¦æœ€é«˜çš„é‚£ä¸ª
            # åŒæ—¶è®¡ç®—å…³é”®è¯åŒ¹é…åº¦ï¼Œæå‡åŒ…å«æ£€ç´¢è¯çš„ç»“æœæ’å
            page_best_results = {}  # {(file_name, page_num): best_result}
            filtered_count = 0
            
            # æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆç”¨äºå…³é”®è¯åŒ¹é…åŠ åˆ†ï¼‰
            query_lower = query.lower().strip()
            query_keywords = set(query_lower.split())
            # å¯¹äºä¸­æ–‡ï¼Œä¹Ÿå°è¯•å°†æ•´ä¸ªæŸ¥è¯¢ä½œä¸ºå®Œæ•´å…³é”®è¯
            if len(query_lower) >= 2:
                query_keywords.add(query_lower)  # æ·»åŠ å®Œæ•´æŸ¥è¯¢ä½œä¸ºå…³é”®è¯
            
            for doc, distance in results:
                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆChromaDBä½¿ç”¨ä½™å¼¦è·ç¦»ï¼‰
                # ä½™å¼¦è·ç¦»: [0, 2]ï¼Œ0è¡¨ç¤ºå®Œå…¨ç›¸åŒ
                # è½¬æ¢ä¸ºç›¸ä¼¼åº¦: similarity = 1 - (distance / 2)
                similarity = 1.0 - (distance / 2.0)
                similarity = max(0.0, min(1.0, similarity))
                
                # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
                if similarity < min_score:
                    filtered_count += 1
                    continue
                
                # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦ï¼ˆå¤§å¹…æå‡åŒ…å«æ£€ç´¢è¯çš„ç»“æœï¼‰
                content_lower = doc.page_content.lower()
                keyword_match_score = 0.0
                matched_keywords = 0
                full_query_matched = False
                
                # é¦–å…ˆæ£€æŸ¥å®Œæ•´æŸ¥è¯¢æ˜¯å¦åŒ¹é…ï¼ˆæœ€é‡è¦ï¼‰
                if query_lower in content_lower:
                    full_query_matched = True
                    count = content_lower.count(query_lower)
                    # å®Œæ•´åŒ¹é…ç»™äºˆå¤§å¹…åŠ åˆ†ï¼šå‡ºç°1æ¬¡+0.4ï¼Œæ¯å¤šå‡ºç°1æ¬¡+0.1ï¼ˆæœ€å¤š+0.6ï¼‰
                    keyword_match_score += min(0.6, 0.4 + (count - 1) * 0.1)
                    matched_keywords += 1
                    print(f"   âœ… å®Œæ•´åŒ¹é…æŸ¥è¯¢ '{query_lower}' åœ¨ {doc.metadata.get('file_name', 'unknown')} é¡µ{doc.metadata.get('page_num', '?')} (å‡ºç°{count}æ¬¡)")
                
                # ç„¶åæ£€æŸ¥å•ä¸ªå…³é”®è¯åŒ¹é…
                for keyword in query_keywords:
                    if keyword == query_lower:
                        continue  # å·²ç»å¤„ç†è¿‡å®Œæ•´æŸ¥è¯¢
                    if len(keyword) >= 2:  # åªè€ƒè™‘é•¿åº¦>=2çš„å…³é”®è¯
                        count = content_lower.count(keyword)
                        if count > 0:
                            matched_keywords += 1
                            # å•ä¸ªå…³é”®è¯åŒ¹é…ï¼šå‡ºç°1æ¬¡+0.2ï¼Œæ¯å¤šå‡ºç°1æ¬¡+0.05ï¼ˆæœ€å¤š+0.3ï¼‰
                            keyword_match_score += min(0.3, 0.2 + (count - 1) * 0.05)
                
                # å¦‚æœåŒ¹é…äº†å¤šä¸ªå…³é”®è¯ï¼Œé¢å¤–åŠ åˆ†
                if matched_keywords >= 2:
                    keyword_match_score += 0.15
                
                # å¦‚æœæ²¡æœ‰åŒ¹é…ä»»ä½•å…³é”®è¯ï¼Œé€‚å½“é™åˆ†ï¼ˆé¿å…ä¸ç›¸å…³ç»“æœæ’åè¿‡é«˜ï¼‰
                if matched_keywords == 0:
                    keyword_match_score = -0.1  # é™åˆ†0.1
                    print(f"   âš ï¸ æ— å…³é”®è¯åŒ¹é…: {doc.metadata.get('file_name', 'unknown')} é¡µ{doc.metadata.get('page_num', '?')} (è¯­ä¹‰åˆ†={similarity:.3f})")
                
                # ç»¼åˆç›¸ä¼¼åº¦ = è¯­ä¹‰ç›¸ä¼¼åº¦ + å…³é”®è¯åŒ¹é…åŠ åˆ†/é™åˆ†
                final_similarity = max(0.0, min(1.0, similarity + keyword_match_score))
                
                metadata = doc.metadata
                page_key = (
                    metadata.get("file_name", ""),
                    metadata.get("page_num", 0)
                )
                
                # å»é‡ï¼šåŒä¸€é¡µé¢çš„å¤šä¸ªchunkï¼Œåªä¿ç•™ç»¼åˆç›¸ä¼¼åº¦æœ€é«˜çš„
                if page_key in page_best_results:
                    if final_similarity > page_best_results[page_key]["score"]:
                        # æ‰¾åˆ°æ›´ç›¸å…³çš„chunkï¼Œæ›¿æ¢
                        page_best_results[page_key] = {
                            "content": doc.page_content,
                            "metadata": metadata,
                            "score": final_similarity,
                            "distance": distance,
                            "semantic_score": similarity,  # ä¿ç•™åŸå§‹è¯­ä¹‰ç›¸ä¼¼åº¦
                            "keyword_boost": keyword_match_score  # å…³é”®è¯åŠ åˆ†
                        }
                else:
                    page_best_results[page_key] = {
                        "content": doc.page_content,
                        "metadata": metadata,
                        "score": final_similarity,
                        "distance": distance,
                        "semantic_score": similarity,
                        "keyword_boost": keyword_match_score
                    }
            
            # è½¬æ¢ä¸ºåˆ—è¡¨
            formatted_results = list(page_best_results.values())
            
            # ä¼˜åŒ–æ’åºï¼šå¦‚æœæŒ‡å®šäº†file_nameï¼Œç»™å½“å‰æ–‡ä»¶çš„ç»“æœåŠ æƒ
            if file_name:
                for result in formatted_results:
                    if result["metadata"].get("file_name") == file_name:
                        # å½“å‰æ–‡ä»¶çš„ç»“æœï¼Œåˆ†æ•°åŠ æƒ +0.2
                        result["score"] = min(1.0, result["score"] + 0.2)
                        result["boosted"] = True
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            formatted_results.sort(key=lambda x: x["score"], reverse=True)
            
            # è°ƒè¯•ä¿¡æ¯
            print(f"   è¿‡æ»¤æ‰ {filtered_count} ä¸ªä½åˆ†ç»“æœ")
            print(f"   å»é‡åç»“æœæ•°: {len(formatted_results)}")
            if formatted_results:
                print(f"   æœ€é«˜åˆ†: {formatted_results[0]['score']:.3f} (è¯­ä¹‰: {formatted_results[0].get('semantic_score', 0):.3f}, å…³é”®è¯åŠ åˆ†: {formatted_results[0].get('keyword_boost', 0):.3f})")
                print(f"   æœ€ä½åˆ†: {formatted_results[-1]['score']:.3f}")
                # æ˜¾ç¤ºå‰3ä¸ªç»“æœçš„è¯¦ç»†ä¿¡æ¯
                print(f"   å‰3ä¸ªç»“æœè¯¦æƒ…:")
                for i, r in enumerate(formatted_results[:3]):
                    print(f"     {i+1}. {r['metadata'].get('file_name', 'unknown')} é¡µ{r['metadata'].get('page_num', '?')}: "
                          f"æ€»åˆ†={r['score']:.3f} (è¯­ä¹‰={r.get('semantic_score', 0):.3f}, "
                          f"å…³é”®è¯={r.get('keyword_boost', 0):.3f})")
                # æ˜¾ç¤ºç»“æœçš„æ–‡ä»¶åˆ†å¸ƒ
                file_distribution = {}
                for r in formatted_results:
                    fn = r['metadata'].get('file_name', 'unknown')
                    file_distribution[fn] = file_distribution.get(fn, 0) + 1
                print(f"   æ–‡ä»¶åˆ†å¸ƒ:")
                for fn, count in file_distribution.items():
                    is_target = " â­" if file_name and fn == file_name else ""
                    print(f"     - {fn}: {count} ä¸ªç»“æœ{is_target}")
            else:
                print(f"   âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„ç»“æœï¼")
                # å¦‚æœæ²¡æœ‰ç»“æœï¼Œé™ä½min_scoreé‡è¯•
                if min_score > 0:
                    print(f"   ğŸ’¡ æç¤º: å½“å‰min_score={min_score}å¯èƒ½è¿‡é«˜ï¼Œå°è¯•é™ä½æˆ–è®¾ä¸º0")
            
            # è¿”å›å‰ top_k ä¸ªç»“æœ
            return formatted_results[:top_k]
            
        except Exception as e:
            error_msg = str(e)
            print(f"âš ï¸  å‘é‡æœç´¢å¤±è´¥: {error_msg}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯Embedding APIé”™è¯¯
            if "500" in error_msg or "InternalServerError" in error_msg or "50500" in error_msg:
                print(f"âŒ Embedding API æœåŠ¡é”™è¯¯ (500)")
                print(f"   å¯èƒ½åŸå› :")
                print(f"   1. Embeddingæ¨¡å‹ä¸æ”¯æŒæˆ–é…ç½®é”™è¯¯")
                print(f"   2. APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
                print(f"   3. API Keyæƒé™ä¸è¶³")
                print(f"ğŸ’¡ è‡ªåŠ¨é™çº§åˆ°å…³é”®è¯æœç´¢...")
                
                # é™çº§åˆ°å…³é”®è¯æœç´¢
                try:
                    keyword_results = self.search_by_keyword(
                        query=query,
                        top_k=top_k,
                        file_name=file_name
                    )
                    if keyword_results:
                        print(f"âœ… å…³é”®è¯æœç´¢æˆåŠŸï¼Œè¿”å› {len(keyword_results)} ä¸ªç»“æœ")
                        print(f"ğŸ’¡ æç¤º: å…³é”®è¯æœç´¢åŸºäºæ–‡æœ¬åŒ¹é…ï¼Œå¯èƒ½ä¸å¦‚è¯­ä¹‰æœç´¢ç²¾ç¡®")
                        return keyword_results
                    else:
                        print(f"âš ï¸  å…³é”®è¯æœç´¢ä¹Ÿæ²¡æœ‰ç»“æœ")
                except Exception as e2:
                    print(f"âŒ å…³é”®è¯æœç´¢ä¹Ÿå¤±è´¥: {e2}")
            else:
                # å…¶ä»–ç±»å‹çš„é”™è¯¯
                print(f"âŒ å‘é‡æœç´¢é‡åˆ°æœªçŸ¥é”™è¯¯: {error_msg}")
                print(f"ğŸ’¡ å°è¯•é™çº§åˆ°å…³é”®è¯æœç´¢...")
                try:
                    keyword_results = self.search_by_keyword(
                        query=query,
                        top_k=top_k,
                        file_name=file_name
                    )
                    if keyword_results:
                        print(f"âœ… å…³é”®è¯æœç´¢æˆåŠŸï¼Œè¿”å› {len(keyword_results)} ä¸ªç»“æœ")
                        return keyword_results
                except Exception as e2:
                    print(f"âŒ å…³é”®è¯æœç´¢ä¹Ÿå¤±è´¥: {e2}")
            
            return []
    
    def search_by_keyword(
        self,
        query: str,
        top_k: int = 10,
        file_name: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        åŸºäºå…³é”®è¯çš„æ–‡æœ¬æœç´¢ï¼ˆä½œä¸ºå‘é‡æœç´¢çš„è¡¥å……ï¼‰
        é€‚ç”¨äºç²¾ç¡®å…³é”®è¯åŒ¹é…
        """
        if not self.vectorstore:
            return []
        
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            all_results = self.vectorstore.get()
            if not all_results or "documents" not in all_results:
                return []
            
            documents = all_results["documents"]
            metadatas = all_results.get("metadatas", [])
            
            # å…³é”®è¯æœç´¢
            query_lower = query.lower()
            results = []
            
            for i, doc_text in enumerate(documents):
                metadata = metadatas[i] if i < len(metadatas) else {}
                
                # æ–‡ä»¶è¿‡æ»¤
                if file_name and metadata.get("file_name") != file_name:
                    continue
                
                doc_text_lower = doc_text.lower()
                
                # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
                if query_lower in doc_text_lower:
                    # è®¡ç®—åŒ¹é…æ¬¡æ•°
                    match_count = doc_text_lower.count(query_lower)
                    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆåŸºäºåŒ¹é…æ¬¡æ•°å’Œæ–‡æ¡£é•¿åº¦ï¼‰
                    score = min(match_count / 10, 1.0)  # æœ€å¤š1.0
                    
                    results.append({
                        "content": doc_text,
                        "metadata": metadata,
                        "score": score,
                        "match_count": match_count,
                        "method": "keyword"
                    })
            
            # æŒ‰åŒ¹é…æ¬¡æ•°æ’åº
            results.sort(key=lambda x: (x["score"], x.get("match_count", 0)), reverse=True)
            
            return results[:top_k]
            
        except Exception as e:
            print(f"âš ï¸  å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_hybrid(
        self,
        query: str,
        top_k: int = 10,
        file_name: Optional[str] = None,
        semantic_weight: float = 0.6,
        keyword_weight: float = 0.4
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢ï¼šç»“åˆè¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            file_name: é™åˆ¶æœç´¢çš„æ–‡ä»¶
            semantic_weight: è¯­ä¹‰æœç´¢æƒé‡ï¼ˆ0-1ï¼‰
            keyword_weight: å…³é”®è¯æœç´¢æƒé‡ï¼ˆ0-1ï¼‰
        """
        # æ‰§è¡Œä¸¤ç§æœç´¢
        semantic_results = self.search_similar_slides(
            query=query,
            top_k=top_k * 2,
            file_name=file_name,
            min_score=0.0
        )
        
        keyword_results = self.search_by_keyword(
            query=query,
            top_k=top_k * 2,
            file_name=file_name
        )
        
        # åˆå¹¶ç»“æœ
        combined = {}
        
        # æ·»åŠ è¯­ä¹‰æœç´¢ç»“æœ
        for result in semantic_results:
            page_key = (
                result["metadata"].get("file_name", ""),
                result["metadata"].get("page_num", 0)
            )
            combined[page_key] = {
                "content": result["content"],
                "metadata": result["metadata"],
                "semantic_score": result["score"] * semantic_weight,
                "keyword_score": 0,
                "final_score": result["score"] * semantic_weight
            }
        
        # æ·»åŠ å…³é”®è¯æœç´¢ç»“æœ
        for result in keyword_results:
            page_key = (
                result["metadata"].get("file_name", ""),
                result["metadata"].get("page_num", 0)
            )
            keyword_score = result["score"] * keyword_weight
            
            if page_key in combined:
                combined[page_key]["keyword_score"] = keyword_score
                combined[page_key]["final_score"] += keyword_score
            else:
                combined[page_key] = {
                    "content": result["content"],
                    "metadata": result["metadata"],
                    "semantic_score": 0,
                    "keyword_score": keyword_score,
                    "final_score": keyword_score
                }
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        final_results = list(combined.values())
        final_results.sort(key=lambda x: x["final_score"], reverse=True)
        
        return final_results[:top_k]
    
    def search_by_file(self, file_name: str) -> List[Dict[str, Any]]:
        """è·å–ç‰¹å®šæ–‡ä»¶çš„æ‰€æœ‰åˆ‡ç‰‡"""
        if not self.vectorstore:
            return []
        
        try:
            results = self.vectorstore.get(where={"file_name": file_name})
            
            formatted_results = []
            if results and "documents" in results:
                for i, doc_content in enumerate(results["documents"]):
                    metadata = results["metadatas"][i] if "metadatas" in results else {}
                    formatted_results.append({
                        "content": doc_content,
                        "metadata": metadata
                    })
            
            return formatted_results
        except Exception as e:
            print(f"âš ï¸  æŒ‰æ–‡ä»¶æœç´¢å¤±è´¥: {e}")
            return []
    
    def delete_file_slides(self, file_name: str) -> bool:
        """åˆ é™¤ç‰¹å®šæ–‡ä»¶çš„æ‰€æœ‰åˆ‡ç‰‡"""
        if not self.vectorstore:
            return False
        
        try:
            results = self.vectorstore.get(where={"file_name": file_name})
            
            if results and "ids" in results:
                ids_to_delete = results["ids"]
                if ids_to_delete:
                    self.vectorstore.delete(ids=ids_to_delete)
                    try:
                        self.vectorstore.persist()
                    except:
                        pass
                    print(f"âœ… å·²åˆ é™¤æ–‡ä»¶ {file_name} çš„ {len(ids_to_delete)} ä¸ªåˆ‡ç‰‡")
                    return True
            
            return False
        except Exception as e:
            print(f"âš ï¸  åˆ é™¤æ–‡ä»¶åˆ‡ç‰‡å¤±è´¥: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        if not self.vectorstore:
            return {"total_documents": 0, "total_files": 0}
        
        try:
            all_results = self.vectorstore.get()
            
            total_docs = len(all_results.get("ids", [])) if all_results else 0
            
            # ç»Ÿè®¡ä¿¡æ¯
            file_types = defaultdict(int)
            file_names = set()
            page_count_by_file = defaultdict(int)
            
            if all_results and "metadatas" in all_results:
                for metadata in all_results["metadatas"]:
                    file_type = metadata.get("file_type", "unknown")
                    file_name = metadata.get("file_name", "unknown")
                    
                    file_types[file_type] += 1
                    file_names.add(file_name)
                    page_count_by_file[file_name] += 1
            
            stats = {
                "total_documents": total_docs,
                "total_files": len(file_names),
                "file_types": dict(file_types),
                "files": dict(page_count_by_file),
                "vector_db_path": self.vector_db_path
            }
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š å‘é‡æ•°æ®åº“ç»Ÿè®¡:")
            print(f"   æ€»æ–‡æ¡£æ•°: {total_docs}")
            print(f"   æ–‡ä»¶æ•°: {len(file_names)}")
            if file_names:
                print(f"   æ–‡ä»¶åˆ—è¡¨:")
                for fn in file_names:
                    print(f"     - {fn}: {page_count_by_file[fn]} é¡µ")
            
            return stats
        except Exception as e:
            print(f"âš ï¸  è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"total_documents": 0, "error": str(e)}
