"""
å‘é‡å­˜å‚¨æœåŠ¡ - ç”¨äºå­˜å‚¨å’Œæ£€ç´¢ PPT/PDF åˆ‡ç‰‡
æ”¯æŒåŸºäºè¯­ä¹‰çš„ç›¸å…³æ€§æ£€ç´¢
"""

import os
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime

from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ä¼˜å…ˆä½¿ç”¨æ–°çš„ langchain-chromaï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ°æ—§ç‰ˆæœ¬
try:
    from langchain_chroma import Chroma
except ImportError:
    try:
        from langchain_community.vectorstores import Chroma
    except ImportError:
        raise ImportError("éœ€è¦å®‰è£… langchain-chroma æˆ– langchain-community")

from src.agents.base import LLMConfig


class VectorStoreService:
    """å‘é‡å­˜å‚¨æœåŠ¡ - å­˜å‚¨ PPT/PDF åˆ‡ç‰‡å¹¶æ”¯æŒè¯­ä¹‰æ£€ç´¢"""
    
    def __init__(self, llm_config: LLMConfig, vector_db_path: str = "./ppt_vector_db"):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨æœåŠ¡
        
        Args:
            llm_config: LLM é…ç½®ï¼ˆç”¨äºåˆ›å»º embeddingsï¼‰
            vector_db_path: å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„
        """
        self.llm_config = llm_config
        self.vector_db_path = vector_db_path
        # åˆå§‹åŒ– embeddings
        # æ³¨æ„ï¼šæŸäº› API å¯èƒ½ä¸æ”¯æŒ model å‚æ•°ï¼Œå…ˆå°è¯•ä¸æŒ‡å®š
        try:
            self.embeddings = OpenAIEmbeddings(
                api_key=llm_config.api_key,
                base_url=llm_config.base_url,
                model="BAAI/bge-large-zh-v1.5"
            )
        except Exception:
            # å¦‚æœæŒ‡å®šæ¨¡å‹å¤±è´¥ï¼Œå°è¯•ä¸æŒ‡å®šæ¨¡å‹ï¼ˆä½¿ç”¨é»˜è®¤ï¼‰
            self.embeddings = OpenAIEmbeddings(
                api_key=llm_config.api_key,
                base_url=llm_config.base_url
            )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=50,
            length_function=len,
        )
        self.vectorstore: Optional[Chroma] = None
        try:
            self._initialize_vectorstore()
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"   æç¤º: å‘é‡å­˜å‚¨åŠŸèƒ½å°†ä¸å¯ç”¨ï¼Œä½†å…¶ä»–åŠŸèƒ½æ­£å¸¸")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸æœåŠ¡ç»§ç»­è¿è¡Œï¼ˆå­˜å‚¨åŠŸèƒ½ä¼šå¤±è´¥ï¼Œä½†ä¸å½±å“å…¶ä»–åŠŸèƒ½ï¼‰
            self.vectorstore = None
    
    def _initialize_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        max_retries = 3
        last_error = None
        
        for attempt in range(max_retries):
            try:
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(self.vector_db_path, exist_ok=True)
                
                # å°è¯•åŠ è½½ç°æœ‰çš„å‘é‡æ•°æ®åº“æˆ–åˆ›å»ºæ–°çš„
                if os.path.exists(self.vector_db_path) and os.listdir(self.vector_db_path):
                    # ç›®å½•å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼Œå°è¯•åŠ è½½
                    try:
                        self.vectorstore = Chroma(
                            persist_directory=self.vector_db_path,
                            embedding_function=self.embeddings
                        )
                        # éªŒè¯åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
                        if self.vectorstore is not None:
                            print(f"âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ (è·¯å¾„: {self.vector_db_path})")
                            return
                    except Exception as load_error:
                        print(f"âš ï¸  åŠ è½½ç°æœ‰æ•°æ®åº“å¤±è´¥ï¼Œå°è¯•åˆ›å»ºæ–°çš„: {load_error}")
                        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ é™¤æ—§ç›®å½•é‡æ–°åˆ›å»º
                        import shutil
                        try:
                            shutil.rmtree(self.vector_db_path)
                            os.makedirs(self.vector_db_path, exist_ok=True)
                        except:
                            pass
                
                # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
                self.vectorstore = Chroma(
                    persist_directory=self.vector_db_path,
                    embedding_function=self.embeddings
                )
                
                # éªŒè¯åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
                if self.vectorstore is not None:
                    print(f"âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ (è·¯å¾„: {self.vector_db_path})")
                    return
                else:
                    raise Exception("å‘é‡æ•°æ®åº“å¯¹è±¡åˆ›å»ºå¤±è´¥")
                    
            except Exception as e:
                last_error = e
                print(f"âš ï¸  åˆå§‹åŒ–å‘é‡æ•°æ®åº“å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(0.5)  # ç­‰å¾…åé‡è¯•
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    print(f"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–æœ€ç»ˆå¤±è´¥: {e}")
                    raise Exception(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}") from last_error
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise Exception(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {last_error}")
    
    def _create_document_id(self, file_name: str, page_num: int, chunk_index: int = 0) -> str:
        """åˆ›å»ºæ–‡æ¡£ ID"""
        return f"{file_name}_{page_num}_{chunk_index}_{uuid.uuid4().hex[:8]}"
    
    def _slide_to_text(self, slide: Dict[str, Any]) -> str:
        """å°†å¹»ç¯ç‰‡æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬ç”¨äºå‘é‡åŒ–"""
        text_parts = []
        
        # æ·»åŠ æ ‡é¢˜
        if slide.get("title"):
            text_parts.append(f"æ ‡é¢˜: {slide['title']}")
        
        # æ·»åŠ å†…å®¹ç‚¹
        if slide.get("raw_points"):
            for point in slide["raw_points"]:
                if isinstance(point, dict):
                    text_parts.append(point.get("text", ""))
                elif isinstance(point, str):
                    text_parts.append(point)
        
        # æ·»åŠ ç±»å‹ä¿¡æ¯
        if slide.get("type"):
            text_parts.append(f"ç±»å‹: {slide['type']}")
        
        return "\n".join(text_parts)
    
    def store_document_slides(
        self,
        file_name: str,
        file_type: str,  # "pdf" æˆ– "pptx"
        slides: List[Dict[str, Any]],
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        å­˜å‚¨æ–‡æ¡£çš„æ‰€æœ‰å¹»ç¯ç‰‡åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            file_name: æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹ ("pdf" æˆ– "pptx")
            slides: å¹»ç¯ç‰‡åˆ—è¡¨ï¼ˆæ¥è‡ª DocumentParserServiceï¼‰
            metadata: é¢å¤–çš„å…ƒæ•°æ®
        
        Returns:
            å­˜å‚¨ç»“æœç»Ÿè®¡
        """
        if not self.vectorstore:
            error_msg = (
                "å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ã€‚å¯èƒ½çš„åŸå› ï¼š\n"
                "1. API Key é…ç½®é”™è¯¯æˆ–æ— æ•ˆ\n"
                "2. Embedding API è°ƒç”¨å¤±è´¥\n"
                "3. æ•°æ®åº“ç›®å½•æƒé™é—®é¢˜\n"
                "4. ä¾èµ–åŒ…æœªæ­£ç¡®å®‰è£…ï¼ˆéœ€è¦ langchain-chroma æˆ– langchain-communityï¼‰\n"
                "è¯·æŸ¥çœ‹ä¸Šé¢çš„é”™è¯¯æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯"
            )
            print(f"âŒ {error_msg}")
            raise Exception("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
        
        documents = []
        metadatas = []
        ids = []
        
        total_chunks = 0
        
        for slide in slides:
            # å°†å¹»ç¯ç‰‡è½¬æ¢ä¸ºæ–‡æœ¬
            slide_text = self._slide_to_text(slide)
            
            if not slide_text.strip():
                continue
            
            # å¦‚æœæ–‡æœ¬è¾ƒé•¿ï¼Œè¿›è¡Œåˆ†å—
            chunks = self.text_splitter.split_text(slide_text)
            
            for chunk_index, chunk in enumerate(chunks):
                if not chunk.strip():
                    continue
                
                # åˆ›å»ºæ–‡æ¡£
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "file_name": file_name,
                        "file_type": file_type,
                        "page_num": slide.get("page_num", 0),
                        "slide_title": slide.get("title", ""),
                        "slide_type": slide.get("type", "content"),
                        "chunk_index": chunk_index,
                        "total_chunks": len(chunks),
                        "stored_at": datetime.now().isoformat(),
                        **(metadata or {})
                    }
                )
                
                doc_id = self._create_document_id(file_name, slide.get("page_num", 0), chunk_index)
                
                documents.append(doc)
                metadatas.append(doc.metadata)
                ids.append(doc_id)
                total_chunks += 1

        if documents:
            # --- ä¿®æ”¹éƒ¨åˆ†ï¼šåˆ†æ‰¹æ¬¡å†™å…¥ï¼Œåº”å¯¹ API é™åˆ¶ ---
            batch_size = 3  # æ ¹æ®æŠ¥é”™ä¿¡æ¯ï¼Œè¿™é‡Œè®¾ä¸º 3ï¼ˆç”šè‡³å¯ä»¥è®¾ä¸º 1 æœ€ç¨³å¦¥ï¼‰
            print(f"ğŸ“¦ æ­£åœ¨åˆ†æ‰¹å­˜å‚¨å‘é‡ï¼Œæ¯æ‰¹ {batch_size} æ¡ï¼Œæ€»è®¡ {len(documents)} æ¡...")

            for i in range(0, len(documents), batch_size):
                batch_docs = documents[i: i + batch_size]
                batch_ids = ids[i: i + batch_size]
                try:
                    self.vectorstore.add_documents(
                        documents=batch_docs,
                        ids=batch_ids
                    )
                    # print(f"  âœ… å·²å®Œæˆ {min(i + batch_size, len(documents))}/{len(documents)}")
                except Exception as batch_error:
                    print(f"  âŒ æ‰¹æ¬¡ {i // batch_size + 1} å­˜å‚¨å¤±è´¥: {batch_error}")
                    # å¦‚æœæŸä¸€æ‰¹å¤±è´¥ï¼Œå¯ä»¥é€‰æ‹©ç»§ç»­æˆ–è·³è¿‡

            # éƒ¨åˆ†æ—§ç‰ˆæœ¬ Chroma éœ€è¦æ‰‹åŠ¨ persistï¼Œæ–°ç‰ˆæœ¬å·²è‡ªåŠ¨æŒä¹…åŒ–
            try:
                self.vectorstore.persist()
            except:
                pass
        
        return {
            "file_name": file_name,
            "file_type": file_type,
            "total_slides": len(slides),
            "total_chunks": total_chunks,
            "stored_at": datetime.now().isoformat()
        }
    
    def search_similar_slides(
        self,
        query: str,
        top_k: int = 5,
        file_name: Optional[str] = None,
        file_type: Optional[str] = None,
        min_score: float = 0.0
    ) -> List[Dict[str, Any]]:
        """
        åŸºäºè¯­ä¹‰æœç´¢ç›¸ä¼¼çš„å¹»ç¯ç‰‡åˆ‡ç‰‡
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
            file_name: å¯é€‰ï¼Œé™åˆ¶æœç´¢ç‰¹å®šæ–‡ä»¶
            file_type: å¯é€‰ï¼Œé™åˆ¶æœç´¢ç‰¹å®šæ–‡ä»¶ç±»å‹
            min_score: æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«ï¼š
            - content: æ–‡æœ¬å†…å®¹
            - metadata: å…ƒæ•°æ®ï¼ˆæ–‡ä»¶ã€é¡µç ç­‰ï¼‰
            - score: ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if not self.vectorstore:
            return []
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where = {}
        if file_name:
            where["file_name"] = file_name
        if file_type:
            where["file_type"] = file_type
        
        try:
            # ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢
            if where:
                results = self.vectorstore.similarity_search_with_score(
                    query,
                    k=top_k,
                    filter=where
                )
            else:
                results = self.vectorstore.similarity_search_with_score(
                    query,
                    k=top_k
                )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for doc, score in results:
                # ç›¸ä¼¼åº¦åˆ†æ•°è½¬æ¢ä¸º 0-1 èŒƒå›´ï¼ˆChromaDB ä½¿ç”¨è·ç¦»ï¼Œéœ€è¦è½¬æ¢ï¼‰
                similarity_score = 1 / (1 + score) if score > 0 else 1.0
                
                if similarity_score >= min_score:
                    formatted_results.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "score": similarity_score,
                        "distance": score
                    })
            
            return formatted_results
        except Exception as e:
            print(f"âš ï¸  æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_by_file(self, file_name: str) -> List[Dict[str, Any]]:
        """
        è·å–ç‰¹å®šæ–‡ä»¶çš„æ‰€æœ‰åˆ‡ç‰‡
        
        Args:
            file_name: æ–‡ä»¶å
        
        Returns:
            è¯¥æ–‡ä»¶çš„æ‰€æœ‰åˆ‡ç‰‡
        """
        if not self.vectorstore:
            return []
        
        try:
            # ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤
            results = self.vectorstore.get(
                where={"file_name": file_name}
            )
            
            formatted_results = []
            if results and "documents" in results:
                for i, doc_content in enumerate(results["documents"]):
                    metadata = results["metadatas"][i] if "metadatas" in results else {}
                    formatted_results.append({
                        "content": doc_content,
                        "metadata": metadata
                    })
            
            return formatted_results
        except Exception as e:
            print(f"âš ï¸  æŒ‰æ–‡ä»¶æœç´¢å¤±è´¥: {e}")
            return []
    
    def delete_file_slides(self, file_name: str) -> bool:
        """
        åˆ é™¤ç‰¹å®šæ–‡ä»¶çš„æ‰€æœ‰åˆ‡ç‰‡
        
        Args:
            file_name: æ–‡ä»¶å
        
        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        if not self.vectorstore:
            return False
        
        try:
            # è·å–è¯¥æ–‡ä»¶çš„æ‰€æœ‰ ID
            results = self.vectorstore.get(
                where={"file_name": file_name}
            )
            
            if results and "ids" in results:
                ids_to_delete = results["ids"]
                if ids_to_delete:
                    self.vectorstore.delete(ids=ids_to_delete)
                    self.vectorstore.persist()
                    return True
            
            return False
        except Exception as e:
            print(f"âš ï¸  åˆ é™¤æ–‡ä»¶åˆ‡ç‰‡å¤±è´¥: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.vectorstore:
            return {"total_documents": 0, "collections": []}
        
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            all_results = self.vectorstore.get()
            
            total_docs = len(all_results.get("ids", [])) if all_results else 0
            
            # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
            file_types = {}
            file_names = set()
            
            if all_results and "metadatas" in all_results:
                for metadata in all_results["metadatas"]:
                    file_type = metadata.get("file_type", "unknown")
                    file_types[file_type] = file_types.get(file_type, 0) + 1
                    file_names.add(metadata.get("file_name", "unknown"))
            
            return {
                "total_documents": total_docs,
                "total_files": len(file_names),
                "file_types": file_types,
                "vector_db_path": self.vector_db_path
            }
        except Exception as e:
            print(f"âš ï¸  è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"total_documents": 0, "error": str(e)}

