"""å¤–éƒ¨èµ„æºæœç´¢æœåŠ¡ - è”ç½‘æœç´¢ Wikipediaã€Arxivã€Web"""

import asyncio
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
import logging

logger = logging.getLogger(__name__)


class SearchResult(BaseModel):
    """æœç´¢ç»“æœé¡¹"""
    title: str = Field(description="æ ‡é¢˜")
    url: str = Field(description="é“¾æ¥")
    source: str = Field(description="æ¥æºï¼ˆwikipedia/arxiv/webï¼‰")
    snippet: str = Field(description="æ‘˜è¦/ç‰‡æ®µ")
    authors: Optional[List[str]] = Field(default=None, description="ä½œè€…ï¼ˆä»…å­¦æœ¯è®ºæ–‡ï¼‰")
    published: Optional[str] = Field(default=None, description="å‘å¸ƒæ—¥æœŸ")
    score: Optional[float] = Field(default=None, description="ç›¸å…³æ€§è¯„åˆ†")


class ExternalSearchResult(BaseModel):
    """å¤–éƒ¨æœç´¢ç»“æœ"""
    query: str = Field(description="æœç´¢æŸ¥è¯¢")
    total_results: int = Field(description="æ€»ç»“æœæ•°")
    results: List[SearchResult] = Field(description="æœç´¢ç»“æœåˆ—è¡¨")
    sources_used: List[str] = Field(description="ä½¿ç”¨çš„æœç´¢æº")


class ExternalSearchService:
    """å¤–éƒ¨èµ„æºæœç´¢æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self._wikipedia_available = False
        self._arxiv_available = False
        self._web_available = False
        
        # å¯¼å…¥å„ä¸ªæœç´¢åº“
        try:
            import wikipedia
            self._wikipedia = wikipedia
            self._wikipedia_available = True
            logger.info("âœ… Wikipedia æœç´¢å·²å¯ç”¨")
        except ImportError:
            logger.warning("âš ï¸ Wikipedia åº“æœªå®‰è£…ï¼ŒWikipedia æœç´¢ä¸å¯ç”¨")
        
        try:
            import arxiv
            self._arxiv = arxiv
            self._arxiv_available = True
            logger.info("âœ… Arxiv æœç´¢å·²å¯ç”¨")
        except ImportError:
            logger.warning("âš ï¸ Arxiv åº“æœªå®‰è£…ï¼ŒArxiv æœç´¢ä¸å¯ç”¨")
        
        try:
            from duckduckgo_search import DDGS
            self._ddgs = DDGS
            self._web_available = True
            logger.info("âœ… Web æœç´¢å·²å¯ç”¨ï¼ˆDuckDuckGoï¼‰")
        except ImportError:
            logger.warning("âš ï¸ DuckDuckGo æœç´¢åº“æœªå®‰è£…ï¼ŒWeb æœç´¢ä¸å¯ç”¨")
    
    async def search_wikipedia(
        self,
        query: str,
        max_results: int = 3,
        lang: str = "zh"
    ) -> List[SearchResult]:
        """æœç´¢ Wikipedia
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            lang: è¯­è¨€ï¼ˆzh/enï¼‰
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self._wikipedia_available:
            logger.warning("Wikipedia æœç´¢ä¸å¯ç”¨")
            return []
        
        try:
            self._wikipedia.set_lang(lang)
  
            search_results = self._wikipedia.search(query, results=max_results)
            
            results = []
            for title in search_results[:max_results]:
                try:
                    page = self._wikipedia.page(title, auto_suggest=False)
                    
                    result = SearchResult(
                        title=page.title,
                        url=page.url,
                        source="wikipedia",
                        snippet=page.summary[:300] + "..." if len(page.summary) > 300 else page.summary,
                        published=None,
                        authors=None
                    )
                    results.append(result)
                except Exception as e:
                    logger.warning(f"è·å– Wikipedia é¡µé¢å¤±è´¥ '{title}': {e}")
                    continue
            
            logger.info(f"âœ… Wikipedia æœç´¢å®Œæˆ: {query} -> {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Wikipedia æœç´¢å¤±è´¥: {e}")
            return []
    
    async def search_arxiv(
        self,
        query: str,
        max_results: int = 3
    ) -> List[SearchResult]:
        """æœç´¢ Arxiv å­¦æœ¯è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self._arxiv_available:
            logger.warning("Arxiv æœç´¢ä¸å¯ç”¨")
            return []
        
        try:
            # åˆ›å»ºæœç´¢å®¢æˆ·ç«¯
            client = self._arxiv.Client()

            search = self._arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=self._arxiv.SortCriterion.Relevance
            )
            
            results = []
            for paper in client.results(search):
                authors = [author.name for author in paper.authors[:3]]  
                if len(paper.authors) > 3:
                    authors.append("et al.")
                
                result = SearchResult(
                    title=paper.title,
                    url=paper.entry_id,
                    source="arxiv",
                    snippet=paper.summary[:300] + "..." if len(paper.summary) > 300 else paper.summary,
                    authors=authors,
                    published=paper.published.strftime("%Y-%m-%d") if paper.published else None
                )
                results.append(result)
            
            logger.info(f"âœ… Arxiv æœç´¢å®Œæˆ: {query} -> {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Arxiv æœç´¢å¤±è´¥: {e}")
            return []
    
    async def search_web(
        self,
        query: str,
        max_results: int = 5
    ) -> List[SearchResult]:
        """æœç´¢ Webï¼ˆä½¿ç”¨ DuckDuckGoï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self._web_available:
            logger.warning("Web æœç´¢ä¸å¯ç”¨")
            return []
        
        try:
            # åˆ›å»ºæœç´¢å®ä¾‹
            ddgs = self._ddgs()
            
            search_results = ddgs.text(query, max_results=max_results)
            
            results = []
            for item in search_results:
                result = SearchResult(
                    title=item.get("title", "æ— æ ‡é¢˜"),
                    url=item.get("href", ""),
                    source="web",
                    snippet=item.get("body", "")[:300],
                    published=None,
                    authors=None
                )
                results.append(result)
            
            logger.info(f"âœ… Web æœç´¢å®Œæˆ: {query} -> {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Web æœç´¢å¤±è´¥: {e}")
            return []
    
    async def search_all(
        self,
        query: str,
        sources: Optional[List[str]] = None,
        max_results_per_source: int = 3
    ) -> ExternalSearchResult:
        """ç»¼åˆæœç´¢æ‰€æœ‰æ¥æº
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            sources: æŒ‡å®šæœç´¢æºåˆ—è¡¨ï¼Œå¦‚ ["wikipedia", "arxiv", "web"]ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
            max_results_per_source: æ¯ä¸ªæ¥æºçš„æœ€å¤§ç»“æœæ•°
        
        Returns:
            ç»¼åˆæœç´¢ç»“æœ
        """
        # é»˜è®¤æœç´¢æ‰€æœ‰å¯ç”¨æº
        if sources is None:
            sources = []
            if self._wikipedia_available:
                sources.append("wikipedia")
            if self._arxiv_available:
                sources.append("arxiv")
            if self._web_available:
                sources.append("web")
        
        logger.info(f"ğŸ” å¼€å§‹ç»¼åˆæœç´¢: {query}, æ¥æº: {sources}")
        
        # å¹¶å‘æœç´¢æ‰€æœ‰æ¥æº
        tasks = []
        sources_used = []
        
        if "wikipedia" in sources and self._wikipedia_available:
            tasks.append(self.search_wikipedia(query, max_results_per_source))
            sources_used.append("wikipedia")
        
        if "arxiv" in sources and self._arxiv_available:
            tasks.append(self.search_arxiv(query, max_results_per_source))
            sources_used.append("arxiv")
        
        if "web" in sources and self._web_available:
            tasks.append(self.search_web(query, max_results_per_source))
            sources_used.append("web")
        
        all_results = []
        if tasks:
            search_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in search_results:
                if isinstance(result, list):
                    all_results.extend(result)
                elif isinstance(result, Exception):
                    logger.error(f"æœç´¢ä»»åŠ¡å¤±è´¥: {result}")
        
        logger.info(f"âœ… ç»¼åˆæœç´¢å®Œæˆ: {query} -> {len(all_results)} ä¸ªç»“æœ")
        
        return ExternalSearchResult(
            query=query,
            total_results=len(all_results),
            results=all_results,
            sources_used=sources_used
        )
    
    async def search_by_concepts(
        self,
        concepts: List[str],
        sources: Optional[List[str]] = None,
        max_results_per_concept: int = 2
    ) -> Dict[str, ExternalSearchResult]:
        """æŒ‰æ¦‚å¿µåˆ—è¡¨æœç´¢
        
        Args:
            concepts: æ¦‚å¿µåˆ—è¡¨
            sources: æŒ‡å®šæœç´¢æº
            max_results_per_concept: æ¯ä¸ªæ¦‚å¿µçš„æœ€å¤§ç»“æœæ•°
        
        Returns:
            æŒ‰æ¦‚å¿µç»„ç»‡çš„æœç´¢ç»“æœ
        """
        logger.info(f"ğŸ” å¼€å§‹æŒ‰æ¦‚å¿µæœç´¢: {len(concepts)} ä¸ªæ¦‚å¿µ")
        
        # å¹¶å‘æœç´¢æ‰€æœ‰æ¦‚å¿µ
        tasks = [
            self.search_all(concept, sources, max_results_per_concept)
            for concept in concepts
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»„ç»‡ç»“æœ
        concept_results = {}
        for concept, result in zip(concepts, results):
            if isinstance(result, ExternalSearchResult):
                concept_results[concept] = result
            else:
                logger.error(f"æ¦‚å¿µ '{concept}' æœç´¢å¤±è´¥: {result}")
                concept_results[concept] = ExternalSearchResult(
                    query=concept,
                    total_results=0,
                    results=[],
                    sources_used=[]
                )
        
        logger.info(f"âœ… æŒ‰æ¦‚å¿µæœç´¢å®Œæˆ: {len(concept_results)} ä¸ªæ¦‚å¿µ")
        return concept_results
    
    def get_available_sources(self) -> List[str]:
        """è·å–å¯ç”¨çš„æœç´¢æºåˆ—è¡¨
        
        Returns:
            å¯ç”¨æœç´¢æºåˆ—è¡¨
        """
        sources = []
        if self._wikipedia_available:
            sources.append("wikipedia")
        if self._arxiv_available:
            sources.append("arxiv")
        if self._web_available:
            sources.append("web")
        return sources
    
    def is_available(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æœç´¢æºå¯ç”¨
        
        Returns:
            æ˜¯å¦æœ‰å¯ç”¨çš„æœç´¢æº
        """
        return self._wikipedia_available or self._arxiv_available or self._web_available

