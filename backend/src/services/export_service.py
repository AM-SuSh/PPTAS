"""
å¯¼å‡ºæœåŠ¡ï¼šå°†AIåˆ†æç»“æœå¯¼å‡ºä¸ºMarkdownæ ¼å¼
"""
from typing import Dict, List, Optional, Any


class ExportService:
    """å¯¼å‡ºæœåŠ¡ï¼Œç”¨äºå°†AIåˆ†æç»“æœè½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    
    def export_summary_markdown(
        self,
        doc_info: Dict[str, Any],
        global_analysis: Optional[Dict[str, Any]] = None,
        page_count: int = 0,
        analyzed_pages: int = 0
    ) -> str:
        """
        å¯¼å‡ºåˆ†ææ‘˜è¦ä¸ºMarkdownæ ¼å¼
        
        Args:
            doc_info: æ–‡æ¡£ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« file_name, file_type, created_at, updated_at
            global_analysis: å…¨å±€åˆ†æç»“æœ
            page_count: æ–‡æ¡£æ€»é¡µæ•°
            analyzed_pages: å·²åˆ†æé¡µæ•°
            
        Returns:
            Markdownæ ¼å¼çš„å­—ç¬¦ä¸²
        """
        lines = []
        
        # æ–‡æ¡£æ ‡é¢˜
        lines.append(f"# {doc_info.get('file_name', 'æœªçŸ¥æ–‡æ¡£')} - AIåˆ†ææ‘˜è¦")
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # æ–‡æ¡£ä¿¡æ¯
        lines.append("## ğŸ“„ æ–‡æ¡£ä¿¡æ¯")
        lines.append("")
        lines.append(f"- **æ–‡ä»¶å**: {doc_info.get('file_name', 'æœªçŸ¥')}")
        lines.append(f"- **æ–‡ä»¶ç±»å‹**: {doc_info.get('file_type', 'unknown')}")
        lines.append(f"- **æ€»é¡µæ•°**: {page_count}")
        lines.append(f"- **å·²åˆ†æé¡µæ•°**: {analyzed_pages}")
        if doc_info.get('created_at'):
            lines.append(f"- **åˆ›å»ºæ—¶é—´**: {doc_info['created_at']}")
        if doc_info.get('updated_at'):
            lines.append(f"- **æ›´æ–°æ—¶é—´**: {doc_info['updated_at']}")
        lines.append("")
        
        # å…¨å±€åˆ†ææ‘˜è¦
        if global_analysis:
            lines.append("## ğŸ“š å…¨å±€åˆ†ææ‘˜è¦")
            lines.append("")
            
            # ä¸»é¢˜
            main_topic = global_analysis.get('main_topic', 'æœªçŸ¥')
            if main_topic and main_topic != 'æœªçŸ¥':
                lines.append(f"### æ ¸å¿ƒä¸»é¢˜")
                lines.append("")
                lines.append(f"{main_topic}")
                lines.append("")
            
            # çŸ¥è¯†æµç¨‹
            knowledge_flow = global_analysis.get('knowledge_flow', '')
            if knowledge_flow:
                lines.append(f"### çŸ¥è¯†é€»è¾‘æµç¨‹")
                lines.append("")
                lines.append(f"{knowledge_flow}")
                lines.append("")
            
            # ç« èŠ‚ç»“æ„
            chapters = global_analysis.get('chapters', [])
            if chapters:
                lines.append("### ç« èŠ‚ç»“æ„")
                lines.append("")
                for i, chapter in enumerate(chapters, 1):
                    title = chapter.get('title', f'ç« èŠ‚{i}')
                    pages = chapter.get('pages', [])
                    key_concepts = chapter.get('key_concepts', [])
                    
                    lines.append(f"#### {i}. {title}")
                    if pages:
                        lines.append(f"- **é¡µç **: {', '.join(map(str, pages))}")
                    if key_concepts:
                        lines.append(f"- **æ ¸å¿ƒæ¦‚å¿µ**: {', '.join(key_concepts)}")
                    lines.append("")
            
            # çŸ¥è¯†ç‚¹å•å…ƒç»Ÿè®¡
            knowledge_units = global_analysis.get('knowledge_units', [])
            if knowledge_units:
                lines.append("### çŸ¥è¯†ç‚¹å•å…ƒç»Ÿè®¡")
                lines.append("")
                lines.append(f"å…±è¯†åˆ«å‡º **{len(knowledge_units)}** ä¸ªçŸ¥è¯†ç‚¹å•å…ƒï¼š")
                lines.append("")
                for unit in knowledge_units:
                    title = unit.get('title', 'æœªçŸ¥çŸ¥è¯†ç‚¹')
                    pages = unit.get('pages', [])
                    core_concepts = unit.get('core_concepts', [])
                    
                    lines.append(f"- **{title}**")
                    if pages:
                        lines.append(f"  - æ¶‰åŠé¡µé¢: {', '.join(map(str, pages))}")
                    if core_concepts:
                        lines.append(f"  - æ ¸å¿ƒæ¦‚å¿µ: {', '.join(core_concepts)}")
                    lines.append("")
        else:
            lines.append("## âš ï¸ å…¨å±€åˆ†æ")
            lines.append("")
            lines.append("è¯¥æ–‡æ¡£å°šæœªè¿›è¡Œå…¨å±€åˆ†æã€‚")
            lines.append("")
        
        lines.append("---")
        lines.append("")
        lines.append("*æœ¬æ–‡æ¡£ç”± PPTAS AI åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*")
        
        return "\n".join(lines)
    
    def export_to_markdown(
        self,
        doc_info: Dict[str, Any],
        global_analysis: Optional[Dict[str, Any]] = None,
        page_analyses: Optional[Dict[int, Dict[str, Any]]] = None,
        include_global: bool = True,
        include_pages: bool = True,
        page_range: Optional[List[int]] = None
    ) -> str:
        """
        å¯¼å‡ºå®Œæ•´åˆ†æå†…å®¹ä¸ºMarkdownæ ¼å¼
        
        Args:
            doc_info: æ–‡æ¡£ä¿¡æ¯å­—å…¸
            global_analysis: å…¨å±€åˆ†æç»“æœ
            page_analyses: é¡µé¢åˆ†æç»“æœå­—å…¸ï¼Œkeyä¸ºpage_idï¼Œvalueä¸ºåˆ†ææ•°æ®
            include_global: æ˜¯å¦åŒ…å«å…¨å±€åˆ†æ
            include_pages: æ˜¯å¦åŒ…å«é¡µé¢åˆ†æ
            page_range: é¡µé¢èŒƒå›´ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            
        Returns:
            Markdownæ ¼å¼çš„å­—ç¬¦ä¸²
        """
        lines = []
        
        # æ–‡æ¡£æ ‡é¢˜
        lines.append(f"# {doc_info.get('file_name', 'æœªçŸ¥æ–‡æ¡£')} - AIåˆ†æè¡¥å……å†…å®¹")
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # æ–‡æ¡£ä¿¡æ¯
        lines.append("## ğŸ“„ æ–‡æ¡£ä¿¡æ¯")
        lines.append("")
        lines.append(f"- **æ–‡ä»¶å**: {doc_info.get('file_name', 'æœªçŸ¥')}")
        lines.append(f"- **æ–‡ä»¶ç±»å‹**: {doc_info.get('file_type', 'unknown')}")
        if doc_info.get('created_at'):
            lines.append(f"- **åˆ›å»ºæ—¶é—´**: {doc_info['created_at']}")
        if doc_info.get('updated_at'):
            lines.append(f"- **æ›´æ–°æ—¶é—´**: {doc_info['updated_at']}")
        if page_range:
            lines.append(f"- **å¯¼å‡ºé¡µé¢èŒƒå›´**: {', '.join(map(str, sorted(page_range)))}")
        lines.append("")
        
        # å…¨å±€åˆ†æ
        if include_global and global_analysis:
            lines.append("## ğŸ“š å…¨å±€åˆ†æ")
            lines.append("")
            
            # ä¸»é¢˜
            main_topic = global_analysis.get('main_topic', 'æœªçŸ¥')
            if main_topic and main_topic != 'æœªçŸ¥':
                lines.append(f"### æ ¸å¿ƒä¸»é¢˜")
                lines.append("")
                lines.append(f"**{main_topic}**")
                lines.append("")
            
            # çŸ¥è¯†æµç¨‹
            knowledge_flow = global_analysis.get('knowledge_flow', '')
            if knowledge_flow:
                lines.append(f"### çŸ¥è¯†é€»è¾‘æµç¨‹")
                lines.append("")
                lines.append(f"{knowledge_flow}")
                lines.append("")
            
            # ç« èŠ‚ç»“æ„
            chapters = global_analysis.get('chapters', [])
            if chapters:
                lines.append("### ç« èŠ‚ç»“æ„")
                lines.append("")
                for i, chapter in enumerate(chapters, 1):
                    title = chapter.get('title', f'ç« èŠ‚{i}')
                    pages = chapter.get('pages', [])
                    key_concepts = chapter.get('key_concepts', [])
                    
                    lines.append(f"#### {i}. {title}")
                    if pages:
                        lines.append(f"- **é¡µç **: {', '.join(map(str, pages))}")
                    if key_concepts:
                        lines.append(f"- **æ ¸å¿ƒæ¦‚å¿µ**: {', '.join(key_concepts)}")
                    lines.append("")
            
            # çŸ¥è¯†ç‚¹å•å…ƒ
            knowledge_units = global_analysis.get('knowledge_units', [])
            if knowledge_units:
                lines.append("### çŸ¥è¯†ç‚¹å•å…ƒ")
                lines.append("")
                for unit in knowledge_units:
                    title = unit.get('title', 'æœªçŸ¥çŸ¥è¯†ç‚¹')
                    pages = unit.get('pages', [])
                    core_concepts = unit.get('core_concepts', [])
                    
                    lines.append(f"#### {title}")
                    if pages:
                        lines.append(f"- **æ¶‰åŠé¡µé¢**: {', '.join(map(str, pages))}")
                    if core_concepts:
                        lines.append(f"- **æ ¸å¿ƒæ¦‚å¿µ**: {', '.join(core_concepts)}")
                    lines.append("")
            
            lines.append("---")
            lines.append("")
        
        # é¡µé¢åˆ†æ
        if include_pages and page_analyses:
            lines.append("## ğŸ“‘ é¡µé¢è¯¦ç»†åˆ†æ")
            lines.append("")
            
            # æŒ‰é¡µç æ’åº
            sorted_pages = sorted(page_analyses.keys())
            
            for page_id in sorted_pages:
                analysis = page_analyses[page_id]
                lines.append(f"### ç¬¬ {page_id} é¡µ")
                lines.append("")
                
                # çŸ¥è¯†èšç±»
                knowledge_clusters = analysis.get('knowledge_clusters', [])
                if knowledge_clusters:
                    lines.append("#### ğŸ” éš¾ç‚¹æ¦‚å¿µè¯†åˆ«")
                    lines.append("")
                    for cluster in knowledge_clusters:
                        concept = cluster.get('concept', 'æœªçŸ¥æ¦‚å¿µ')
                        explanation = cluster.get('explanation', '')
                        lines.append(f"- **{concept}**")
                        if explanation:
                            lines.append(f"  {explanation}")
                        lines.append("")
                
                # ç»“æ„ç†è§£
                structure_notes = analysis.get('structure_notes', '')
                if structure_notes:
                    lines.append("#### ğŸ“ ç»“æ„ç†è§£")
                    lines.append("")
                    lines.append(structure_notes)
                    lines.append("")
                
                # çŸ¥è¯†ç¼ºå£
                gaps = analysis.get('gaps', [])
                if gaps:
                    lines.append("#### âš ï¸ çŸ¥è¯†ç¼ºå£")
                    lines.append("")
                    for gap in gaps:
                        gap_type = gap.get('type', 'æœªçŸ¥ç±»å‹')
                        description = gap.get('description', '')
                        lines.append(f"- **{gap_type}**: {description}")
                    lines.append("")
                
                # ç†è§£ç¬”è®°
                understanding_notes = analysis.get('understanding_notes', '')
                if understanding_notes:
                    lines.append("#### ğŸ“ ç†è§£ç¬”è®°")
                    lines.append("")
                    lines.append(understanding_notes)
                    lines.append("")
                
                # æ·±åº¦åˆ†æ
                deep_analysis = analysis.get('deep_analysis', '')
                if deep_analysis:
                    lines.append("#### ğŸ§  æ·±åº¦åˆ†æ")
                    lines.append("")
                    lines.append(deep_analysis)
                    lines.append("")
                
                lines.append("---")
                lines.append("")
        
        lines.append("*æœ¬æ–‡æ¡£ç”± PPTAS AI åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*")
        
        return "\n".join(lines)
