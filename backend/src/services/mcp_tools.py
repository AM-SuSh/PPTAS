"""
MCP (Model Context Protocol) å·¥å…·é›†æˆ
æ”¯æŒç»´åŸºç™¾ç§‘ã€Arxivã€Google Scholar ç­‰å¤–éƒ¨çŸ¥è¯†æº
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import requests
from bs4 import BeautifulSoup
from langchain_core.documents import Document
import xml.etree.ElementTree as ET
import re
from urllib.parse import quote

# å°è¯•å¯¼å…¥ LLM é…ç½®ï¼ˆç”¨äºç¿»è¯‘ï¼‰
try:
    from src.config import ConfigManager
    from langchain_openai import ChatOpenAI
    _llm_available = True
except ImportError:
    _llm_available = False
    print("âš ï¸  LLM é…ç½®ä¸å¯ç”¨ï¼Œç¿»è¯‘åŠŸèƒ½å°†è¢«ç¦ç”¨")


def _translate_to_english(text: str) -> str:
    """å°†ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ï¼ˆç”¨äº Arxiv æœç´¢ï¼‰"""
    if not _llm_available:
        return text
    
    # ç®€å•åˆ¤æ–­ï¼šå¦‚æœåŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œåˆ™å°è¯•ç¿»è¯‘
    if not re.search(r'[\u4e00-\u9fff]', text):
        return text  # æ²¡æœ‰ä¸­æ–‡å­—ç¬¦ï¼Œç›´æ¥è¿”å›
    
    try:
        config_manager = ConfigManager()
        llm_config = config_manager.get_llm_config()
        
        llm = ChatOpenAI(
            api_key=llm_config.api_key,
            base_url=llm_config.base_url,
            model=llm_config.model,
            temperature=0.3,
            max_retries=2
        )
        
        prompt = f"""è¯·å°†ä»¥ä¸‹ä¸­æ–‡æŸ¥è¯¢ç¿»è¯‘æˆè‹±æ–‡ï¼Œç”¨äºå­¦æœ¯è®ºæ–‡æœç´¢ã€‚åªè¿”å›è‹±æ–‡ç¿»è¯‘ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚

ä¸­æ–‡æŸ¥è¯¢ï¼š{text}

è‹±æ–‡ç¿»è¯‘ï¼š"""
        
        response = llm.invoke(prompt)
        translated = response.content.strip()
        
        # æ¸…ç†ç¿»è¯‘ç»“æœï¼ˆç§»é™¤å¯èƒ½çš„å¼•å·æˆ–å¤šä½™å†…å®¹ï¼‰
        translated = re.sub(r'^["\']|["\']$', '', translated)
        translated = translated.split('\n')[0].strip()
        
        if translated and len(translated) > 0:
            print(f"      ğŸŒ ç¿»è¯‘: '{text}' -> '{translated}'")
            return translated
        else:
            print(f"      âš ï¸  ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
            return text
    except Exception as e:
        print(f"      âš ï¸  ç¿»è¯‘å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
        return text


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    title: str
    content: str
    url: str
    source: str
    metadata: Dict[str, Any]


class WikipediaMCP:
    """ç»´åŸºç™¾ç§‘ MCP å·¥å…·"""
    
    def __init__(self, language: str = "zh"):
        self.language = language
        self.api_url = f"https://{language}.wikipedia.org/w/api.php"
        self.headers = {
            "User-Agent": "PPTAS-Bot/1.0 (https://github.com/user/pptas)"
        }
    
    def search(self, query: str, limit: int = 3) -> List[Document]:
        """æœç´¢ç»´åŸºç™¾ç§‘"""
        params = {
            "action": "query",
            "format": "json",
            "list": "search",
            "srsearch": query,
            "srlimit": limit
        }
        
        try:
            response = requests.get(self.api_url, params=params, timeout=10, headers=self.headers)
            response.raise_for_status()  # æ£€æŸ¥ HTTP çŠ¶æ€
            
            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not response.text:
                print(f"Wikipedia search error: Empty response for query '{query}'")
                return []
            
            data = response.json()
            
            documents = []
            for item in data.get("query", {}).get("search", []):
                # è·å–é¡µé¢å†…å®¹
                content = self._get_page_content(item["title"])
                if content:
                    documents.append(Document(
                        page_content=content,
                        metadata={
                            "source": "Wikipedia",
                            "title": item["title"],
                            "url": f"https://{self.language}.wikipedia.org/wiki/{item['title'].replace(' ', '_')}"
                        }
                    ))
            
            return documents
        except Exception as e:
            print(f"Wikipedia search error: {type(e).__name__}: {e}")
            return []
    
    def _get_page_content(self, title: str) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹æ‘˜è¦"""
        params = {
            "action": "query",
            "format": "json",
            "prop": "extracts",
            "exintro": True,
            "explaintext": True,
            "titles": title
        }
        
        try:
            response = requests.get(self.api_url, params=params, timeout=10, headers=self.headers)
            response.raise_for_status()
            
            if not response.text:
                return None
            
            data = response.json()
            pages = data.get("query", {}).get("pages", {})
            
            for page in pages.values():
                content = page.get("extract", "")
                if content:
                    return content[:1000]  # é™åˆ¶é•¿åº¦
            
            return None
        except Exception as e:
            print(f"Get Wikipedia page content error: {type(e).__name__}: {e}")
            return None


class ArxivMCP:
    """Arxiv MCP å·¥å…·"""
    
    def __init__(self):
        self.api_url = "http://export.arxiv.org/api/query"
    
    def search(self, query: str, max_results: int = 3) -> List[Document]:
        """æœç´¢ Arxiv è®ºæ–‡"""
        # å¦‚æœæŸ¥è¯¢æ˜¯ä¸­æ–‡ï¼Œå…ˆç¿»è¯‘æˆè‹±æ–‡
        original_query = query.strip()
        query_clean = _translate_to_english(original_query)
        
        # æ¸…ç†æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        query_clean = query_clean.strip()
        # å¦‚æœæŸ¥è¯¢åŒ…å«å¤šä¸ªè¯ï¼Œä½¿ç”¨ORè¿æ¥
        if " " in query_clean:
            # å°†å¤šä¸ªè¯ç”¨ORè¿æ¥
            words = query_clean.split()
            search_query = " OR ".join([f"all:{word}" for word in words[:3]])  # æœ€å¤š3ä¸ªè¯
        else:
            search_query = f"all:{query_clean}"
        
        params = {
            "search_query": search_query,
            "start": 0,
            "max_results": max_results,
            "sortBy": "relevance",
            "sortOrder": "descending"
        }
        
        print(f"      Arxiv API URL: {self.api_url}")
        print(f"      Arxiv æœç´¢æŸ¥è¯¢: {search_query}")
        
        try:
            response = requests.get(self.api_url, params=params, timeout=15)
            print(f"      Arxiv HTTPçŠ¶æ€: {response.status_code}")
            
            if response.status_code != 200:
                print(f"      âš ï¸  Arxiv APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}")
                print(f"      å“åº”å†…å®¹: {response.text[:200]}")
                return []
            
            if not response.content:
                print(f"      âš ï¸  Arxiv APIè¿”å›ç©ºå“åº”")
                return []
            
            root = ET.fromstring(response.content)
            
            documents = []
            entries = root.findall("{http://www.w3.org/2005/Atom}entry")
            print(f"      Arxiv æ‰¾åˆ° {len(entries)} ä¸ªæ¡ç›®")
            
            if len(entries) == 0:
                print(f"      âš ï¸  Arxiv æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®ºæ–‡")
                print(f"      å¯èƒ½åŸå› :")
                print(f"      1. æŸ¥è¯¢è¯ä¸åŒ¹é…ï¼ˆArxivä¸»è¦æ”¶å½•è‹±æ–‡è®ºæ–‡ï¼‰")
                print(f"      2. æŸ¥è¯¢è¯å¤ªå…·ä½“æˆ–å¤ªæ–°")
                print(f"      3. ç½‘ç»œé—®é¢˜")
            
            for entry in entries:
                try:
                    title_elem = entry.find("{http://www.w3.org/2005/Atom}title")
                    summary_elem = entry.find("{http://www.w3.org/2005/Atom}summary")
                    id_elem = entry.find("{http://www.w3.org/2005/Atom}id")
                    
                    if title_elem is None or summary_elem is None or id_elem is None:
                        continue
                    
                    title = title_elem.text.strip() if title_elem.text else ""
                    summary = summary_elem.text.strip() if summary_elem.text else ""
                    link = id_elem.text.strip() if id_elem.text else ""
                    
                    if not title:
                        continue
                    
                    # è·å–ä½œè€…
                    authors = []
                    for author in entry.findall("{http://www.w3.org/2005/Atom}author"):
                        name_elem = author.find("{http://www.w3.org/2005/Atom}name")
                        if name_elem is not None and name_elem.text:
                            authors.append(name_elem.text)
                    
                    documents.append(Document(
                        page_content=f"{title}\n\n{summary[:800]}",
                        metadata={
                            "source": "Arxiv",
                            "title": title,
                            "authors": ", ".join(authors) if authors else "",
                            "url": link
                        }
                    ))
                except Exception as e:
                    print(f"      âš ï¸  è§£æArxivæ¡ç›®å¤±è´¥: {e}")
                    continue
            
            print(f"      âœ… Arxiv æˆåŠŸè§£æ {len(documents)} ä¸ªæ–‡æ¡£")
            return documents
        except requests.exceptions.RequestException as e:
            print(f"      âŒ Arxiv ç½‘ç»œè¯·æ±‚å¤±è´¥: {type(e).__name__}: {e}")
            return []
        except ET.ParseError as e:
            print(f"      âŒ Arxiv XMLè§£æå¤±è´¥: {e}")
            print(f"      å“åº”å†…å®¹å‰500å­—ç¬¦: {response.content[:500] if 'response' in locals() else 'N/A'}")
            return []
        except Exception as e:
            print(f"      âŒ Arxiv æœç´¢å¤±è´¥: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return []


class GoogleScholarMCP:
    """Google Scholar MCP å·¥å…·ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    
    def search(self, query: str, num_results: int = 3) -> List[Document]:
        """æœç´¢ Google Scholar
        æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–ç‰ˆï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ SerpAPI ç­‰æœåŠ¡
        """
        url = f"https://scholar.google.com/scholar?q={query}"
        
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            documents = []
            results = soup.find_all('div', class_='gs_ri')[:num_results]
            
            for result in results:
                title_elem = result.find('h3', class_='gs_rt')
                title = title_elem.get_text() if title_elem else "Unknown"
                
                snippet_elem = result.find('div', class_='gs_rs')
                snippet = snippet_elem.get_text() if snippet_elem else ""
                
                link_elem = title_elem.find('a') if title_elem else None
                link = link_elem['href'] if link_elem else ""
                
                documents.append(Document(
                    page_content=f"{title}\n\n{snippet[:500]}",
                    metadata={
                        "source": "Google Scholar",
                        "title": title,
                        "url": link
                    }
                ))
            
            return documents
        except Exception as e:
            print(f"Google Scholar search error: {e}")
            return []


class BaiduBaikeMCP:
    """ç™¾åº¦ç™¾ç§‘ MCP å·¥å…·"""
    
    def __init__(self):
        self.base_url = "https://baike.baidu.com"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    
    def search(self, query: str, fallback: bool = True) -> List[Document]:
        """æœç´¢ç™¾åº¦ç™¾ç§‘ï¼ˆçµæ´»æœç´¢ï¼Œæ”¯æŒç›¸å…³è¯æ¡ï¼Œä¿åº•æœç´¢ï¼‰"""
        # ç”Ÿæˆå¤šä¸ªæœç´¢å…³é”®è¯å˜ä½“
        search_variants = self._generate_search_variants(query)
        
        print(f"      Baike æŸ¥è¯¢: '{query}'")
        print(f"      Baike æœç´¢å˜ä½“: {search_variants}")
        
        all_documents = []
        seen_urls = set()
        
        # å°è¯•æ¯ä¸ªæœç´¢å˜ä½“
        for variant in search_variants:
            if len(all_documents) >= 3:  # æœ€å¤šè¿”å›3ä¸ªç»“æœ
                break
            
            variant_clean = variant.strip()
            if not variant_clean:
                continue
            
            # URLç¼–ç 
            encoded_query = quote(variant_clean.encode('utf-8'))
            search_url = f"{self.base_url}/search?word={encoded_query}"
            
            try:
                response = requests.get(search_url, headers=self.headers, timeout=10)
                print(f"      ğŸ” æœç´¢å˜ä½“ '{variant_clean}': HTTP {response.status_code}")
                
                if response.status_code != 200:
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æ–¹æ³•1: å°è¯•ç›´æ¥è®¿é—®è¯æ¡é¡µé¢
                direct_url = f"{self.base_url}/item/{encoded_query}"
                try:
                    direct_response = requests.get(direct_url, headers=self.headers, timeout=10)
                    if direct_response.status_code == 200:
                        direct_soup = BeautifulSoup(direct_response.text, 'html.parser')
                        summary = direct_soup.find('div', class_='lemma-summary')
                        if summary:
                            content = summary.get_text().strip()[:1000]
                            title_elem = direct_soup.find('h1', class_='lemmaWgt-lemmaTitle-title')
                            title = title_elem.get_text().strip() if title_elem else variant_clean
                            
                            doc_url = direct_url
                            if doc_url not in seen_urls:
                                seen_urls.add(doc_url)
                                all_documents.append(Document(
                                    page_content=content,
                                    metadata={
                                        "source": "Baidu Baike",
                                        "title": title,
                                        "url": doc_url
                                    }
                                ))
                                print(f"      âœ… ç›´æ¥è®¿é—®æˆåŠŸ: {title}")
                                continue
                except Exception:
                    pass
                
                # æ–¹æ³•2: ä»æœç´¢ç»“æœé¡µé¢è·å–å¤šä¸ªç»“æœ
                # æŸ¥æ‰¾æ‰€æœ‰è¯æ¡é“¾æ¥
                links = soup.find_all('a', href=re.compile(r'/item/'))
                if links:
                    print(f"      æ‰¾åˆ° {len(links)} ä¸ªè¯æ¡é“¾æ¥")
                    for link_elem in links[:5]:  # å¢åŠ å°è¯•æ•°é‡
                        if len(all_documents) >= 3:
                            break
                        
                        href = link_elem.get('href', '')
                        if not href:
                            continue
                        
                        # æ„å»ºå®Œæ•´URL
                        if href.startswith('//'):
                            full_url = 'https:' + href
                        elif href.startswith('/'):
                            full_url = self.base_url + href
                        else:
                            full_url = href
                        
                        if full_url in seen_urls:
                            continue
                        
                        try:
                            content_response = requests.get(full_url, headers=self.headers, timeout=10)
                            if content_response.status_code == 200:
                                content_soup = BeautifulSoup(content_response.text, 'html.parser')
                                summary = content_soup.find('div', class_='lemma-summary')
                                if summary:
                                    content = summary.get_text().strip()[:1000]
                                    title_elem = content_soup.find('h1', class_='lemmaWgt-lemmaTitle-title')
                                    title = title_elem.get_text().strip() if title_elem else link_elem.get_text().strip()
                                    
                                    if title and content:
                                        seen_urls.add(full_url)
                                        all_documents.append(Document(
                                            page_content=content,
                                            metadata={
                                                "source": "Baidu Baike",
                                                "title": title,
                                                "url": full_url
                                            }
                                        ))
                                        print(f"      âœ… è·å–è¯æ¡: {title}")
                        except Exception:
                            continue
                
                # æ–¹æ³•3: å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»æœç´¢ç»“æœé¡µé¢æå–æ–‡æœ¬æ‘˜è¦
                if len(all_documents) == 0:
                    # æŸ¥æ‰¾æœç´¢ç»“æœæ‘˜è¦
                    result_items = soup.find_all(['div', 'dd'], class_=re.compile(r'search|result|item'))
                    for item in result_items[:3]:
                        text = item.get_text().strip()
                        if text and len(text) > 50:  # è‡³å°‘50å­—ç¬¦
                            # å°è¯•æå–æ ‡é¢˜
                            title_elem = item.find(['a', 'h3', 'h4'])
                            title = title_elem.get_text().strip() if title_elem else query
                            
                            # æå–é“¾æ¥
                            link_elem = item.find('a', href=re.compile(r'/item/'))
                            if link_elem:
                                href = link_elem.get('href', '')
                                if href.startswith('//'):
                                    full_url = 'https:' + href
                                elif href.startswith('/'):
                                    full_url = self.base_url + href
                                else:
                                    full_url = href
                            else:
                                full_url = f"{self.base_url}/search?word={encoded_query}"
                            
                            if full_url not in seen_urls:
                                seen_urls.add(full_url)
                                all_documents.append(Document(
                                    page_content=text[:1000],
                                    metadata={
                                        "source": "Baidu Baike",
                                        "title": title,
                                        "url": full_url
                                    }
                                ))
                                print(f"      âœ… ä»æœç´¢ç»“æœæå–: {title}")
                                break
                
            except requests.exceptions.RequestException:
                continue
            except Exception:
                continue
        
        # ä¿åº•ï¼šå¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é€šç”¨è¯æ¡
        if len(all_documents) == 0 and fallback:
            print(f"      âš ï¸  æœªæ‰¾åˆ°ç›´æ¥åŒ¹é…ï¼Œå°è¯•ä¿åº•æœç´¢...")
            # å°è¯•æœç´¢æ ¸å¿ƒæ¦‚å¿µ
            core_concepts = self._extract_core_concepts(query)
            for concept in core_concepts[:2]:  # æœ€å¤šå°è¯•2ä¸ªæ ¸å¿ƒæ¦‚å¿µ
                if len(all_documents) > 0:
                    break
                try:
                    encoded = quote(concept.encode('utf-8'))
                    fallback_url = f"{self.base_url}/item/{encoded}"
                    response = requests.get(fallback_url, headers=self.headers, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        summary = soup.find('div', class_='lemma-summary')
                        if summary:
                            content = summary.get_text().strip()[:1000]
                            title_elem = soup.find('h1', class_='lemmaWgt-lemmaTitle-title')
                            title = title_elem.get_text().strip() if title_elem else concept
                            
                            all_documents.append(Document(
                                page_content=content,
                                metadata={
                                    "source": "Baidu Baike",
                                    "title": title,
                                    "url": fallback_url
                                }
                            ))
                            print(f"      âœ… ä¿åº•æœç´¢æˆåŠŸ: {title}")
                except Exception:
                    continue
        
        print(f"      âœ… Baike æ€»å…±æ‰¾åˆ° {len(all_documents)} æ¡ç»“æœ")
        return all_documents[:3]  # æœ€å¤šè¿”å›3ä¸ª
    
    def _extract_core_concepts(self, query: str) -> List[str]:
        """æå–æ ¸å¿ƒæ¦‚å¿µï¼ˆç”¨äºä¿åº•æœç´¢ï¼‰"""
        concepts = []
        
        # ç§»é™¤å¸¸è§ä¿®é¥°è¯
        cleaned = re.sub(r'(åœ¨|çš„|ä¸­|å’Œ|ä¸|åŠ|åº”ç”¨|æ–¹æ³•|æŠ€æœ¯|ç³»ç»Ÿ|æ¨¡å‹|æ”»å‡»|å®‰å…¨|éšç§)', ' ', query)
        words = [w for w in cleaned.split() if len(w) >= 2]
        
        # ä¼˜å…ˆé€‰æ‹©è¾ƒé•¿çš„è¯ï¼ˆé€šå¸¸æ˜¯æ ¸å¿ƒæ¦‚å¿µï¼‰
        words.sort(key=len, reverse=True)
        concepts.extend(words[:3])
        
        # å¦‚æœæŸ¥è¯¢æœ¬èº«æ˜¯å•ä¸ªè¯ï¼Œä¹ŸåŠ å…¥
        if len(query.split()) == 1 and query not in concepts:
            concepts.insert(0, query)
        
        return concepts[:3]  # æœ€å¤š3ä¸ª
    
    def _generate_search_variants(self, query: str) -> List[str]:
        """ç”Ÿæˆæœç´¢å…³é”®è¯å˜ä½“ï¼ˆæ›´çµæ´»çš„æœç´¢ï¼‰"""
        variants = []
        
        # 1. åŸå§‹æŸ¥è¯¢
        variants.append(query.strip())
        
        # 2. ç§»é™¤æ‹¬å·å†…å®¹
        no_brackets = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', query).strip()
        if no_brackets and no_brackets != query:
            variants.append(no_brackets)
        
        # 3. æå–æ ¸å¿ƒè¯ï¼ˆç§»é™¤"çš„"ã€"åœ¨"ã€"ä¸­"ç­‰åŠ©è¯ï¼‰
        core_words = re.sub(r'[çš„åœ¨ä¸­çš„å’Œä¸åŠ]', ' ', query)
        core_words = ' '.join([w for w in core_words.split() if len(w) > 1])
        if core_words and core_words != query:
            variants.append(core_words)
        
        # 4. åªå–ç¬¬ä¸€ä¸ªè¯ï¼ˆå¦‚æœæ˜¯å¤åˆè¯ï¼‰
        first_word = query.split()[0] if query.split() else query
        if first_word and first_word != query and len(first_word) >= 2:
            variants.append(first_word)
        
        # 5. æå–å…³é”®è¯ï¼ˆç§»é™¤å¸¸è§ä¿®é¥°è¯ï¼‰
        keywords = re.sub(r'(åœ¨|çš„|ä¸­|å’Œ|ä¸|åŠ|åº”ç”¨|æ–¹æ³•|æŠ€æœ¯|ç³»ç»Ÿ|æ¨¡å‹)', ' ', query)
        keywords = ' '.join([w for w in keywords.split() if len(w) > 1])
        if keywords and keywords != query:
            variants.append(keywords)
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_variants = []
        for v in variants:
            if v and v not in seen:
                seen.add(v)
                unique_variants.append(v)
        
        return unique_variants[:5]  # æœ€å¤š5ä¸ªå˜ä½“


class MCPRouter:
    """MCP å·¥å…·è·¯ç”±å™¨ - æ™ºèƒ½é€‰æ‹©æœ€ä½³å·¥å…·"""
    
    def __init__(self):
        self.tools = {
            "wikipedia": WikipediaMCP(),
            "arxiv": ArxivMCP(),
            "scholar": GoogleScholarMCP(),
            "baike": BaiduBaikeMCP()
        }
        # å¯ç”¨æ‰€æœ‰æºï¼Œè®©è°ƒç”¨è€…å†³å®šä½¿ç”¨å“ªäº›
        self.enabled_sources = ["arxiv", "wikipedia", "baike"]  
    
    def search(self, query: str, preferred_sources: List[str] = None) -> List[Document]:
        """æ™ºèƒ½æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            preferred_sources: ä¼˜å…ˆä½¿ç”¨çš„æºï¼Œå¦‚ ["arxiv", "wikipedia"]
        """
        all_documents = []
        
        # å¦‚æœæŒ‡å®šäº†ä¼˜å…ˆæºï¼Œä½¿ç”¨å®ƒä»¬
        if preferred_sources:
            print(f"ğŸ” MCPRouter: ä½¿ç”¨æŒ‡å®šæº {preferred_sources} æœç´¢ '{query}'")
            for source in preferred_sources:
                if source not in self.tools:
                    print(f"   âš ï¸  æº '{source}' ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                    continue
                
                try:
                    print(f"   ğŸ” æ­£åœ¨æœç´¢ {source}...")
                    if source == "arxiv":
                        docs = self.tools[source].search(query, max_results=3)
                    elif source == "baike":
                        # ç™¾åº¦ä½œä¸ºä¿åº•ï¼Œç¡®ä¿èƒ½æœåˆ°ä¸œè¥¿
                        docs = self.tools[source].search(query, fallback=True)
                    elif source == "wikipedia":
                        docs = self.tools[source].search(query, limit=3)
                    else:
                        docs = self.tools[source].search(query)
                    
                    print(f"   âœ… {source} è¿”å› {len(docs)} æ¡ç»“æœ")
                    all_documents.extend(docs)
                except Exception as e:
                    print(f"   âŒ {source} æœç´¢å¤±è´¥: {type(e).__name__}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
        else:
            # è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨ Arxivï¼ˆæ›´ç¨³å®šï¼‰
            print(f"ğŸ” MCPRouter: è‡ªåŠ¨é€‰æ‹©æºæœç´¢ '{query}'")
            try:
                docs = self.tools["arxiv"].search(query, max_results=3)
                all_documents.extend(docs)
                print(f"   âœ… arxiv è¿”å› {len(docs)} æ¡ç»“æœ")
            except Exception as e:
                print(f"   âŒ Arxiv æœç´¢å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ç»“æœï¼Œä¸è¿”å›å ä½ç¬¦ï¼ˆè®©è°ƒç”¨è€…å¤„ç†ï¼‰
        if not all_documents:
            print(f"   âš ï¸  æ‰€æœ‰æºéƒ½æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
        
        # å»é‡
        seen_urls = set()
        unique_docs = []
        for doc in all_documents:
            url = doc.metadata.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_docs.append(doc)
            elif not url:
                # å…è®¸æœ¬åœ°å ä½ç¬¦æ–‡æ¡£
                unique_docs.append(doc)
        
        return unique_docs[:5]  # é™åˆ¶ç»“æœæ•°é‡
    
    def _is_academic_query(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå­¦æœ¯æŸ¥è¯¢"""
        academic_keywords = [
            "algorithm", "model", "neural", "learning", "theory",
            "ç®—æ³•", "æ¨¡å‹", "ç¥ç»", "å­¦ä¹ ", "ç†è®º", "å…¬å¼", "è¯æ˜"
        ]
        return any(keyword in query.lower() for keyword in academic_keywords)
