# src/config.py
"""
é…ç½®æ–‡ä»¶ç®¡ç†
é€‚é… config.json ç»“æ„
"""

import os
import json
from pathlib import Path
from typing import Dict, Any, Optional
from dataclasses import dataclass

@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    api_key: str = ""
    model: str = "gpt-3.5-turbo"
    base_url: Optional[str] = None
    temperature: float = 0.7
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        # å¦‚æœ api_key ä¸ºç©ºï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
        if not self.api_key:
            self.api_key = os.getenv('OPENAI_API_KEY', '')
        
        # å¦‚æœ base_url ä¸ºç©ºï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
        if not self.base_url:
            self.base_url = os.getenv('OPENAI_BASE_URL', None)

@dataclass
class RetrievalConfig:
    """æ£€ç´¢é…ç½®"""
    preferred_sources: list = None
    max_results: int = 3
    local_rag_priority: bool = True
    
    def __post_init__(self):
        if self.preferred_sources is None:
            self.preferred_sources = ["arxiv", "wikipedia"]

@dataclass
class ExpansionConfig:
    """æ‰©å±•é…ç½®"""
    max_revisions: int = 2
    min_gap_priority: int = 3
    temperature: float = 0.7

@dataclass
class StreamingConfig:
    """æµå¼é…ç½®"""
    enabled: bool = True
    chunk_size: int = 50

@dataclass
class KnowledgeBaseConfig:
    """çŸ¥è¯†åº“é…ç½®"""
    path: str = "./knowledge_base"
    chunk_size: int = 1000
    chunk_overlap: int = 200

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    _instance = None
    _config_data = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ConfigManager, cls).__new__(cls)
            cls._instance._load_config()
        return cls._instance
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_paths = [
            Path(__file__).parent.parent / "config.json",
            Path(__file__).parent / "config.json",
            Path.cwd() / "config.json",
        ]
        
        for config_path in config_paths:
            if config_path.exists():
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        self._config_data = json.load(f)
                    print(f"ğŸ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
                    break
                except Exception as e:
                    print(f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        
        if self._config_data is None:
            print("âš ï¸  æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self._config_data = {}
    
    def get_llm_config(self) -> LLMConfig:
        """è·å– LLM é…ç½®"""
        llm_data = self._config_data.get('llm', {})
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆç”¨äºå…¶ä»–å¯èƒ½ç›´æ¥è¯»å–ç¯å¢ƒå˜é‡çš„æ¨¡å—ï¼‰
        if 'api_key' in llm_data:
            os.environ['OPENAI_API_KEY'] = llm_data['api_key']
        if 'base_url' in llm_data:
            os.environ['OPENAI_BASE_URL'] = llm_data['base_url']
        
        return LLMConfig(
            api_key=llm_data.get('api_key', ''),
            model=llm_data.get('model', 'gpt-3.5-turbo'),
            base_url=llm_data.get('base_url'),
            temperature=self._config_data.get('expansion', {}).get('temperature', 0.7)
        )
    
    def get_retrieval_config(self) -> RetrievalConfig:
        """è·å–æ£€ç´¢é…ç½®"""
        retrieval_data = self._config_data.get('retrieval', {})
        return RetrievalConfig(
            preferred_sources=retrieval_data.get('preferred_sources', ["arxiv", "wikipedia"]),
            max_results=retrieval_data.get('max_results', 3),
            local_rag_priority=retrieval_data.get('local_rag_priority', True)
        )
    
    def get_expansion_config(self) -> ExpansionConfig:
        """è·å–æ‰©å±•é…ç½®"""
        expansion_data = self._config_data.get('expansion', {})
        return ExpansionConfig(
            max_revisions=expansion_data.get('max_revisions', 2),
            min_gap_priority=expansion_data.get('min_gap_priority', 3),
            temperature=expansion_data.get('temperature', 0.7)
        )
    
    def get_streaming_config(self) -> StreamingConfig:
        """è·å–æµå¼é…ç½®"""
        streaming_data = self._config_data.get('streaming', {})
        return StreamingConfig(
            enabled=streaming_data.get('enabled', True),
            chunk_size=streaming_data.get('chunk_size', 50)
        )
    
    def get_knowledge_base_config(self) -> KnowledgeBaseConfig:
        """è·å–çŸ¥è¯†åº“é…ç½®"""
        kb_data = self._config_data.get('knowledge_base', {})
        return KnowledgeBaseConfig(
            path=kb_data.get('path', "./knowledge_base"),
            chunk_size=kb_data.get('chunk_size', 1000),
            chunk_overlap=kb_data.get('chunk_overlap', 200)
        )

# å…¨å±€é…ç½®å®ä¾‹
config_manager = ConfigManager()

# å¿«æ·è®¿é—®å‡½æ•°
def get_llm_config() -> LLMConfig:
    """è·å– LLM é…ç½®"""
    return config_manager.get_llm_config()

def get_retrieval_config() -> RetrievalConfig:
    """è·å–æ£€ç´¢é…ç½®"""
    return config_manager.get_retrieval_config()

def get_expansion_config() -> ExpansionConfig:
    """è·å–æ‰©å±•é…ç½®"""
    return config_manager.get_expansion_config()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    llm_config = get_llm_config()
    print(f"LLM é…ç½®:")
    print(f"  API Key: {llm_config.api_key[:10]}..." if llm_config.api_key else "  API Key: æœªè®¾ç½®")
    print(f"  æ¨¡å‹: {llm_config.model}")
    print(f"  Base URL: {llm_config.base_url}")
    print(f"  æ¸©åº¦: {llm_config.temperature}")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦å·²è®¾ç½®
    print(f"\nç¯å¢ƒå˜é‡æ£€æŸ¥:")
    print(f"  OPENAI_API_KEY: {'å·²è®¾ç½®' if os.getenv('OPENAI_API_KEY') else 'æœªè®¾ç½®'}")
    print(f"  OPENAI_BASE_URL: {os.getenv('OPENAI_BASE_URL', 'æœªè®¾ç½®')}")